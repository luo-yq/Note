总结
1.建用户，2.ssh免密，3.jdk，4.安装Hadoop2.7, 5.配置core,hdf,mr,yarn ，6.


1. 创建用户

groupadd server
useradd -g server server
passwd server

2. ssh免密

1）以hadoop用户远程登录K-Master服务器，在K-Master服务器上生成公钥和私钥密码对，密钥默认存储在”/home/hadoop/.ssh”目录下，生成公钥和私钥密码对时，无需输入密码，直接回车即可。
2）将公钥追加到”authorized_keys”文件
3）赋予权限
4）验证本机能无密码访问

su server
cd /home/server/
rm -rf .ssh
mkdir .ssh
ssh-keygen -t rsa -P ""
#ssh-keygen -t rsa

cd /home/server/
cat .ssh/id_rsa.pub >> .ssh/authorized_keys

chmod 700 ~/.ssh
chmod 600 .ssh/authorized_keys

ssh server210



hostnamectl  
hostnamectl set-hostname server210



三台主机改配置文件:
vi /etc/ssh/sshd_config
#禁用root账户登录，如果是用root用户登录请开启
PermitRootLogin yes

# 是否让 sshd 去检查用户家目录或相关档案的权限数据，
# 这是为了担心使用者将某些重要档案的权限设错，可能会导致一些问题所致。
# 例如使用者的 ~.ssh/ 权限设错时，某些特殊情况下会不许用户登入
StrictModes no

# 是否允许用户自行使用成对的密钥系统进行登入行为，仅针对 version 2。
# 至于自制的公钥数据就放置于用户家目录下的 .ssh/authorized_keys 内
RSAAuthentication yes
PubkeyAuthentication yes
AuthorizedKeysFile      .ssh/authorized_keys

# 有了证书登录了，就禁用密码登录吧，安全要紧
PasswordAuthentication no




安装JDK，Hadoop2.7需要JDK7

Hadoop2.7 包含 common,hdfs,yarn和MapReduce

su server
cd /home/server
wget http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gz 
tar -xzvf hadoop-2.7.4.tar.gz
rm hadoop-2.7.4.tar.gz
mkdir -p tmp dfs/data dfs/name

vi /home/server/hadoop-2.7.4/etc/hadoop/core-site.xml
 <configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://192.168.195.210:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/home/server/tmp</value>
    </property>
    <property>
        <name>io.file.buffer.size</name>
        <value>131702</value>
    </property>
 </configuration>

vi /home/server/hadoop-2.7.4/etc/hadoop/hdfs-site.xml
 <configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/home/server/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/home/server/dfs/data</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>192.168.195.211:9001</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>
 </configuration>

cp /home/server/hadoop-2.7.4/etc/hadoop/mapred-site.xml.template /home/server/hadoop-2.7.4/etc/hadoop/mapred-site.xml
vi /home/server/hadoop-2.7.4/etc/hadoop/mapred-site.xml
 <configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>192.168.195.211:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>192.168.195.211:19888</value>
    </property>
 </configuration>


 vi /home/server/hadoop-2.7.4/etc/hadoop/yarn-site.xml
 <configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>192.168.195.211:8032</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>192.168.195.211:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>192.168.195.211:8031</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>192.168.195.211:8033</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>192.168.195.211:8088</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>768</value>
    </property>
 </configuration>

9、vi /home/server/hadoop-2.7.4/etc/hadoop/hadoop-env.sh
   vi /home/server/hadoop-2.7.4/etc/hadoop/yarn-env.sh 
export JAVA_HOME=/opt/java/jdk1.8.0_73

10、vi /home/server/hadoop-2.7.4/etc/hadoop/slaves

删除默认的localhost，增加2个从节点，
192.168.195.212
192.168.195.213

11、将配置好的Hadoop复制到各个节点对应位置上，通过scp传送， 
scp -r /home/server 192.168.195.212:/home/
scp -r /home/server 192.168.195.213:/home/

12、在Master服务器启动hadoop，从节点会自动启动，cd /home/server/hadoop-2.7.4 
(1)初始化，输入命令，bin/hdfs namenode -format
(2)全部启动sbin/start-all.sh，也可以分开sbin/start-dfs.sh、sbin/start-yarn.sh
(3)停止的话，输入命令，sbin/stop-all.sh
(4)输入命令，jps，可以看到相关信息
查看集群状态 bin/hadoop dfsadmin -report

sudo firewall-cmd --zone=public --add-port=8088/tcp --permanent  
sudo firewall-cmd --zone=public --add-port=50070/tcp --permanent  
firewall-cmd --reload

13、Web访问，要先开放端口或者直接关闭防火墙
(1)输入命令，systemctl stop firewalld.service
(2)浏览器打开http://192.168.195.211:8088/
(3)浏览器打开http://192.168.195.211:50070/










spark安装scala版本

wget http://www.scala-lang.org/files/archive/scala-2.11.8.tgz
tar -zxf scala-2.11.8.tgz
mv scala-2.11.8 /usr/lib

vi /etc/profile
# 在文件末尾位置加入以下代码
export SCALA_HOME=/usr/lib/scala-2.11.8
export PATH=$PATH:$SCALA_HOME/bin

source /etc/profile

su server
cd ~
wget  http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.7.tgz
tar -zxf spark-2.0.0-bin-hadoop2.7.tgz

vi ~/.bash_profile
# 在文件末尾位置加入以下代码
export SPARK_HOME=$HOME/spark-2.0.0-bin-hadoop2.7
source  ~/.bash_profile

 cd $SPARK_HOME
 
 ./sbin/start-all.sh

 例子，计算Pi
 ./bin/run-example SparkPi


vi $SPARK_HOME/sbin/spark-config.sh

export JAVA_HOME=/opt/java/jdk1.8.0_73

 
sudo firewall-cmd --zone=public --add-port=8080/tcp --permanent  
sudo firewall-cmd --zone=public --add-port=8081/tcp --permanent  
sudo firewall-cmd --zone=public --add-port=7077/tcp --permanent  
firewall-cmd --reload


http://192.168.195.211:8080/
http://192.168.195.211:7077/


配置集群 cp conf/slaves.template conf/slaves
sever211
sever212
sever213 





wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.10/zookeeper-3.4.10.tar.gz
tar -xzvf zookeeper-3.4.10.tar.gz
rm zookeeper-3.4.10.tar.gz
mkdir -p data/zookeeper logs/zookeeper
cp zookeeper-3.4.10/conf/zoo_sample.cfg zookeeper-3.4.10/conf/zoo.cfg
vi zookeeper-3.4.10/conf/zoo.cfg


tickTime=2000    
initLimit=10   
syncLimit=5   
dataDir=/home/server/data/zookeeper 
dataLogDir=/home/server/logs/zookeeper   
clientPort=2181  
server.211=192.168.195.211:2888:3888  
server.212=192.168.195.212:2888:3888    
server.213=192.168.195.213:2888:3888 


echo 211 >> /home/server/data/zookeeper/myid
写入对应的id号

tickTime: zookeeper中使用的基本时间单位, 毫秒值.
dataDir: 数据目录. 可以是任意目录.
dataLogDir: log目录, 同样可以是任意目录. 如果没有设置该参数, 将使用和dataDir相同的设置.
clientPort: 监听client连接的端口号.
initLimit: zookeeper集群中的包含多台server, 其中一台为leader, 集群中其余的server为follower. initLimit参数配置初始化连接时, follower和leader之间的最长心跳时间. 此时该参数设置为5, 说明时间限制为5倍tickTime, 即5*2000=10000ms=10s.
syncLimit: 该参数配置leader和follower之间发送消息, 请求和应答的最大时间长度. 此时该参数设置为2, 说明时间限制为2倍tickTime, 即4000ms.
server.X=A:B:C 其中X是一个数字, 表示这是第几号server. A是该server所在的IP地址. B配置该server和集群中的leader交换消息所使用的端口. C配置选举leader时所使用的端口. 由于配置的是伪集群模式, 所以各个server的B, C参数必须不同.



启动zookeeper
bin/zkServer.sh start  

 Server启动之后, 就可以启动client连接server了, 执行脚本:
bin/zkCli.sh -server localhost:2181 






wget https://github.com/apache/storm/archive/v1.1.1.tar.gz

wget http://mirror.bit.edu.cn/apache/storm/apache-storm-1.1.1/apache-storm-1.1.1.tar.gz 
tar -xzvf apache-storm-1.1.1.tar.gz 
rm  apache-storm-1.1.1.tar.gz 

mkdir -p data/storm logs/storm

cd apache-storm-1.1.1
vi conf/storm.yaml

1) storm.zookeeper.servers：这是 Storm 关联的 ZooKeeper 集群的地址列表，
注意，如果你使用的 ZooKeeper 集群的端口不是默认端口，你还需要相应地配置 storm.zookeeper.port。

2) storm.local.dir：Nimbus 和 Supervisor 后台进程都需要一个用于存放一些状态数据（比如 jar 包、配置文件等等）的目录。你可以在每个机器上创建好这个目录，赋予相应的读写权限，并将该目录写入配置文件中 

3) nimbus.host：集群的工作节点需要知道集群中的哪台机器是主机，以便从主机上下载拓扑以及配置文件 

4) supervisor.slots.ports：你需要通过此配置项配置每个 Supervisor 机器能够运行的工作进程（worker）数。每个 worker 都需要一个单独的端口来接收消息，这个配置项就定义了 worker 可以使用的端口列表。如果你在这里定义了 5 个端口，那么 Storm 就会在该机器上分配最多 5 个worker。如果定义 3 个端口，那 Storm 至多只会运行三个 worker。此项的默认值是 6700、6701、6702、6703 四个端口  
ui.port: storm ui端口号，默认8080


storm.zookeeper.servers:
  - "192.168.195.211"
  - "192.168.195.212"
  - "192.168.195.213"
 

storm.local.dir: "/home/server/data/storm"

nimbus.host: "192.168.195.211"

supervisor.slots.ports:
    - 6700
    - 6701
    - 6702
    - 6703

ui.port: 18080


sudo firewall-cmd --zone=public --add-port=18080/tcp --permanent  
firewall-cmd --reload

 配置外部库与环境变量（可选）
如果你需要使用某些外部库或者定制插件的功能，你可以将相关 jar 包放入 extlib/ 与 extlib-daemon 目录下。注意，extlib-daemon 目录仅仅用于存储后台进程（Nimbus，Supervisor，DRPC，UI，Logviewer）所需的 jar 包，例如，HDFS 以及定制的调度库。另外，可以使用STORM_EXT_CLASSPATH 和 STORM_EXT_CLASSPATH_DAEMON 两个环境变量来配置普通外部库与“仅用于后台进程”外部库的 classpath



使用 “storm” 脚本启动后台进程
最后一步是启动所有的 Storm 后台进程。注意，这些进程必须在严格监控下运行。因为 Storm 是个与 ZooKeeper 相似的快速失败系统，其进程很容易被各种异常错误终止。之所以设计成这种模式，是为了确保 Storm 进程可以在任何时刻安全地停止并且在进程重新启动之后恢复征程。这也是 Storm 不在处理过程中保存任何状态的原因 —— 在这种情况下，如果有 Nimbus 或者 Supervisor 重新启动，运行中的拓扑不会受到任何影响。下面是启动后台进程的方法：

Nimbus：在 master 机器上，在监控下执行 bin/storm nimbus 命令。
Supervisor：在每个工作节点上，在监控下执行 bin/storm supervisor 命令。Supervisor 的后台进程主要负责启动/停止该机器上的 worker 进程。
UI：在 master 机器上，在监控下执行 bin/storm ui 命令启动 Storm UI（Storm UI 是一个可以在浏览器中方便地监控集群与拓扑运行状况的站点）后台进程。可以通过 http://{nimbus.host}:8080 来访问 UI 站点。
可以看出，启动后台进程非常简单。同时，各个后台进程也会将日志信息记录到 Storm 安装程序的 logs/ 目录中（这是 Storm 的默认设置，日志文件的路径与相关配置信息可以在 {STORM_HOME}/logback/cluster.xml 文件中修改 —— 译者注）。



flume 安装未成功

wget http://mirror.bit.edu.cn/apache/flume/1.5.2/apache-flume-1.5.2-bin.tar.gz 
tar -xzvf apache-flume-1.5.2-bin.tar.gz 
rm apache-flume-1.5.2-bin.tar.gz 
cd apache-flume-1.5.2-bin

mkdir -p tmp/flume data/flume logs/flume

vi ~/.bash_profile
export FLUME_HOME=/home/server/apache-flume-1.5.2-bin
export PATH=$PATH:$FLUME_HOME/bin
source ~/.bash_profile

cd $FLUME_HOME
cp conf/flume-conf.properties.template conf/flume-conf.properties
vi conf/flume-conf.properties   添加agent1
#agent1 name
agent1.sources=source1
agent1.sinks=sink1
agent1.channels=channel1


#Spooling Directory 监控目录
#set source1
agent1.sources.source1.type=spooldir
agent1.sources.source1.spoolDir=/home/server/hadoop-2.7.4/logs
agent1.sources.source1.channels=channel1
agent1.sources.source1.fileHeader = false
agent1.sources.source1.interceptors = i1
agent1.sources.source1.interceptors.i1.type = timestamp

#set sink1
agent1.sinks.sink1.type=hdfs
agent1.sinks.sink1.hdfs.path=/home/server/data/flume
agent1.sinks.sink1.hdfs.fileType=DataStream
agent1.sinks.sink1.hdfs.writeFormat=TEXT
agent1.sinks.sink1.hdfs.rollInterval=1
agent1.sinks.sink1.channel=channel1
agent1.sinks.sink1.hdfs.filePrefix=%Y-%m-%d

#set channel1
agent1.channels.channel1.type=file
agent1.channels.channel1.checkpointDir=/home/server/tmp/flume/point
agent1.channels.channel1.dataDirs=/home/server/tmp/flume

cp conf/flume-env.sh.template conf/flume-env.sh
vi conf/flume-env.sh
JAVA_HOME=/opt/java/jdk1.8.0_73

启动
$FLUME_HOME/bin/flume-ng agent -n agent1 -c conf -f conf/flume-conf.properties -Dflume.root.logger=DEBUG,console








Elasticsearch：需要2g内存


Logstash：处理传入日志的Logstash的服务器组件
Elasticsearch：存储所有日志
Kibana：用于搜索和可视化日志的Web界面，将通过Nginx
Filebeat代理：安装在将其日志发送到Logstash的客户端服务器，Filebeat充当日志传送代理，利用伐木工具网络协议与Logstash进行通信






rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch
 
echo '[elasticsearch-5.x]
name=Elasticsearch repository for 5.x packages
baseurl=https://artifacts.elastic.co/packages/5.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
' | sudo tee /etc/yum.repos.d/elasticsearch.repo


yum makecache
yum install elasticsearch -y


vi /etc/sysconfig/elasticsearch
JAVA_HOME=/opt/java/jdk1.8.0_73


要将Elasticsearch配置为在系统引导时自动启动，请运行以下命令：
sudo /bin/systemctl daemon-reload
sudo /bin/systemctl enable elasticsearch.service
 
Elasticsearch可以按如下方式启动和停止：
sudo systemctl start elasticsearch.service
sudo systemctl stop elasticsearch.service




sudo firewall-cmd --zone=public --add-port=9200/tcp --permanent  
firewall-cmd --reload



这些命令不会提供有关Elasticsearch是否已成功启动的反馈。相反，此信息将写入位于/ var / log / elasticsearch /中的日志文件中。

默认情况下，Elasticsearch服务不会记录systemd日志中的信息。要启用journalctl日志记录，必须从elasticsearch中的ExecStart命令行中删除–quiet选项。服务文件。
# 注释24行的 --quiet \
vim /etc/systemd/system/multi-user.target.wants/elasticsearch.service
 
当启用systemd日志记录时，使用journalctl命令可以获得日志记录信息：
使用tail查看journal：
sudo journalctl -f
 
要列出elasticsearch服务的日记帐分录：
sudo journalctl --unit elasticsearch
 
要从给定时间开始列出elasticsearch服务的日记帐分录：
sudo journalctl --unit elasticsearch --since  "2017-1-4 10:17:16"

# since 表示指定时间之前的记录
 
使用man journalctl 查看journalctl 更多使用方法
检查Elasticsearch是否正在运行

您可以通过向localhost上的端口9200发送HTTP请求来测试Elasticsearch节点是否正在运行：
curl -XGET 'localhost:9200/?pretty'
 
我们能得到下面这样的回显：
{
  "name" : "De-LRNO",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "DeJzplWhQQK5uGitXr8jjA",
  "version" : {
    "number" : "5.1.1",
    "build_hash" : "5395e21",
    "build_date" : "2016-12-06T12:36:15.409Z",
    "build_snapshot" : false,
    "lucene_version" : "6.3.0"
  },
  "tagline" : "You Know, for Search"
}
 
配置 Elasticsearch

Elasticsearch 从默认的/etc/elasticsearch/elasticsearch.yml加载配置文件， 
配置文件的格式考： 
https://www.elastic.co/guide/en/elasticsearch/reference/current/settings.html
[root@linuxprobe ~]# egrep -v "^#|^$" /etc/elasticsearch/elasticsearch.yml 
[root@linuxprobe ~]# egrep -v "^#|^$" /etc/elasticsearch/elasticsearch.yml
node.name: node-1
path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch
network.host: 10.1.1.53  # 默认localhost，自定义为ip
http.port: 9200
 
RPM还具有系统配置文件（/etc/sysconfig/elasticsearch），允许您设置以下参数：
[root@linuxprobe elasticsearch]# egrep -v "^#|^$" /etc/sysconfig/elasticsearch 
ES_HOME=/usr/share/elasticsearch
JAVA_HOME=/usr/java/jdk1.8.0_111
CONF_DIR=/etc/elasticsearch
DATA_DIR=/var/lib/elasticsearch
LOG_DIR=/var/log/elasticsearch
PID_DIR=/var/run/elasticsearch
 
日志配置

Elasticsearch使用Log4j 2进行日志记录。 Log4j 2可以使用log4j2配置。属性文件。 Elasticsearch公开单个属性$ {sys：es。日志}，可以在配置文件中引用以确定日志文件的位置;这将在运行时解析为Elasticsearch日志文件的前缀。

例如，如果您的日志目录是/var/log/elasticsearch并且您的集群名为production，那么$ {sys：es。 logs}将解析为/var/log/elasticsearch/production。

默认日志配置存在：/etc/elasticsearch/log4j2.properties









rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch

echo '[kibana-5.x]
name=Kibana repository for 5.x packages
baseurl=https://artifacts.elastic.co/packages/5.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
' | sudo tee /etc/yum.repos.d/kibana.repo

yum makecache && yum install kibana -y



使用systemd运行Kibana

要将Kibana配置为在系统引导时自动启动，请运行以下命令：
sudo /bin/systemctl daemon-reload
sudo /bin/systemctl enable kibana.service
 
Kibana可以如下启动和停止
sudo systemctl start kibana.service
sudo systemctl stop kibana.service
 
配置Kibana

Kibana默认从/etc/kibana/kibana.yml文件加载其配置。 


sudo firewall-cmd --zone=public --add-port=5601/tcp --permanent  
firewall-cmd --reload

http://192.168.195.210:5601 





echo '[nginx]
name=nginx repo
baseurl=http://nginx.org/packages/centos/$releasever/$basearch/
gpgcheck=0
enabled=1
' | sudo tee /etc/yum.repos.d/nginx.repo


yum install nginx httpd-tools -y
使用htpasswd创建一个名为“kibanaadmin”的管理员用户（可以使用其他名称），该用户可以访问Kibana Web界面：
htpasswd -c /etc/nginx/htpasswd.users kibanaadmin

egrep -v "#|^$" /etc/nginx/conf.d/kibana.conf 
server {
    listen       80;
    server_name  kibana.aniu.co;
    access_log  /var/log/nginx/kibana.aniu.co.access.log main;
    error_log   /var/log/nginx/kibana.aniu.co.access.log;
    auth_basic "Restricted Access";
    auth_basic_user_file /etc/nginx/htpasswd.users;
    location / {
        proxy_pass http://localhost:5601;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;        
    }
}

# 启动nginx并验证配置

sudo systemctl start nginx
sudo systemctl enable nginx

SELinux已禁用。如果不是这样，您可能需要运行以下命令使Kibana正常工作： 
sudo setsebool -P httpd_can_network_connect 1







安装Logstash

创建Logstash源
# 导入公共签名密钥
rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch

# 将以下内容添加到具有.repo后缀的文件中的/etc/yum.repos.d/目录中，如logstash.repo
echo '[logstash-5.x]
name=Elastic repository for 5.x packages
baseurl=https://artifacts.elastic.co/packages/5.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
' | sudo tee /etc/yum.repos.d/logstash.repo
 
使用yum安装logstash
yum makecache && yum install logstash -y
 






 
生成SSL证书

由于我们将使用Filebeat将日志从我们的客户端服务器发送到我们的ELK服务器，我们需要创建一个SSL证书和密钥对。 Filebeat使用该证书来验证ELK Server的身份。使用以下命令创建将存储证书和私钥的目录：

使用以下命令（在ELK服务器的FQDN中替换）在适当的位置（/etc/pki/tls/ …）中生成SSL证书和私钥：
cd /etc/pki/tls
sudo openssl req -subj '/CN=ELK_server_fqdn/' -x509 -days 3650 -batch -nodes -newkey rsa:2048 -keyout private/logstash-forwarder.key -out certs/logstash-forwarder.crt

# 注：ELK_server_fqdn自定义，示例如下：
[root@linuxprobe ~]# cd /etc/pki/tls
[root@linuxprobe tls]# sudo openssl req -subj '/CN=kibana.aniu.co/' -x509 -days 3650 -batch -nodes -newkey rsa:2048 -keyout private/logstash-forwarder.key -out certs/logstash-forwarder.crt
Generating a 2048 bit RSA private key
.+++
...........................................................................................................+++
writing new private key to 'private/logstash-forwarder.key'
-----
 
logstash-forwarder.crt文件将被复制到，所有将日志发送到Logstash的服务器
配置Logstash

Logstash配置文件为JSON格式，驻留在/etc/logstash/conf.d中。该配置由三个部分组成：输入，过滤和输出。

创建一个名为01-beats-input.conf的配置文件，并设置我们的“filebeat”输入：
sudo vi /etc/logstash/conf.d/01-beats-input.conf
 
插入以下输入配置
input {
  beats {
    port => 5044
    ssl => true
    ssl_certificate => "/etc/pki/tls/certs/logstash-forwarder.crt"
    ssl_key => "/etc/pki/tls/private/logstash-forwarder.key"
  }
}
 
保存退出，监听TCP 5044端口上beats 输入，使用上面创建的SSL证书加密

创建一个名为10-syslog-filter.conf的配置文件，我们将为syslog消息添加一个过滤器：
sudo vim /etc/logstash/conf.d/10-syslog-filter.conf
 
插入以下输入配置
filter {
  if [type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
      add_field => [ "received_at", "%{@timestamp}" ]
      add_field => [ "received_from", "%{host}" ]
    }
    syslog_pri { }
    date {
      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }
  }
}
 
save和quit。此过滤器查找标记为“syslog”类型（由Filebeat）的日志，并且将尝试使用grok解析传入的syslog日志，以使其结构化和可查询。
创建一个名为logstash-simple的配置文件,示例文件：
vim /etc/logstash/conf.d/logstash-simple.conf
1
插入以下输入配置
input { stdin { } }
output {
  elasticsearch { hosts => ["localhost:9200"] }
  stdout { codec => rubydebug }
}
 
这个输出基本上配置Logstash来存储input数据到Elasticsearch中，运行在localhost：9200
运行Logstash使用Systemd

sudo systemctl start logstash.service
sudo systemctl enable logstash.service
 
注： 到这里可能会出现Logstash重启失败，等问题，查看日志，锁定具体问题，一般不会出错
加载Kibana仪表板

Elastic提供了几个样例Kibana仪表板和Beats索引模式，可以帮助我们开始使用Kibana。虽然我们不会在本教程中使用仪表板，我们仍将加载它们，以便我们可以使用它包括的Filebeat索引模式。

首先，将示例仪表板归档下载到您的主目录：
cd /usr/local/src
curl -L -O https://download.elastic.co/beats/dashboards/beats-dashboards-1.1.0.zip
 
安装unzip包，解压beats
sudo yum -y install unzip
unzip beats-dashboards-*.zip
./load.sh
 
这些是我们刚加载的索引模式：
[packetbeat-]YYYY.MM.DD
[topbeat-]YYYY.MM.DD
[filebeat-]YYYY.MM.DD
[winlogbeat-]YYYY.MM.DD
我们开始使用Kibana时，我们将选择Filebeat索引模式作为默认值。
在Elasticsearch中加载Filebeat索引模板

因为我们计划使用Filebeat将日志发送到Elasticsearch，我们应该加载Filebeat索引模板。索引模板将配置Elasticsearch以智能方式分析传入的Filebeat字段。

首先，将Filebeat索引模板下载到您的主目录：
cd /usr/local/src
curl -O https://gist.githubusercontent.com/thisismitch/3429023e8438cc25b86c/raw/d8c479e2a1adcea8b1fe86570e42abab0f10f364/filebeat-index-template.json
 
然后使用此命令加载模板：
# 注：执行命令的位置和json模板相同

[root@linuxprobe src]# curl -XPUT 'http://localhost:9200/_template/filebeat?pretty' -d@filebeat-index-template.json
{
  "acknowledged" : true
}
 
现在我们的ELK服务器已准备好接收Filebeat数据，移动到在每个客户端服务器上设置Filebeat。
设置Filebeat（添加客户端服务器）

对于要将日志发送到ELK服务器的每个CentOS或RHEL 7服务器，请执行以下步骤。
复制ssl证书

在ELK服务器上，将先决条件教程中创建的SSL证书复制到客户端服务器：
# 使用SCP远程实现复制
yum -y install openssh-clinets

# 
scp /etc/pki/tls/certs/logstash-forwarder.crt root@linux-node1:/tmp

# 注：如果不适用ip，记得在ELK服务器上设置hosts
 
在提供您的登录凭据后，请确保证书复制成功。它是客户端服务器和ELK服务器之间的通信所必需的,在客户端服务器上，将ELK服务器的SSL证书复制到适当的位置（/etc/pki/tls/certs）:
[root@linux-node1 ~]# sudo mkdir -p /etc/pki/tls/certs
[root@linux-node1 ~]# sudo cp /tmp/logstash-forwarder.crt /etc/pki/tls/certs/
 
安装Filebeat包

在客户端服务器上，创建运行以下命令将Elasticsearch公用GPG密钥导入rpm：参考上面：
sudo rpm --import http://packages.elastic.co/GPG-KEY-elasticsearch
#
echo '[elasticsearch-5.x]
name=Elasticsearch repository for 5.x packages
baseurl=https://artifacts.elastic.co/packages/5.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
' | sudo tee /etc/yum.repos.d/elasticsearch.repo
 
源创建完成之后使用yum安装filebeat
yum makecache && yum install filebeat -y
sudo chkconfig --add filebeat
 
配置filebeat

[root@linux-node1 ~]# egrep -v "#|^$" /etc/filebeat/filebeat.yml 
filebeat.prospectors:
- input_type: log
  paths:
    - /var/log/secure         # 新增
    - /var/log/messages       # 新增
    - /var/log/*.log
output.elasticsearch:
  hosts: ["localhost:9200"]
output.logstash:
  hosts: ["kibana.aniu.co:5044"]   # 修改为ELK上Logstash的连接方式
  ssl.certificate_authorities: ["/etc/pki/tls/certs/logstash-forwarder.crt"] # 新增
 
Filebeat的配置文件是YAML格式的，注意缩进
启动filebeat
sudo systemctl start filebeat
sudo systemctl enable filebeat
 
注：客户端前提是已经配置完成elasticsearch服务，并且设置好域名解析 
filebeat启动完成后，可以观察ELK上面的journalctl -f和logstash，以及客户端的filebeat日志，查看filebeat是否生效
连接Kibana