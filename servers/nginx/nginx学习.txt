Nginx 特点

Nginx 做为 HTTP 服务器，有以下几项基本特性：

处理静态文件，索引文件以及自动索引；打开文件描述符缓冲．

无缓存的反向代理加速，简单的负载均衡和容错．

FastCGI，简单的负载均衡和容错．

模块化的结构。包括 gzipping, byte ranges, chunked responses,以及 SSI-filter 等 filter。如果由 FastCGI 或其它代理服务器处理单页中存在的多个 SSI，则这项处理可以并行运行，而不需要相互等待。

支持 SSL 和 TLSSNI．
Nginx 专为性能优化而开发，性能是其最重要的考量,实现上非常注重效率 。它支持内核 Poll 模型，能经受高负载的考验,有报告表明能支持高达 50,000 个并发连接数。

Nginx 具有很高的稳定性。其它 HTTP 服务器，当遇到访问的峰值，或者有人恶意发起慢速连接时，也很可能会导致服务器物理内存耗尽频繁交换，失去响应，只能重启服务器。例如当前 apache 一旦上到 200 个以上进程，web响应速度就明显非常缓慢了。而 Nginx 采取了分阶段资源分配技术，使得它的 CPU 与内存占用率非常低。Nginx 官方表示保持 10,000 个没有活动的连接，它只占 2.5M 内存，所以类似 DOS 这样的攻击对 Nginx 来说基本上是毫无用处的。就稳定性而言,Nginx 比 lighthttpd 更胜一筹。

Nginx 支持热部署。它的启动特别容易, 并且几乎可以做到 7*24 不间断运行，即使运行数个月也不需要重新启动。你还能够在不间断服务的情况下，对软件版本进行进行升级。

Nginx 采用 master-slave 模型,能够充分利用 SMP 的优势，且能够减少工作进程在磁盘 I/O 的阻塞延迟。当采用 select()/poll() 调用时，还可以限制每个进程的连接数。

Nginx 代码质量非常高，代码很规范，手法成熟，模块扩展也很容易。特别值得一提的是强大的 Upstream 与 Filter 链。Upstream 为诸如 reverse proxy,与其他服务器通信模块的编写奠定了很好的基础。而 Filter 链最酷的部分就是各个 filter 不必等待前一个 filter 执行完毕。它可以把前一个 filter 的输出做为当前 filter 的输入，这有点像 Unix 的管线。这意味着，一个模块可以开始压缩从后端服务器发送过来的请求，且可以在模块接收完后端服务器的整个请求之前把压缩流转向客户端。

Nginx 采用了一些 os 提供的最新特性如对 sendfile (Linux2.2+)，accept-filter (FreeBSD4.1+)，TCP_DEFER_ACCEPT (Linux 2.4+)的支持，从而大大提高了性能。

当然，Nginx 还很年轻，多多少少存在一些问题，比如：Nginx 是俄罗斯人创建，虽然前几年文档比较少，但是目前文档方面比较全面，英文资料居多，中文的资料也比较多，而且有专门的书籍和资料可供查找。

Nginx 的作者和社区都在不断的努力完善，我们有理由相信 Nginx 将继续以高速的增长率来分享轻量级 HTTP 服务器市场，会有一个更美好的未来。



初探 Nginx 架构

众所周知，Nginx 性能高，而 Nginx 的高性能与其架构是分不开的。那么 Nginx 究竟是怎么样的呢？这一节我们先来初识一下 Nginx 框架吧。

Nginx 在启动后，在 unix 系统中会以 daemon 的方式在后台运行，后台进程包含一个 master 进程和多个 worker 进程。我们也可以手动地关掉后台模式，让 Nginx 在前台运行，并且通过配置让 Nginx 取消 master 进程，从而可以使 Nginx 以单进程方式运行。很显然，生产环境下我们肯定不会这么做，所以关闭后台模式，一般是用来调试用的，在后面的章节里面，我们会详细地讲解如何调试 Nginx。所以，我们可以看到，Nginx 是以多进程的方式来工作的，当然 Nginx 也是支持多线程的方式的，只是我们主流的方式还是多进程的方式，也是 Nginx 的默认方式。Nginx 采用多进程的方式有诸多好处，所以我就主要讲解 Nginx 的多进程模式吧。

刚才讲到，Nginx 在启动后，会有一个 master 进程和多个 worker 进程。master 进程主要用来管理 worker 进程，包含：接收来自外界的信号，向各 worker 进程发送信号，监控 worker 进程的运行状态，当 worker 进程退出后(异常情况下)，会自动重新启动新的 worker 进程。而基本的网络事件，则是放在 worker 进程中来处理了。多个 worker 进程之间是对等的，他们同等竞争来自客户端的请求，各进程互相之间是独立的。一个请求，只可能在一个 worker 进程中处理，一个 worker 进程，不可能处理其它进程的请求。worker 进程的个数是可以设置的，一般我们会设置与机器cpu核数一致，这里面的原因与 Nginx 的进程模型以及事件处理模型是分不开的。Nginx 的进程模型，可以由下图来表示：



在 Nginx 启动后，如果我们要操作 Nginx，要怎么做呢？从上文中我们可以看到，master 来管理 worker 进程，所以我们只需要与 master 进程通信就行了。master 进程会接收来自外界发来的信号，再根据信号做不同的事情。所以我们要控制 Nginx，只需要通过 kill 向 master 进程发送信号就行了。比如kill -HUP pid，则是告诉 Nginx，从容地重启 Nginx，我们一般用这个信号来重启 Nginx，或重新加载配置，因为是从容地重启，因此服务是不中断的。master 进程在接收到 HUP 信号后是怎么做的呢？首先 master 进程在接到信号后，会先重新加载配置文件，然后再启动新的 worker 进程，并向所有老的 worker 进程发送信号，告诉他们可以光荣退休了。新的 worker 在启动后，就开始接收新的请求，而老的 worker 在收到来自 master 的信号后，就不再接收新的请求，并且在当前进程中的所有未处理完的请求处理完成后，再退出。当然，直接给 master 进程发送信号，这是比较老的操作方式，Nginx 在 0.8 版本之后，引入了一系列命令行参数，来方便我们管理。比如，./nginx -s reload，就是来重启 Nginx，./nginx -s stop，就是来停止 Nginx 的运行。如何做到的呢？我们还是拿 reload 来说，我们看到，执行命令时，我们是启动一个新的 Nginx 进程，而新的 Nginx 进程在解析到 reload 参数后，就知道我们的目的是控制 Nginx 来重新加载配置文件了，它会向 master 进程发送信号，然后接下来的动作，就和我们直接向 master 进程发送信号一样了。

现在，我们知道了当我们在操作 Nginx 的时候，Nginx 内部做了些什么事情，那么，worker 进程又是如何处理请求的呢？我们前面有提到，worker 进程之间是平等的，每个进程，处理请求的机会也是一样的。当我们提供 80 端口的 http 服务时，一个连接请求过来，每个进程都有可能处理这个连接，怎么做到的呢？首先，每个 worker 进程都是从 master 进程 fork 过来，在 master 进程里面，先建立好需要 listen 的 socket（listenfd）之后，然后再 fork 出多个 worker 进程。所有 worker 进程的 listenfd 会在新连接到来时变得可读，为保证只有一个进程处理该连接，所有 worker 进程在注册 listenfd 读事件前抢 accept_mutex，抢到互斥锁的那个进程注册 listenfd 读事件，在读事件里调用 accept 接受该连接。当一个 worker 进程在 accept 这个连接之后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，这样一个完整的请求就是这样的了。我们可以看到，一个请求，完全由 worker 进程来处理，而且只在一个 worker 进程中处理。

那么，Nginx 采用这种进程模型有什么好处呢？当然，好处肯定会很多了。首先，对于每个 worker 进程来说，独立的进程，不需要加锁，所以省掉了锁带来的开销，同时在编程以及问题查找时，也会方便很多。其次，采用独立的进程，可以让互相之间不会影响，一个进程退出后，其它进程还在工作，服务不会中断，master 进程则很快启动新的 worker 进程。当然，worker 进程的异常退出，肯定是程序有 bug 了，异常退出，会导致当前 worker 上的所有请求失败，不过不会影响到所有请求，所以降低了风险。当然，好处还有很多，大家可以慢慢体会。

上面讲了很多关于 Nginx 的进程模型，接下来，我们来看看 Nginx 是如何处理事件的。

有人可能要问了，Nginx 采用多 worker 的方式来处理请求，每个 worker 里面只有一个主线程，那能够处理的并发数很有限啊，多少个 worker 就能处理多少个并发，何来高并发呢？非也，这就是 Nginx 的高明之处，Nginx 采用了异步非阻塞的方式来处理请求，也就是说，Nginx 是可以同时处理成千上万个请求的。想想 apache 的常用工作方式（apache 也有异步非阻塞版本，但因其与自带某些模块冲突，所以不常用），每个请求会独占一个工作线程，当并发数上到几千时，就同时有几千的线程在处理请求了。这对操作系统来说，是个不小的挑战，线程带来的内存占用非常大，线程的上下文切换带来的 cpu 开销很大，自然性能就上不去了，而这些开销完全是没有意义的。

为什么 Nginx 可以采用异步非阻塞的方式来处理呢，或者异步非阻塞到底是怎么回事呢？我们先回到原点，看看一个请求的完整过程。首先，请求过来，要建立连接，然后再接收数据，接收数据后，再发送数据。具体到系统底层，就是读写事件，而当读写事件没有准备好时，必然不可操作，如果不用非阻塞的方式来调用，那就得阻塞调用了，事件没有准备好，那就只能等了，等事件准备好了，你再继续吧。阻塞调用会进入内核等待，cpu 就会让出去给别人用了，对单线程的 worker 来说，显然不合适，当网络事件越多时，大家都在等待呢，cpu 空闲下来没人用，cpu利用率自然上不去了，更别谈高并发了。好吧，你说加进程数，这跟apache的线程模型有什么区别，注意，别增加无谓的上下文切换。所以，在 Nginx 里面，最忌讳阻塞的系统调用了。不要阻塞，那就非阻塞喽。非阻塞就是，事件没有准备好，马上返回 EAGAIN，告诉你，事件还没准备好呢，你慌什么，过会再来吧。好吧，你过一会，再来检查一下事件，直到事件准备好了为止，在这期间，你就可以先去做其它事情，然后再来看看事件好了没。虽然不阻塞了，但你得不时地过来检查一下事件的状态，你可以做更多的事情了，但带来的开销也是不小的。所以，才会有了异步非阻塞的事件处理机制，具体到系统调用就是像 select/poll/epoll/kqueue 这样的系统调用。它们提供了一种机制，让你可以同时监控多个事件，调用他们是阻塞的，但可以设置超时时间，在超时时间之内，如果有事件准备好了，就返回。这种机制正好解决了我们上面的两个问题，拿 epoll 为例(在后面的例子中，我们多以 epoll 为例子，以代表这一类函数)，当事件没准备好时，放到 epoll 里面，事件准备好了，我们就去读写，当读写返回 EAGAIN 时，我们将它再次加入到 epoll 里面。这样，只要有事件准备好了，我们就去处理它，只有当所有事件都没准备好时，才在 epoll 里面等着。这样，我们就可以并发处理大量的并发了，当然，这里的并发请求，是指未处理完的请求，线程只有一个，所以同时能处理的请求当然只有一个了，只是在请求间进行不断地切换而已，切换也是因为异步事件未准备好，而主动让出的。这里的切换是没有任何代价，你可以理解为循环处理多个准备好的事件，事实上就是这样的。与多线程相比，这种事件处理方式是有很大的优势的，不需要创建线程，每个请求占用的内存也很少，没有上下文切换，事件处理非常的轻量级。并发数再多也不会导致无谓的资源浪费（上下文切换）。更多的并发数，只是会占用更多的内存而已。 我之前有对连接数进行过测试，在 24G 内存的机器上，处理的并发请求数达到过 200 万。现在的网络服务器基本都采用这种方式，这也是nginx性能高效的主要原因。

我们之前说过，推荐设置 worker 的个数为 cpu 的核数，在这里就很容易理解了，更多的 worker 数，只会导致进程来竞争 cpu 资源了，从而带来不必要的上下文切换。而且，nginx为了更好的利用多核特性，提供了 cpu 亲缘性的绑定选项，我们可以将某一个进程绑定在某一个核上，这样就不会因为进程的切换带来 cache 的失效。像这种小的优化在 Nginx 中非常常见，同时也说明了 Nginx 作者的苦心孤诣。比如，Nginx 在做 4 个字节的字符串比较时，会将 4 个字符转换成一个 int 型，再作比较，以减少 cpu 的指令数等等。

现在，知道了 Nginx 为什么会选择这样的进程模型与事件模型了。对于一个基本的 Web 服务器来说，事件通常有三种类型，网络事件、信号、定时器。从上面的讲解中知道，网络事件通过异步非阻塞可以很好的解决掉。如何处理信号与定时器？

首先，信号的处理。对 Nginx 来说，有一些特定的信号，代表着特定的意义。信号会中断掉程序当前的运行，在改变状态后，继续执行。如果是系统调用，则可能会导致系统调用的失败，需要重入。关于信号的处理，大家可以学习一些专业书籍，这里不多说。对于 Nginx 来说，如果nginx正在等待事件（epoll_wait 时），如果程序收到信号，在信号处理函数处理完后，epoll_wait 会返回错误，然后程序可再次进入 epoll_wait 调用。

另外，再来看看定时器。由于 epoll_wait 等函数在调用的时候是可以设置一个超时时间的，所以 Nginx 借助这个超时时间来实现定时器。nginx里面的定时器事件是放在一颗维护定时器的红黑树里面，每次在进入 epoll_wait前，先从该红黑树里面拿到所有定时器事件的最小时间，在计算出 epoll_wait 的超时时间后进入 epoll_wait。所以，当没有事件产生，也没有中断信号时，epoll_wait 会超时，也就是说，定时器事件到了。这时，nginx会检查所有的超时事件，将他们的状态设置为超时，然后再去处理网络事件。由此可以看出，当我们写 Nginx 代码时，在处理网络事件的回调函数时，通常做的第一个事情就是判断超时，然后再去处理网络事件。

我们可以用一段伪代码来总结一下 Nginx 的事件处理模型：

    while (true) {
        for t in run_tasks:
            t.handler();
        update_time(&now);
        timeout = ETERNITY;
        for t in wait_tasks: /* sorted already */
            if (t.time <= now) {
                t.timeout_handler();
            } else {
                timeout = t.time - now;
                break;
            }
        nevents = poll_function(events, timeout);
        for i in nevents:
            task t;
            if (events[i].type == READ) {
                t.handler = read_handler;
            } else { /* events[i].type == WRITE */
                t.handler = write_handler;
            }
            run_tasks_add(t);
    }
好，本节我们讲了进程模型，事件模型，包括网络事件，信号，定时器事件。




Nginx 基础概念

connection
在 Nginx 中 connection 就是对 tcp 连接的封装，其中包括连接的 socket，读事件，写事件。利用 Nginx 封装的 connection，我们可以很方便的使用 Nginx 来处理与连接相关的事情，比如，建立连接，发送与接受数据等。而 Nginx 中的 http 请求的处理就是建立在 connection之上的，所以 Nginx 不仅可以作为一个web服务器，也可以作为邮件服务器。当然，利用 Nginx 提供的 connection，我们可以与任何后端服务打交道。

结合一个 tcp 连接的生命周期，我们看看 Nginx 是如何处理一个连接的。首先，Nginx 在启动时，会解析配置文件，得到需要监听的端口与 ip 地址，然后在 Nginx 的 master 进程里面，先初始化好这个监控的 socket(创建 socket，设置 addrreuse 等选项，绑定到指定的 ip 地址端口，再 listen)，然后再 fork 出多个子进程出来，然后子进程会竞争 accept 新的连接。此时，客户端就可以向 Nginx 发起连接了。当客户端与服务端通过三次握手建立好一个连接后，Nginx 的某一个子进程会 accept 成功，得到这个建立好的连接的 socket，然后创建 Nginx 对连接的封装，即 ngx_connection_t 结构体。接着，设置读写事件处理函数并添加读写事件来与客户端进行数据的交换。最后，Nginx 或客户端来主动关掉连接，到此，一个连接就寿终正寝了。

当然，Nginx 也是可以作为客户端来请求其它 server 的数据的（如 upstream 模块），此时，与其它 server 创建的连接，也封装在 ngx_connection_t 中。作为客户端，Nginx 先获取一个 ngx_connection_t 结构体，然后创建 socket，并设置 socket 的属性（ 比如非阻塞）。然后再通过添加读写事件，调用 connect/read/write 来调用连接，最后关掉连接，并释放 ngx_connection_t。

在 Nginx 中，每个进程会有一个连接数的最大上限，这个上限与系统对 fd 的限制不一样。在操作系统中，通过 ulimit -n，我们可以得到一个进程所能够打开的 fd 的最大数，即 nofile，因为每个 socket 连接会占用掉一个 fd，所以这也会限制我们进程的最大连接数，当然也会直接影响到我们程序所能支持的最大并发数，当 fd 用完后，再创建 socket 时，就会失败。Nginx 通过设置 worker_connectons 来设置每个进程支持的最大连接数。如果该值大于 nofile，那么实际的最大连接数是 nofile，Nginx 会有警告。Nginx 在实现时，是通过一个连接池来管理的，每个 worker 进程都有一个独立的连接池，连接池的大小是 worker_connections。这里的连接池里面保存的其实不是真实的连接，它只是一个 worker_connections 大小的一个 ngx_connection_t 结构的数组。并且，Nginx 会通过一个链表 free_connections 来保存所有的空闲 ngx_connection_t，每次获取一个连接时，就从空闲连接链表中获取一个，用完后，再放回空闲连接链表里面。

在这里，很多人会误解 worker_connections 这个参数的意思，认为这个值就是 Nginx 所能建立连接的最大值。其实不然，这个值是表示每个 worker 进程所能建立连接的最大值，所以，一个 Nginx 能建立的最大连接数，应该是worker_connections * worker_processes。当然，这里说的是最大连接数，对于 HTTP 请求本地资源来说，能够支持的最大并发数量是worker_connections * worker_processes，而如果是 HTTP 作为反向代理来说，最大并发数量应该是worker_connections * worker_processes/2。因为作为反向代理服务器，每个并发会建立与客户端的连接和与后端服务的连接，会占用两个连接。

那么，我们前面有说过一个客户端连接过来后，多个空闲的进程，会竞争这个连接，很容易看到，这种竞争会导致不公平，如果某个进程得到 accept 的机会比较多，它的空闲连接很快就用完了，如果不提前做一些控制，当 accept 到一个新的 tcp 连接后，因为无法得到空闲连接，而且无法将此连接转交给其它进程，最终会导致此 tcp 连接得不到处理，就中止掉了。很显然，这是不公平的，有的进程有空余连接，却没有处理机会，有的进程因为没有空余连接，却人为地丢弃连接。那么，如何解决这个问题呢？首先，Nginx 的处理得先打开 accept_mutex 选项，此时，只有获得了 accept_mutex 的进程才会去添加accept事件，也就是说，Nginx会控制进程是否添加 accept 事件。Nginx 使用一个叫 ngx_accept_disabled 的变量来控制是否去竞争 accept_mutex 锁。在第一段代码中，计算 ngx_accept_disabled 的值，这个值是 Nginx 单进程的所有连接总数的八分之一，减去剩下的空闲连接数量，得到的这个 ngx_accept_disabled 有一个规律，当剩余连接数小于总连接数的八分之一时，其值才大于 0，而且剩余的连接数越小，这个值越大。再看第二段代码，当 ngx_accept_disabled 大于 0 时，不会去尝试获取 accept_mutex 锁，并且将 ngx_accept_disabled 减 1，于是，每次执行到此处时，都会去减 1，直到小于 0。不去获取 accept_mutex 锁，就是等于让出获取连接的机会，很显然可以看出，当空余连接越少时，ngx_accept_disable 越大，于是让出的机会就越多，这样其它进程获取锁的机会也就越大。不去 accept，自己的连接就控制下来了，其它进程的连接池就会得到利用，这样，Nginx 就控制了多进程间连接的平衡了。

    ngx_accept_disabled = ngx_cycle->connection_n / 8
        - ngx_cycle->free_connection_n;

    if (ngx_accept_disabled > 0) {
        ngx_accept_disabled--;

    } else {
        if (ngx_trylock_accept_mutex(cycle) == NGX_ERROR) {
            return;
        }

        if (ngx_accept_mutex_held) {
            flags |= NGX_POST_EVENTS;

        } else {
            if (timer == NGX_TIMER_INFINITE
                    || timer > ngx_accept_mutex_delay)
            {
                timer = ngx_accept_mutex_delay;
            }
        }
    }
好了，连接就先介绍到这，本章的目的是介绍基本概念，知道在 Nginx 中连接是个什么东西就行了，而且连接是属于比较高级的用法，在后面的模块开发高级篇会有专门的章节来讲解连接与事件的实现及使用。

request
这节我们讲 request，在 Nginx 中我们指的是 http 请求，具体到 Nginx 中的数据结构是ngx_http_request_t。ngx_http_request_t 是对一个 http 请求的封装。 我们知道，一个 http 请求，包含请求行、请求头、请求体、响应行、响应头、响应体。

http 请求是典型的请求-响应类型的的网络协议，而 http 是文本协议，所以我们在分析请求行与请求头，以及输出响应行与响应头，往往是一行一行的进行处理。如果我们自己来写一个 http 服务器，通常在一个连接建立好后，客户端会发送请求过来。然后我们读取一行数据，分析出请求行中包含的 method、uri、http_version 信息。然后再一行一行处理请求头，并根据请求 method 与请求头的信息来决定是否有请求体以及请求体的长度，然后再去读取请求体。得到请求后，我们处理请求产生需要输出的数据，然后再生成响应行，响应头以及响应体。在将响应发送给客户端之后，一个完整的请求就处理完了。当然这是最简单的 webserver 的处理方式，其实 Nginx 也是这样做的，只是有一些小小的区别，比如，当请求头读取完成后，就开始进行请求的处理了。Nginx 通过 ngx_http_request_t 来保存解析请求与输出响应相关的数据。

那接下来，简要讲讲 Nginx 是如何处理一个完整的请求的。对于 Nginx 来说，一个请求是从ngx_http_init_request 开始的，在这个函数中，会设置读事件为 ngx_http_process_request_line，也就是说，接下来的网络事件，会由 ngx_http_process_request_line 来执行。从ngx_http_process_request_line 的函数名，我们可以看到，这就是来处理请求行的，正好与之前讲的，处理请求的第一件事就是处理请求行是一致的。通过 ngx_http_read_request_header 来读取请求数据。然后调用 ngx_http_parse_request_line 函数来解析请求行。Nginx 为提高效率，采用状态机来解析请求行，而且在进行 method 的比较时，没有直接使用字符串比较，而是将四个字符转换成一个整型，然后一次比较以减少 cpu 的指令数，这个前面有说过。很多人可能很清楚一个请求行包含请求的方法，uri，版本，却不知道其实在请求行中，也是可以包含有 host 的。比如一个请求 GET http://www.taobao.com/uri HTTP/1.0 这样一个请求行也是合法的，而且 host 是 www.taobao.com，这个时候，Nginx 会忽略请求头中的 host 域，而以请求行中的这个为准来查找虚拟主机。另外，对于对于 http0.9 版来说，是不支持请求头的，所以这里也是要特别的处理。所以，在后面解析请求头时，协议版本都是 1.0 或 1.1。整个请求行解析到的参数，会保存到 ngx_http_request_t 结构当中。

在解析完请求行后，Nginx 会设置读事件的 handler 为 ngx_http_process_request_headers，然后后续的请求就在 ngx_http_process_request_headers 中进行读取与解析。ngx_http_process_request_headers 函数用来读取请求头，跟请求行一样，还是调用 ngx_http_read_request_header 来读取请求头，调用 ngx_http_parse_header_line 来解析一行请求头，解析到的请求头会保存到 ngx_http_request_t 的域 headers_in 中，headers_in 是一个链表结构，保存所有的请求头。而 HTTP 中有些请求是需要特别处理的，这些请求头与请求处理函数存放在一个映射表里面，即 ngx_http_headers_in，在初始化时，会生成一个 hash 表，当每解析到一个请求头后，就会先在这个 hash 表中查找，如果有找到，则调用相应的处理函数来处理这个请求头。比如:Host 头的处理函数是 ngx_http_process_host。

当 Nginx 解析到两个回车换行符时，就表示请求头的结束，此时就会调用 ngx_http_process_request 来处理请求了。ngx_http_process_request 会设置当前的连接的读写事件处理函数为 ngx_http_request_handler，然后再调用 ngx_http_handler 来真正开始处理一个完整的http请求。这里可能比较奇怪，读写事件处理函数都是ngx_http_request_handler，其实在这个函数中，会根据当前事件是读事件还是写事件，分别调用 ngx_http_request_t 中的 read_event_handler 或者是 write_event_handler。由于此时，我们的请求头已经读取完成了，之前有说过，Nginx 的做法是先不读取请求 body，所以这里面我们设置 read_event_handler 为 ngx_http_block_reading，即不读取数据了。刚才说到，真正开始处理数据，是在 ngx_http_handler 这个函数里面，这个函数会设置 write_event_handler 为 ngx_http_core_run_phases，并执行 ngx_http_core_run_phases 函数。ngx_http_core_run_phases 这个函数将执行多阶段请求处理，Nginx 将一个 http 请求的处理分为多个阶段，那么这个函数就是执行这些阶段来产生数据。因为 ngx_http_core_run_phases 最后会产生数据，所以我们就很容易理解，为什么设置写事件的处理函数为 ngx_http_core_run_phases 了。在这里，我简要说明了一下函数的调用逻辑，我们需要明白最终是调用 ngx_http_core_run_phases 来处理请求，产生的响应头会放在 ngx_http_request_t 的 headers_out 中，这一部分内容，我会放在请求处理流程里面去讲。Nginx 的各种阶段会对请求进行处理，最后会调用 filter 来过滤数据，对数据进行加工，如 truncked 传输、gzip 压缩等。这里的 filter 包括 header filter 与 body filter，即对响应头或响应体进行处理。filter 是一个链表结构，分别有 header filter 与 body filter，先执行 header filter 中的所有 filter，然后再执行 body filter 中的所有 filter。在 header filter 中的最后一个 filter，即 ngx_http_header_filter，这个 filter 将会遍历所有的响应头，最后需要输出的响应头在一个连续的内存，然后调用 ngx_http_write_filter 进行输出。ngx_http_write_filter 是 body filter 中的最后一个，所以 Nginx 首先的 body 信息，在经过一系列的 body filter 之后，最后也会调用 ngx_http_write_filter 来进行输出(有图来说明)。

这里要注意的是，Nginx 会将整个请求头都放在一个 buffer 里面，这个 buffer 的大小通过配置项 client_header_buffer_size 来设置，如果用户的请求头太大，这个 buffer 装不下，那 Nginx 就会重新分配一个新的更大的 buffer 来装请求头，这个大 buffer 可以通过 large_client_header_buffers 来设置，这个 large_buffer 这一组 buffer，比如配置 48k，就是表示有四个 8k 大小的 buffer 可以用。注意，为了保存请求行或请求头的完整性，一个完整的请求行或请求头，需要放在一个连续的内存里面，所以，一个完整的请求行或请求头，只会保存在一个 buffer 里面。这样，如果请求行大于一个 buffer 的大小，就会返回 414 错误，如果一个请求头大小大于一个 buffer 大小，就会返回 400 错误。在了解了这些参数的值，以及 Nginx 实际的做法之后，在应用场景，我们就需要根据实际的需求来调整这些参数，来优化我们的程序了。

处理流程图：



以上这些，就是 Nginx 中一个 http 请求的生命周期了。我们再看看与请求相关的一些概念吧。

keepalive
当然，在 Nginx 中，对于 http1.0 与 http1.1 也是支持长连接的。什么是长连接呢？我们知道，http 请求是基于 TCP 协议之上的，那么，当客户端在发起请求前，需要先与服务端建立 TCP 连接，而每一次的 TCP 连接是需要三次握手来确定的，如果客户端与服务端之间网络差一点，这三次交互消费的时间会比较多，而且三次交互也会带来网络流量。当然，当连接断开后，也会有四次的交互，当然对用户体验来说就不重要了。而 http 请求是请求应答式的，如果我们能知道每个请求头与响应体的长度，那么我们是可以在一个连接上面执行多个请求的，这就是所谓的长连接，但前提条件是我们先得确定请求头与响应体的长度。对于请求来说，如果当前请求需要有body，如 POST 请求，那么 Nginx 就需要客户端在请求头中指定 content-length 来表明 body 的大小，否则返回 400 错误。也就是说，请求体的长度是确定的，那么响应体的长度呢？先来看看 http 协议中关于响应 body 长度的确定：

对于 http1.0 协议来说，如果响应头中有 content-length 头，则以 content-length 的长度就可以知道 body 的长度了，客户端在接收 body 时，就可以依照这个长度来接收数据，接收完后，就表示这个请求完成了。而如果没有 content-length 头，则客户端会一直接收数据，直到服务端主动断开连接，才表示 body 接收完了。

而对于 http1.1 协议来说，如果响应头中的 Transfer-encoding 为 chunked 传输，则表示 body 是流式输出，body 会被分成多个块，每块的开始会标识出当前块的长度，此时，body 不需要通过长度来指定。如果是非 chunked 传输，而且有 content-length，则按照 content-length 来接收数据。否则，如果是非 chunked，并且没有 content-length，则客户端接收数据，直到服务端主动断开连接。
从上面，我们可以看到，除了 http1.0 不带 content-length 以及 http1.1 非 chunked 不带 content-length 外，body 的长度是可知的。此时，当服务端在输出完 body 之后，会可以考虑使用长连接。能否使用长连接，也是有条件限制的。如果客户端的请求头中的 connection为close，则表示客户端需要关掉长连接，如果为 keep-alive，则客户端需要打开长连接，如果客户端的请求中没有 connection 这个头，那么根据协议，如果是 http1.0，则默认为 close，如果是 http1.1，则默认为 keep-alive。如果结果为 keepalive，那么，Nginx 在输出完响应体后，会设置当前连接的 keepalive 属性，然后等待客户端下一次请求。当然，Nginx 不可能一直等待下去，如果客户端一直不发数据过来，岂不是一直占用这个连接？所以当 Nginx 设置了 keepalive 等待下一次的请求时，同时也会设置一个最大等待时间，这个时间是通过选项 keepalive_timeout 来配置的，如果配置为 0，则表示关掉 keepalive，此时，http 版本无论是 1.1 还是 1.0，客户端的 connection 不管是 close 还是 keepalive，都会强制为 close。

如果服务端最后的决定是 keepalive 打开，那么在响应的 http 头里面，也会包含有 connection 头域，其值是"Keep-Alive"，否则就是"Close"。如果 connection 值为 close，那么在 Nginx 响应完数据后，会主动关掉连接。所以，对于请求量比较大的 Nginx 来说，关掉 keepalive 最后会产生比较多的 time-wait 状态的 socket。一般来说，当客户端的一次访问，需要多次访问同一个 server 时，打开 keepalive 的优势非常大，比如图片服务器，通常一个网页会包含很多个图片。打开 keepalive 也会大量减少 time-wait 的数量。

pipe
在 http1.1 中，引入了一种新的特性，即 pipeline。那么什么是 pipeline 呢？pipeline 其实就是流水线作业，它可以看作为 keepalive 的一种升华，因为 pipeline 也是基于长连接的，目的就是利用一个连接做多次请求。如果客户端要提交多个请求，对于keepalive来说，那么第二个请求，必须要等到第一个请求的响应接收完全后，才能发起，这和 TCP 的停止等待协议是一样的，得到两个响应的时间至少为2*RTT。而对 pipeline 来说，客户端不必等到第一个请求处理完后，就可以马上发起第二个请求。得到两个响应的时间可能能够达到1*RTT。Nginx 是直接支持 pipeline 的，但是，Nginx 对 pipeline 中的多个请求的处理却不是并行的，依然是一个请求接一个请求的处理，只是在处理第一个请求的时候，客户端就可以发起第二个请求。这样，Nginx 利用 pipeline 减少了处理完一个请求后，等待第二个请求的请求头数据的时间。其实 Nginx 的做法很简单，前面说到，Nginx 在读取数据时，会将读取的数据放到一个 buffer 里面，所以，如果 Nginx 在处理完前一个请求后，如果发现 buffer 里面还有数据，就认为剩下的数据是下一个请求的开始，然后就接下来处理下一个请求，否则就设置 keepalive。

lingering_close
lingering_close，字面意思就是延迟关闭，也就是说，当 Nginx 要关闭连接时，并非立即关闭连接，而是先关闭 tcp 连接的写，再等待一段时间后再关掉连接的读。为什么要这样呢？我们先来看看这样一个场景。Nginx 在接收客户端的请求时，可能由于客户端或服务端出错了，要立即响应错误信息给客户端，而 Nginx 在响应错误信息后，大分部情况下是需要关闭当前连接。Nginx 执行完 write()系统调用把错误信息发送给客户端，write()系统调用返回成功并不表示数据已经发送到客户端，有可能还在 tcp 连接的 write buffer 里。接着如果直接执行 close()系统调用关闭 tcp 连接，内核会首先检查 tcp 的 read buffer 里有没有客户端发送过来的数据留在内核态没有被用户态进程读取，如果有则发送给客户端 RST 报文来关闭 tcp 连接丢弃 write buffer 里的数据，如果没有则等待 write buffer 里的数据发送完毕，然后再经过正常的 4 次分手报文断开连接。所以,当在某些场景下出现 tcp write buffer 里的数据在 write()系统调用之后到 close()系统调用执行之前没有发送完毕，且 tcp read buffer 里面还有数据没有读，close()系统调用会导致客户端收到 RST 报文且不会拿到服务端发送过来的错误信息数据。那客户端肯定会想，这服务器好霸道，动不动就 reset 我的连接，连个错误信息都没有。

在上面这个场景中，我们可以看到，关键点是服务端给客户端发送了 RST 包，导致自己发送的数据在客户端忽略掉了。所以，解决问题的重点是，让服务端别发 RST 包。再想想，我们发送 RST 是因为我们关掉了连接，关掉连接是因为我们不想再处理此连接了，也不会有任何数据产生了。对于全双工的 TCP 连接来说，我们只需要关掉写就行了，读可以继续进行，我们只需要丢掉读到的任何数据就行了，这样的话，当我们关掉连接后，客户端再发过来的数据，就不会再收到 RST 了。当然最终我们还是需要关掉这个读端的，所以我们会设置一个超时时间，在这个时间过后，就关掉读，客户端再发送数据来就不管了，作为服务端我会认为，都这么长时间了，发给你的错误信息也应该读到了，再慢就不关我事了，要怪就怪你 RP 不好了。当然，正常的客户端，在读取到数据后，会关掉连接，此时服务端就会在超时时间内关掉读端。这些正是 lingering_close 所做的事情。协议栈提供 SO_LINGER 这个选项，它的一种配置情况就是来处理 lingering_close 的情况的，不过 Nginx 是自己实现的 lingering_close。lingering_close 存在的意义就是来读取剩下的客户端发来的数据，所以 Nginx 会有一个读超时时间，通过 lingering_timeout 选项来设置，如果在 lingering_timeout 时间内还没有收到数据，则直接关掉连接。Nginx 还支持设置一个总的读取时间，通过 lingering_time 来设置，这个时间也就是 Nginx 在关闭写之后，保留 socket 的时间，客户端需要在这个时间内发送完所有的数据，否则 Nginx 在这个时间过后，会直接关掉连接。当然，Nginx 是支持配置是否打开 lingering_close 选项的，通过 lingering_close 选项来配置。

那么，我们在实际应用中，是否应该打开 lingering_close 呢？这个就没有固定的推荐值了，如 Maxim Dounin所说，lingering_close 的主要作用是保持更好的客户端兼容性，但是却需要消耗更多的额外资源（比如连接会一直占着）。

这节，我们介绍了 Nginx 中，连接与请求的基本概念，下节，我们讲基本的数据结构。




基本数据结构

Nginx 的作者为追求极致的高效，自己实现了很多颇具特色的 Nginx 风格的数据结构以及公共函数。比如，Nginx 提供了带长度的字符串，根据编译器选项优化过的字符串拷贝函数 ngx_copy 等。所以，在我们写 Nginx 模块时，应该尽量调用 Nginx 提供的 api，尽管有些 api 只是对 glibc 的宏定义。本节，我们介绍 string、list、buffer、chain 等一系列最基本的数据结构及相关api的使用技巧以及注意事项。

ngx_str_t
在 Nginx 源码目录的 src/core 下面的 ngx_string.h|c 里面，包含了字符串的封装以及字符串相关操作的 api。Nginx 提供了一个带长度的字符串结构 ngx_str_t，它的原型如下：

    typedef struct {
        size_t      len;
        u_char     *data;
    } ngx_str_t;
在结构体当中，data 指向字符串数据的第一个字符，字符串的结束用长度来表示，而不是由'\\0'来表示结束。所以，在写 Nginx 代码时，处理字符串的方法跟我们平时使用有很大的不一样，但要时刻记住，字符串不以'\\0'结束，尽量使用 Nginx 提供的字符串操作的 api 来操作字符串。

那么，Nginx 这样做有什么好处呢？首先，通过长度来表示字符串长度，减少计算字符串长度的次数。其次，Nginx 可以重复引用一段字符串内存，data 可以指向任意内存，长度表示结束，而不用去 copy 一份自己的字符串(因为如果要以'\\0'结束，而不能更改原字符串，所以势必要 copy 一段字符串)。我们在 ngx_http_request_t 结构体的成员中，可以找到很多字符串引用一段内存的例子，比如 request_line、uri、args 等等，这些字符串的 data 部分，都是指向在接收数据时创建 buffer 所指向的内存中，uri，args 就没有必要 copy 一份出来。这样的话，减少了很多不必要的内存分配与拷贝。

正是基于此特性，在 Nginx 中，必须谨慎的去修改一个字符串。在修改字符串时需要认真的去考虑：是否可以修改该字符串；字符串修改后，是否会对其它的引用造成影响。在后面介绍 ngx_unescape_uri 函数的时候，就会看到这一点。但是，使用 Nginx 的字符串会产生一些问题，glibc 提供的很多系统 api 函数大多是通过'\\0'来表示字符串的结束，所以我们在调用系统 api 时，就不能直接传入 str->data 了。此时，通常的做法是创建一段 str->len + 1 大小的内存，然后 copy 字符串，最后一个字节置为'\\0'。比较 hack 的做法是，将字符串最后一个字符的后一个字符 backup 一个，然后设置为'\\0'，在做完调用后，再由 backup 改回来，但前提条件是，你得确定这个字符是可以修改的，而且是有内存分配，不会越界，但一般不建议这么做。 接下来，看看 Nginx 提供的操作字符串相关的 api。

    #define ngx_string(str)     { sizeof(str) - 1, (u_char *) str }
ngx_string(str) 是一个宏，它通过一个以'\\0'结尾的普通字符串 str 构造一个 Nginx 的字符串，鉴于其中采用 sizeof 操作符计算字符串长度，因此参数必须是一个常量字符串。

    #define ngx_null_string     { 0, NULL }
定义变量时，使用 ngx_null_string 初始化字符串为空字符串，符串的长度为 0，data 为 NULL。

    #define ngx_str_set(str, text)                                               \
        (str)->len = sizeof(text) - 1; (str)->data = (u_char *) text
ngx_str_set 用于设置字符串 str 为 text，由于使用 sizeof 计算长度，故 text 必须为常量字符串。

    #define ngx_str_null(str)   (str)->len = 0; (str)->data = NULL
ngx_str_null 用于设置字符串 str 为空串，长度为 0，data 为 NULL。

上面这四个函数，使用时一定要小心，ngx_string 与 ngx_null_string 是“{，}”格式的，故只能用于赋值时初始化，如：

    ngx_str_t str = ngx_string("hello world");
    ngx_str_t str1 = ngx_null_string;
如果向下面这样使用，就会有问题，这里涉及到c语言中对结构体变量赋值操作的语法规则，在此不做介绍。

    ngx_str_t str, str1;
    str = ngx_string("hello world");    // 编译出错
    str1 = ngx_null_string;                // 编译出错
这种情况，可以调用 ngx_str_set 与 ngx_str_null 这两个函数来做:

    ngx_str_t str, str1;
    ngx_str_set(&str, "hello world");    
    ngx_str_null(&str1);
按照 C99 标准，您也可以这么做：

    ngx_str_t str, str1;
    str  = (ngx_str_t) ngx_string("hello world");
    str1 = (ngx_str_t) ngx_null_string;
另外要注意的是，ngx_string 与 ngx_str_set 在调用时，传进去的字符串一定是常量字符串，否则会得到意想不到的错误(因为 ngx_str_set 内部使用了 sizeof()，如果传入的是 u_char*，那么计算的是这个指针的长度，而不是字符串的长度)。如：

   ngx_str_t str;
   u_char *a = "hello world";
   ngx_str_set(&str, a);    // 问题产生
此外，值得注意的是，由于 ngx_str_set 与 ngx_str_null 实际上是两行语句，故在 if/for/while 等语句中单独使用需要用花括号括起来，例如：

   ngx_str_t str;
   if (cond)
      ngx_str_set(&str, "true");     // 问题产生
   else
      ngx_str_set(&str, "false");    // 问题产生
   void ngx_strlow(u_char *dst, u_char *src, size_t n);
将 src 的前 n 个字符转换成小写存放在 dst 字符串当中，调用者需要保证 dst 指向的空间大于等于n，且指向的空间必须可写。操作不会对原字符串产生变动。如要更改原字符串，可以：

    ngx_strlow(str->data, str->data, str->len);
    ngx_strncmp(s1, s2, n)
区分大小写的字符串比较，只比较前n个字符。

    ngx_strcmp(s1, s2)
区分大小写的不带长度的字符串比较。

    ngx_int_t ngx_strcasecmp(u_char *s1, u_char *s2);
不区分大小写的不带长度的字符串比较。

    ngx_int_t ngx_strncasecmp(u_char *s1, u_char *s2, size_t n);
不区分大小写的带长度的字符串比较，只比较前 n 个字符。

u_char * ngx_cdecl ngx_sprintf(u_char *buf, const char *fmt, ...);
u_char * ngx_cdecl ngx_snprintf(u_char *buf, size_t max, const char *fmt, ...);
u_char * ngx_cdecl ngx_slprintf(u_char *buf, u_char *last, const char *fmt, ...);
上面这三个函数用于字符串格式化，ngx_snprintf 的第二个参数 max 指明 buf 的空间大小，ngx_slprintf 则通过 last 来指明 buf 空间的大小。推荐使用第二个或第三个函数来格式化字符串，ngx_sprintf 函数还是比较危险的，容易产生缓冲区溢出漏洞。在这一系列函数中，Nginx 在兼容 glibc 中格式化字符串的形式之外，还添加了一些方便格式化 Nginx 类型的一些转义字符，比如%V用于格式化 ngx_str_t 结构。在 Nginx 源文件的 ngx_string.c 中有说明：

    /*
     * supported formats:
     *    %[0][width][x][X]O        off_t
     *    %[0][width]T              time_t
     *    %[0][width][u][x|X]z      ssize_t/size_t
     *    %[0][width][u][x|X]d      int/u_int
     *    %[0][width][u][x|X]l      long
     *    %[0][width|m][u][x|X]i    ngx_int_t/ngx_uint_t
     *    %[0][width][u][x|X]D      int32_t/uint32_t
     *    %[0][width][u][x|X]L      int64_t/uint64_t
     *    %[0][width|m][u][x|X]A    ngx_atomic_int_t/ngx_atomic_uint_t
     *    %[0][width][.width]f      double, max valid number fits to %18.15f
     *    %P                        ngx_pid_t
     *    %M                        ngx_msec_t
     *    %r                        rlim_t
     *    %p                        void *
     *    %V                        ngx_str_t *
     *    %v                        ngx_variable_value_t *
     *    %s                        null-terminated string
     *    %*s                       length and string
     *    %Z                        '\0'
     *    %N                        '\n'
     *    %c                        char
     *    %%                        %
     *
     *  reserved:
     *    %t                        ptrdiff_t
     *    %S                        null-terminated wchar string
     *    %C                        wchar
     */
这里特别要提醒的是，我们最常用于格式化 ngx_str_t 结构，其对应的转义符是%V，传给函数的一定要是指针类型，否则程序就会 coredump 掉。这也是我们最容易犯的错。比如：

    ngx_str_t str = ngx_string("hello world");
    u_char buffer[1024];
    ngx_snprintf(buffer, 1024, "%V", &str);    // 注意，str取地址
    void ngx_encode_base64(ngx_str_t *dst, ngx_str_t *src);
    ngx_int_t ngx_decode_base64(ngx_str_t *dst, ngx_str_t *src);
这两个函数用于对 str 进行 base64 编码与解码，调用前，需要保证 dst 中有足够的空间来存放结果，如果不知道具体大小，可先调用 ngx_base64_encoded_length 与 ngx_base64_decoded_length 来预估最大占用空间。

    uintptr_t ngx_escape_uri(u_char *dst, u_char *src, size_t size,
        ngx_uint_t type);
对 src 进行编码，根据 type 来按不同的方式进行编码，如果 dst 为 NULL，则返回需要转义的字符的数量，由此可得到需要的空间大小。type 的类型可以是：

    #define NGX_ESCAPE_URI         0
    #define NGX_ESCAPE_ARGS        1
    #define NGX_ESCAPE_HTML        2
    #define NGX_ESCAPE_REFRESH     3
    #define NGX_ESCAPE_MEMCACHED   4
    #define NGX_ESCAPE_MAIL_AUTH   5
    void ngx_unescape_uri(u_char **dst, u_char **src, size_t size, ngx_uint_t type);
对 src 进行反编码，type 可以是 0、NGX_UNESCAPE_URI、NGX_UNESCAPE_REDIRECT 这三个值。如果是 0，则表示 src 中的所有字符都要进行转码。如果是 NGX_UNESCAPE_URI 与 NGX_UNESCAPE_REDIRECT，则遇到'?'后就结束了，后面的字符就不管了。而 NGX_UNESCAPE_URI 与 NGX_UNESCAPE_REDIRECT 之间的区别是 NGX_UNESCAPE_URI 对于遇到的需要转码的字符，都会转码，而 NGX_UNESCAPE_REDIRECT 则只会对非可见字符进行转码。

    uintptr_t ngx_escape_html(u_char *dst, u_char *src, size_t size);
对 html 标签进行编码。

当然，我这里只介绍了一些常用的 api 的使用，大家可以先熟悉一下，在实际使用过程中，遇到不明白的，最快最直接的方法就是去看源码，看 api 的实现或看 Nginx 自身调用 api 的地方是怎么做的，代码就是最好的文档。

ngx_pool_t
ngx_pool_t是一个非常重要的数据结构，在很多重要的场合都有使用，很多重要的数据结构也都在使用它。那么它究竟是一个什么东西呢？简单的说，它提供了一种机制，帮助管理一系列的资源（如内存，文件等），使得对这些资源的使用和释放统一进行，免除了使用过程中考虑到对各种各样资源的什么时候释放，是否遗漏了释放的担心。

例如对于内存的管理，如果我们需要使用内存，那么总是从一个 ngx_pool_t 的对象中获取内存，在最终的某个时刻，我们销毁这个 ngx_pool_t 对象，所有这些内存都被释放了。这样我们就不必要对对这些内存进行 malloc 和 free 的操作，不用担心是否某块被malloc出来的内存没有被释放。因为当 ngx_pool_t 对象被销毁的时候，所有从这个对象中分配出来的内存都会被统一释放掉。

再比如我们要使用一系列的文件，但是我们打开以后，最终需要都关闭，那么我们就把这些文件统一登记到一个 ngx_pool_t 对象中，当这个 ngx_pool_t 对象被销毁的时候，所有这些文件都将会被关闭。

从上面举的两个例子中我们可以看出，使用 ngx_pool_t 这个数据结构的时候，所有的资源的释放都在这个对象被销毁的时刻，统一进行了释放，那么就会带来一个问题，就是这些资源的生存周期（或者说被占用的时间）是跟 ngx_pool_t 的生存周期基本一致（ngx_pool_t 也提供了少量操作可以提前释放资源）。从最高效的角度来说，这并不是最好的。比如，我们需要依次使用 A，B，C 三个资源，且使用完 B 的时候，A 就不会再被使用了，使用C的时候 A 和 B 都不会被使用到。如果不使用 ngx_pool_t 来管理这三个资源，那我们可能从系统里面申请 A，使用 A，然后在释放 A。接着申请 B，使用 B，再释放 B。最后申请 C，使用 C，然后释放 C。但是当我们使用一个 ngx_pool_t 对象来管理这三个资源的时候，A，B 和 C 的释放是在最后一起发生的，也就是在使用完 C 以后。诚然，这在客观上增加了程序在一段时间的资源使用量。但是这也减轻了程序员分别管理三个资源的生命周期的工作。这也就是有所得，必有所失的道理。实际上是一个取舍的问题，要看在具体的情况下，你更在乎的是哪个。

可以看一下在 Nginx 里面一个典型的使用 ngx_pool_t 的场景，对于 Nginx 处理的每个 http request, Nginx 会生成一个 ngx_pool_t 对象与这个 http request 关联，所有处理过程中需要申请的资源都从这个 ngx_pool_t 对象中获取，当这个 http request 处理完成以后，所有在处理过程中申请的资源，都将随着这个关联的 ngx_pool_t 对象的销毁而释放。

ngx_pool_t 相关结构及操作被定义在文件src/core/ngx_palloc.h|c中。

    typedef struct ngx_pool_s        ngx_pool_t; 

    struct ngx_pool_s {
        ngx_pool_data_t       d;
        size_t                max;
        ngx_pool_t           *current;
        ngx_chain_t          *chain;
        ngx_pool_large_t     *large;
        ngx_pool_cleanup_t   *cleanup;
        ngx_log_t            *log;
    };
从 ngx_pool_t 的一般使用者的角度来说，可不用关注 ngx_pool_t 结构中各字段作用。所以这里也不会进行详细的解释，当然在说明某些操作函数的使用的时候，如有必要，会进行说明。

下面我们来分别解释下 ngx_pool_t 的相关操作。

    ngx_pool_t *ngx_create_pool(size_t size, ngx_log_t *log);
创建一个初始节点大小为 size 的 pool，log 为后续在该 pool 上进行操作时输出日志的对象。 需要说明的是 size 的选择，size 的大小必须小于等于 NGX_MAX_ALLOC_FROM_POOL，且必须大于 sizeof(ngx_pool_t)。

选择大于 NGX_MAX_ALLOC_FROM_POOL 的值会造成浪费，因为大于该限制的空间不会被用到（只是说在第一个由 ngx_pool_t 对象管理的内存块上的内存，后续的分配如果第一个内存块上的空闲部分已用完，会再分配的）。

选择小于 sizeof(ngx_pool_t)的值会造成程序崩溃。由于初始大小的内存块中要用一部分来存储 ngx_pool_t 这个信息本身。

当一个 ngx_pool_t 对象被创建以后，该对象的 max 字段被赋值为 size-sizeof(ngx_pool_t)和 NGX_MAX_ALLOC_FROM_POOL 这两者中比较小的。后续的从这个 pool 中分配的内存块，在第一块内存使用完成以后，如果要继续分配的话，就需要继续从操作系统申请内存。当内存的大小小于等于 max 字段的时候，则分配新的内存块，链接在 d 这个字段（实际上是 d.next 字段）管理的一条链表上。当要分配的内存块是比 max 大的，那么从系统中申请的内存是被挂接在 large 字段管理的一条链表上。我们暂且把这个称之为大块内存链和小块内存链。

    void *ngx_palloc(ngx_pool_t *pool, size_t size); 
从这个 pool 中分配一块为 size 大小的内存。注意，此函数分配的内存的起始地址按照 NGX_ALIGNMENT 进行了对齐。对齐操作会提高系统处理的速度，但会造成少量内存的浪费。


    void *ngx_pnalloc(ngx_pool_t *pool, size_t size); 
从这个 pool 中分配一块为 size 大小的内存。但是此函数分配的内存并没有像上面的函数那样进行过对齐。

.. code:: c

void *ngx_pcalloc(ngx_pool_t *pool, size_t size);
该函数也是分配size大小的内存，并且对分配的内存块进行了清零。内部实际上是转调用ngx_palloc实现的。


    void *ngx_pmemalign(ngx_pool_t *pool, size_t size, size_t alignment);
按照指定对齐大小 alignment 来申请一块大小为 size 的内存。此处获取的内存不管大小都将被置于大内存块链中管理。

    ngx_int_t ngx_pfree(ngx_pool_t *pool, void *p);
对于被置于大块内存链，也就是被 large 字段管理的一列内存中的某块进行释放。该函数的实现是顺序遍历 large 管理的大块内存链表。所以效率比较低下。如果在这个链表中找到了这块内存，则释放，并返回 NGX_OK。否则返回 NGX_DECLINED。

由于这个操作效率比较低下，除非必要，也就是说这块内存非常大，确应及时释放，否则一般不需要调用。反正内存在这个 pool 被销毁的时候，总归会都释放掉的嘛！

    ngx_pool_cleanup_t *ngx_pool_cleanup_add(ngx_pool_t *p, size_t size); 
ngx_pool_t 中的 cleanup 字段管理着一个特殊的链表，该链表的每一项都记录着一个特殊的需要释放的资源。对于这个链表中每个节点所包含的资源如何去释放，是自说明的。这也就提供了非常大的灵活性。意味着，ngx_pool_t 不仅仅可以管理内存，通过这个机制，也可以管理任何需要释放的资源，例如，关闭文件，或者删除文件等等。下面我们看一下这个链表每个节点的类型:

    typedef struct ngx_pool_cleanup_s  ngx_pool_cleanup_t;
    typedef void (*ngx_pool_cleanup_pt)(void *data);

    struct ngx_pool_cleanup_s {
        ngx_pool_cleanup_pt   handler;
        void                 *data;
        ngx_pool_cleanup_t   *next;
    };
data: 指明了该节点所对应的资源。

handler: 是一个函数指针，指向一个可以释放 data 所对应资源的函数。该函数只有一个参数，就是 data。

next: 指向该链表中下一个元素。
看到这里，ngx_pool_cleanup_add 这个函数的用法，我相信大家都应该有一些明白了。但是这个参数 size 是起什么作用的呢？这个 size 就是要存储这个 data 字段所指向的资源的大小，该函数会为 data 分配 size 大小的空间。

比如我们需要最后删除一个文件。那我们在调用这个函数的时候，把 size 指定为存储文件名的字符串的大小，然后调用这个函数给 cleanup 链表中增加一项。该函数会返回新添加的这个节点。我们然后把这个节点中的 data 字段拷贝为文件名。把 hander 字段赋值为一个删除文件的函数（当然该函数的原型要按照 void (\*ngx_pool_cleanup_pt)(void \*data)）。

    void ngx_destroy_pool(ngx_pool_t *pool);
该函数就是释放 pool 中持有的所有内存，以及依次调用 cleanup 字段所管理的链表中每个元素的 handler 字段所指向的函数，来释放掉所有该 pool 管理的资源。并且把 pool 指向的 ngx_pool_t 也释放掉了，完全不可用了。

    void ngx_reset_pool(ngx_pool_t *pool);
该函数释放 pool 中所有大块内存链表上的内存，小块内存链上的内存块都修改为可用。但是不会去处理 cleanup链表上的项目。

ngx_array_t
ngx_array_t 是 Nginx 内部使用的数组结构。Nginx 的数组结构在存储上与大家认知的 C 语言内置的数组有相似性，比如实际上存储数据的区域也是一大块连续的内存。但是数组除了存储数据的内存以外还包含一些元信息来描述相关的一些信息。下面我们从数组的定义上来详细的了解一下。ngx_array_t 的定义位于src/core/ngx_array.c|h里面。

    typedef struct ngx_array_s       ngx_array_t;
    struct ngx_array_s {
        void        *elts;
        ngx_uint_t   nelts;
        size_t       size;
        ngx_uint_t   nalloc;
        ngx_pool_t  *pool;
    };
elts: 指向实际的数据存储区域。

nelts: 数组实际元素个数。

size: 数组单个元素的大小，单位是字节。

nalloc: 数组的容量。表示该数组在不引发扩容的前提下，可以最多存储的元素的个数。当 nelts 增长到达 nalloc 时，如果再往此数组中存储元素，则会引发数组的扩容。数组的容量将会扩展到原有容量的 2 倍大小。实际上是分配新的一块内存，新的一块内存的大小是原有内存大小的 2 倍。原有的数据会被拷贝到新的一块内存中。

pool: 该数组用来分配内存的内存池。
下面介绍 ngx_array_t 相关操作函数。

    ngx_array_t *ngx_array_create(ngx_pool_t *p, ngx_uint_t n, size_t size);
创建一个新的数组对象，并返回这个对象。

p: 数组分配内存使用的内存池；
n: 数组的初始容量大小，即在不扩容的情况下最多可以容纳的元素个数。
size: 单个元素的大小，单位是字节。
    void ngx_array_destroy(ngx_array_t *a);
销毁该数组对象，并释放其分配的内存回内存池。

    void *ngx_array_push(ngx_array_t *a);
在数组 a 上新追加一个元素，并返回指向新元素的指针。需要把返回的指针使用类型转换，转换为具体的类型，然后再给新元素本身或者是各字段（如果数组的元素是复杂类型）赋值。

    void *ngx_array_push_n(ngx_array_t *a, ngx_uint_t n);
在数组 a 上追加 n 个元素，并返回指向这些追加元素的首个元素的位置的指针。

    static ngx_inline ngx_int_t ngx_array_init(ngx_array_t *array, ngx_pool_t *pool, ngx_uint_t n, size_t size);
如果一个数组对象是被分配在堆上的，那么当调用 ngx_array_destroy 销毁以后，如果想再次使用，就可以调用此函数。

如果一个数组对象是被分配在栈上的，那么就需要调用此函数，进行初始化的工作以后，才可以使用。

注意事项 由于使用 ngx_palloc 分配内存，数组在扩容时，旧的内存不会被释放，会造成内存的浪费。因此，最好能提前规划好数组的容量，在创建或者初始化的时候一次搞定，避免多次扩容，造成内存浪费。

ngx_hash_t
ngx_hash_t 是 Nginx 自己的 hash 表的实现。定义和实现位于src/core/ngx_hash.h|c中。ngx_hash_t 的实现也与数据结构教科书上所描述的 hash 表的实现是大同小异。对于常用的解决冲突的方法有线性探测，二次探测和开链法等。ngx_hash_t 使用的是最常用的一种，也就是开链法，这也是 STL 中的 hash 表使用的方法。

但是 ngx_hash_t 的实现又有其几个显著的特点:

ngx_hash_t 不像其他的 hash 表的实现，可以插入删除元素，它只能一次初始化，就构建起整个 hash 表以后，既不能再删除，也不能在插入元素了。
ngx_hash_t 的开链并不是真的开了一个链表，实际上是开了一段连续的存储空间，几乎可以看做是一个数组。这是因为 ngx_hash_t 在初始化的时候，会经历一次预计算的过程，提前把每个桶里面会有多少元素放进去给计算出来，这样就提前知道每个桶的大小了。那么就不需要使用链表，一段连续的存储空间就足够了。这也从一定程度上节省了内存的使用。
从上面的描述，我们可以看出来，这个值越大，越造成内存的浪费。就两步，首先是初始化，然后就可以在里面进行查找了。下面我们详细来看一下。

ngx_hash_t 的初始化。

 ngx_int_t ngx_hash_init(ngx_hash_init_t *hinit, ngx_hash_key_t *names,
 ngx_uint_t nelts);
首先我们来看一下初始化函数。该函数的第一个参数 hinit 是初始化的一些参数的一个集合。 names 是初始化一个 ngx_hash_t 所需要的所有 key 的一个数组。而 nelts 就是 key 的个数。下面先看一下 ngx_hash_init_t 类型，该类型提供了初始化一个 hash 表所需要的一些基本信息。

    typedef struct {
        ngx_hash_t       *hash;
        ngx_hash_key_pt   key;

        ngx_uint_t        max_size;
        ngx_uint_t        bucket_size;

        char             *name;
        ngx_pool_t       *pool;
        ngx_pool_t       *temp_pool;
    } ngx_hash_init_t;
hash: 该字段如果为 NULL，那么调用完初始化函数后，该字段指向新创建出来的 hash 表。如果该字段不为 NULL，那么在初始的时候，所有的数据被插入了这个字段所指的 hash 表中。

key: 指向从字符串生成 hash 值的 hash 函数。Nginx 的源代码中提供了默认的实现函数 ngx_hash_key_lc。

max_size: hash 表中的桶的个数。该字段越大，元素存储时冲突的可能性越小，每个桶中存储的元素会更少，则查询起来的速度更快。当然，这个值越大，越造成内存的浪费也越大，(实际上也浪费不了多少)。
:bucket_size: 每个桶的最大限制大小，单位是字节。如果在初始化一个 hash 表的时候，发现某个桶里面无法存的下所有属于该桶的元素，则 hash 表初始化失败。
name: 该 hash 表的名字。

pool: 该 hash 表分配内存使用的 pool。

temp_pool: 该 hash 表使用的临时 pool，在初始化完成以后，该 pool 可以被释放和销毁掉。

下面来看一下存储 hash 表 key 的数组的结构。

    typedef struct {
        ngx_str_t         key;
        ngx_uint_t        key_hash;
        void             *value;
    } ngx_hash_key_t;
key 和 value 的含义显而易见，就不用解释了。key_hash 是对 key 使用 hash 函数计算出来的值。

对这两个结构分析完成以后，我想大家应该都已经明白这个函数应该是如何使用了吧。该函数成功初始化一个 hash 表以后，返回 NGX_OK，否则返回 NGX_ERROR。

    void *ngx_hash_find(ngx_hash_t *hash, ngx_uint_t key, u_char *name, size_t len);
在 hash 里面查找 key 对应的 value。实际上这里的 key 是对真正的 key（也就是 name）计算出的 hash 值。len 是 name 的长度。

如果查找成功，则返回指向 value 的指针，否则返回 NULL。

ngx_hash_wildcard_t
Nginx 为了处理带有通配符的域名的匹配问题，实现了 ngx_hash_wildcard_t 这样的 hash 表。他可以支持两种类型的带有通配符的域名。一种是通配符在前的，例如：\*.abc.com，也可以省略掉星号，直接写成.abc.com。这样的 key，可以匹配 www.abc.com，qqq.www.abc.com 之类的。另外一种是通配符在末尾的，例如：mail.xxx.\*，请特别注意通配符在末尾的不像位于开始的通配符可以被省略掉。这样的通配符，可以匹配 mail.xxx.com、mail.xxx.com.cn、mail.xxx.net 之类的域名。

有一点必须说明，就是一个 ngx_hash_wildcard_t 类型的 hash 表只能包含通配符在前的key或者是通配符在后的key。不能同时包含两种类型的通配符的 key。ngx_hash_wildcard_t 类型变量的构建是通过函数 ngx_hash_wildcard_init 完成的，而查询是通过函数 ngx_hash_find_wc_head 或者 ngx_hash_find_wc_tail 来做的。ngx_hash_find_wc_head 查询包含通配符在前的 key 的 hash 表的，而 ngx_hash_find_wc_tail 是查询包含通配符在后的 key 的 hash 表的。

下面详细说明这几个函数的用法。

    ngx_int_t ngx_hash_wildcard_init(ngx_hash_init_t *hinit, ngx_hash_key_t *names,
        ngx_uint_t nelts);
该函数用来构建一个可以包含通配符 key 的 hash 表。

hinit: 构造一个通配符 hash 表的一些参数的一个集合。关于该参数对应的类型的说明，请参见 ngx_hash_t 类型中 ngx_hash_init 函数的说明。

names: 构造此 hash 表的所有的通配符 key 的数组。特别要注意的是这里的 key 已经都是被预处理过的。例如：\*.abc.com或者.abc.com被预处理完成以后，变成了com.abc.。而mail.xxx.\*则被预处理为mail.xxx.。为什么会被处理这样？这里不得不简单地描述一下通配符 hash 表的实现原理。当构造此类型的 hash 表的时候，实际上是构造了一个 hash 表的一个“链表”，是通过 hash 表中的 key “链接”起来的。比如：对于\*.abc.com将会构造出 2 个 hash 表，第一个 hash 表中有一个 key 为 com 的表项，该表项的 value 包含有指向第二个 hash 表的指针，而第二个 hash 表中有一个表项 abc，该表项的 value 包含有指向\*.abc.com对应的 value 的指针。那么查询的时候，比如查询 www.abc.com 的时候，先查 com，通过查 com 可以找到第二级的 hash 表，在第二级 hash 表中，再查找 abc，依次类推，直到在某一级的 hash 表中查到的表项对应的 value 对应一个真正的值而非一个指向下一级 hash 表的指针的时候，查询过程结束。这里有一点需要特别注意的，就是 names 数组中元素的 value 值低两位 bit 必须为 0（有特殊用途）。如果不满足这个条件，这个 hash 表查询不出正确结果。

nelts: names 数组元素的个数。
该函数执行成功返回 NGX_OK，否则 NGX_ERROR。

    void *ngx_hash_find_wc_head(ngx_hash_wildcard_t *hwc, u_char *name, size_t len);
该函数查询包含通配符在前的 key 的 hash 表的。

hwc: hash 表对象的指针。
name: 需要查询的域名，例如: www.abc.com。
len: name 的长度。
该函数返回匹配的通配符对应 value。如果没有查到，返回 NULL。

    void *ngx_hash_find_wc_tail(ngx_hash_wildcard_t *hwc, u_char *name, size_t len);
该函数查询包含通配符在末尾的 key 的 hash 表的。

参数及返回值请参加上个函数的说明。

ngx_hash_combined_t
组合类型 hash 表，该 hash 表的定义如下：

    typedef struct {
        ngx_hash_t            hash;
        ngx_hash_wildcard_t  *wc_head;
        ngx_hash_wildcard_t  *wc_tail;
    } ngx_hash_combined_t;
从其定义显见，该类型实际上包含了三个 hash 表，一个普通 hash 表，一个包含前向通配符的 hash 表和一个包含后向通配符的 hash 表。

Nginx 提供该类型的作用，在于提供一个方便的容器包含三个类型的 hash 表，当有包含通配符的和不包含通配符的一组 key 构建 hash 表以后，以一种方便的方式来查询，你不需要再考虑一个 key 到底是应该到哪个类型的 hash 表里去查了。

构造这样一组合 hash 表的时候，首先定义一个该类型的变量，再分别构造其包含的三个子 hash 表即可。

对于该类型 hash 表的查询，Nginx 提供了一个方便的函数 ngx_hash_find_combined。

    void *ngx_hash_find_combined(ngx_hash_combined_t *hash, ngx_uint_t key,
    u_char *name, size_t len);
该函数在此组合 hash 表中，依次查询其三个子 hash 表，看是否匹配，一旦找到，立即返回查找结果，也就是说如果有多个可能匹配，则只返回第一个匹配的结果。

hash: 此组合 hash 表对象。
key: 根据 name 计算出的 hash 值。
name: key 的具体内容。
len: name 的长度。
返回查询的结果，未查到则返回 NULL。

ngx_hash_keys_arrays_t
大家看到在构建一个 ngx_hash_wildcard_t 的时候，需要对通配符的哪些 key 进行预处理。这个处理起来比较麻烦。而当有一组 key，这些里面既有无通配符的 key，也有包含通配符的 key 的时候。我们就需要构建三个 hash 表，一个包含普通的 key 的 hash 表，一个包含前向通配符的 hash 表，一个包含后向通配符的 hash 表（或者也可以把这三个 hash 表组合成一个 ngx_hash_combined_t）。在这种情况下，为了让大家方便的构造这些 hash 表，Nginx 提供给了此辅助类型。

该类型以及相关的操作函数也定义在src/core/ngx_hash.h|c里。我们先来看一下该类型的定义。

    typedef struct {
        ngx_uint_t        hsize;

        ngx_pool_t       *pool;
        ngx_pool_t       *temp_pool;

        ngx_array_t       keys;
        ngx_array_t      *keys_hash;

        ngx_array_t       dns_wc_head;
        ngx_array_t      *dns_wc_head_hash;

        ngx_array_t       dns_wc_tail;
        ngx_array_t      *dns_wc_tail_hash;
    } ngx_hash_keys_arrays_t;
hsize: 将要构建的 hash 表的桶的个数。对于使用这个结构中包含的信息构建的三种类型的 hash 表都会使用此参数。

pool: 构建这些 hash 表使用的 pool。

temp_pool: 在构建这个类型以及最终的三个 hash 表过程中可能用到临时 pool。该 temp_pool 可以在构建完成以后，被销毁掉。这里只是存放临时的一些内存消耗。

keys: 存放所有非通配符 key 的数组。

keys_hash: 这是个二维数组，第一个维度代表的是 bucket 的编号，那么 keys_hash[i] 中存放的是所有的 key 算出来的 hash 值对 hsize 取模以后的值为 i 的 key。假设有 3 个 key,分别是 key1,key2 和 key3 假设 hash 值算出来以后对 hsize 取模的值都是 i，那么这三个 key 的值就顺序存放在keys_hash[i][0],keys_hash[i][1], keys_hash[i][2]。该值在调用的过程中用来保存和检测是否有冲突的 key 值，也就是是否有重复。

dns_wc_head: 放前向通配符 key 被处理完成以后的值。比如：\*.abc.com 被处理完成以后，变成 “com.abc.” 被存放在此数组中。

dns_wc_tail: 存放后向通配符 key 被处理完成以后的值。比如：mail.xxx.\* 被处理完成以后，变成 “mail.xxx.” 被存放在此数组中。

dns_wc_head_hash: 该值在调用的过程中用来保存和检测是否有冲突的前向通配符的 key 值，也就是是否有重复。

dns_wc_tail_hash: 该值在调用的过程中用来保存和检测是否有冲突的后向通配符的 key 值，也就是是否有重复。
在定义一个这个类型的变量，并对字段 pool 和 temp_pool 赋值以后，就可以调用函数 ngx_hash_add_key 把所有的 key 加入到这个结构中了，该函数会自动实现普通 key，带前向通配符的 key 和带后向通配符的 key 的分类和检查，并将这个些值存放到对应的字段中去，然后就可以通过检查这个结构体中的 keys、dns_wc_head、dns_wc_tail 三个数组是否为空，来决定是否构建普通 hash 表，前向通配符 hash 表和后向通配符 hash 表了（在构建这三个类型的 hash 表的时候，可以分别使用 keys、dns_wc_head、dns_wc_tail三个数组）。

构建出这三个 hash 表以后，可以组合在一个 ngx_hash_combined_t 对象中，使用 ngx_hash_find_combined 进行查找。或者是仍然保持三个独立的变量对应这三个 hash 表，自己决定何时以及在哪个 hash 表中进行查询。

    ngx_int_t ngx_hash_keys_array_init(ngx_hash_keys_arrays_t *ha, ngx_uint_t type);
初始化这个结构，主要是对这个结构中的 ngx_array_t 类型的字段进行初始化，成功返回 NGX_OK。

ha: 该结构的对象指针。

type: 该字段有 2 个值可选择，即 NGX_HASH_SMALL 和 NGX_HASH_LARGE。用来指明将要建立的 hash 表的类型，如果是 NGX_HASH_SMALL，则有比较小的桶的个数和数组元素大小。NGX_HASH_LARGE 则相反。
    ngx_int_t ngx_hash_add_key(ngx_hash_keys_arrays_t *ha, ngx_str_t *key,
    void *value, ngx_uint_t flags);
一般是循环调用这个函数，把一组键值对加入到这个结构体中。返回 NGX_OK 是加入成功。返回 NGX_BUSY 意味着key值重复。

ha: 该结构的对象指针。

key: 参数名自解释了。

value: 参数名自解释了。

flags: 有两个标志位可以设置，NGX_HASH_WILDCARD_KEY 和 NGX_HASH_READONLY_KEY。同时要设置的使用逻辑与操作符就可以了。NGX_HASH_READONLY_KEY 被设置的时候，在计算 hash 值的时候，key 的值不会被转成小写字符，否则会。NGX_HASH_WILDCARD_KEY 被设置的时候，说明 key 里面可能含有通配符，会进行相应的处理。如果两个标志位都不设置，传 0。
有关于这个数据结构的使用，可以参考src/http/ngx_http.c中的 ngx_http_server_names 函数。

ngx_chain_t
Nginx 的 filter 模块在处理从别的 filter 模块或者是 handler 模块传递过来的数据（实际上就是需要发送给客户端的 http response）。这个传递过来的数据是以一个链表的形式(ngx_chain_t)。而且数据可能被分多次传递过来。也就是多次调用 filter 的处理函数，以不同的 ngx_chain_t。

该结构被定义在src/core/ngx_buf.h|c。下面我们来看一下 ngx_chain_t 的定义。

    typedef struct ngx_chain_s       ngx_chain_t;

    struct ngx_chain_s {
        ngx_buf_t    *buf;
        ngx_chain_t  *next;
    };
就 2 个字段，next 指向这个链表的下个节点。buf 指向实际的数据。所以在这个链表上追加节点也是非常容易，只要把末尾元素的 next 指针指向新的节点，把新节点的 next 赋值为 NULL 即可。

    ngx_chain_t *ngx_alloc_chain_link(ngx_pool_t *pool);
该函数创建一个 ngx_chain_t 的对象，并返回指向对象的指针，失败返回 NULL。

    #define ngx_free_chain(pool, cl)                                             \
        cl->next = pool->chain;                                                  \
    pool->chain = cl
该宏释放一个 ngx_chain_t 类型的对象。如果要释放整个 chain，则迭代此链表，对每个节点使用此宏即可。

注意: 对 ngx_chaint_t 类型的释放，并不是真的释放了内存，而仅仅是把这个对象挂在了这个 pool 对象的一个叫做 chain 的字段对应的 chain 上，以供下次从这个 pool 上分配 ngx_chain_t 类型对象的时候，快速的从这个 pool->chain上 取下链首元素就返回了，当然，如果这个链是空的，才会真的在这个 pool 上使用 ngx_palloc 函数进行分配。

ngx_buf_t
这个 ngx_buf_t 就是这个 ngx_chain_t 链表的每个节点的实际数据。该结构实际上是一种抽象的数据结构，它代表某种具体的数据。这个数据可能是指向内存中的某个缓冲区，也可能指向一个文件的某一部分，也可能是一些纯元数据（元数据的作用在于指示这个链表的读取者对读取的数据进行不同的处理）。

该数据结构位于src/core/ngx_buf.h|c文件中。我们来看一下它的定义。

    struct ngx_buf_s {
        u_char          *pos;
        u_char          *last;
        off_t            file_pos;
        off_t            file_last;

        u_char          *start;         /* start of buffer */
        u_char          *end;           /* end of buffer */
        ngx_buf_tag_t    tag;
        ngx_file_t      *file;
        ngx_buf_t       *shadow;

        /* the buf's content could be changed */
        unsigned         temporary:1;

        /*
         * the buf's content is in a memory cache or in a read only memory
         * and must not be changed
         */
        unsigned         memory:1;

        /* the buf's content is mmap()ed and must not be changed */
        unsigned         mmap:1;

        unsigned         recycled:1;
        unsigned         in_file:1;
        unsigned         flush:1;
        unsigned         sync:1;
        unsigned         last_buf:1;
        unsigned         last_in_chain:1;

        unsigned         last_shadow:1;
        unsigned         temp_file:1;

        /* STUB */ int   num;
    };
pos：当 buf 所指向的数据在内存里的时候，pos 指向的是这段数据开始的位置。

last：当 buf 所指向的数据在内存里的时候，last 指向的是这段数据结束的位置。

file_pos：当 buf 所指向的数据是在文件里的时候，file_pos 指向的是这段数据的开始位置在文件中的偏移量。

file_last：当 buf 所指向的数据是在文件里的时候，file_last 指向的是这段数据的结束位置在文件中的偏移量。

start：当 buf 所指向的数据在内存里的时候，这一整块内存包含的内容可能被包含在多个 buf 中(比如在某段数据中间插入了其他的数据，这一块数据就需要被拆分开)。那么这些 buf 中的 start 和 end 都指向这一块内存的开始地址和结束地址。而 pos 和 last 指向本 buf 所实际包含的数据的开始和结尾。

end：解释参见 start。

tag：实际上是一个void *类型的指针，使用者可以关联任意的对象上去，只要对使用者有意义。

file：当 buf 所包含的内容在文件中时，file字段指向对应的文件对象。

shadow：当这个 buf 完整 copy 了另外一个 buf 的所有字段的时候，那么这两个 buf 指向的实际上是同一块内存，或者是同一个文件的同一部分，此时这两个 buf 的 shadow 字段都是指向对方的。那么对于这样的两个 buf，在释放的时候，就需要使用者特别小心，具体是由哪里释放，要提前考虑好，如果造成资源的多次释放，可能会造成程序崩溃！

temporary：为 1 时表示该 buf 所包含的内容是在一个用户创建的内存块中，并且可以被在 filter 处理的过程中进行变更，而不会造成问题。

memory：为 1 时表示该 buf 所包含的内容是在内存中，但是这些内容却不能被进行处理的 filter 进行变更。

mmap：为 1 时表示该 buf 所包含的内容是在内存中, 是通过 mmap 使用内存映射从文件中映射到内存中的，这些内容却不能被进行处理的 filter 进行变更。

recycled：可以回收的。也就是这个 buf 是可以被释放的。这个字段通常是配合 shadow 字段一起使用的，对于使用 ngx_create_temp_buf 函数创建的 buf，并且是另外一个 buf 的 shadow，那么可以使用这个字段来标示这个buf是可以被释放的。

in_file：为 1 时表示该 buf 所包含的内容是在文件中。

flush：遇到有 flush 字段被设置为 1 的 buf 的 chain，则该 chain 的数据即便不是最后结束的数据（last_buf被设置，标志所有要输出的内容都完了），也会进行输出，不会受 postpone_output 配置的限制，但是会受到发送速率等其他条件的限制。

last_buf：数据被以多个 chain 传递给了过滤器，此字段为 1 表明这是最后一个 buf。

last_in_chain：在当前的 chain 里面，此 buf 是最后一个。特别要注意的是 last_in_chain 的 buf 不一定是last_buf，但是 last_buf 的 buf 一定是 last_in_chain 的。这是因为数据会被以多个 chain 传递给某 个filter 模块。

last_shadow：在创建一个 buf 的 shadow 的时候，通常将新创建的一个 buf 的 last_shadow 置为 1。

temp_file:由于受到内存使用的限制，有时候一些 buf 的内容需要被写到磁盘上的临时文件中去，那么这时，就设置此标志。
对于此对象的创建，可以直接在某个 ngx_pool_t 上分配，然后根据需要，给对应的字段赋值。也可以使用定义好的 2 个宏：

    #define ngx_alloc_buf(pool)  ngx_palloc(pool, sizeof(ngx_buf_t))
    #define ngx_calloc_buf(pool) ngx_pcalloc(pool, sizeof(ngx_buf_t))
这两个宏使用类似函数，也是不说自明的。

对于创建 temporary 字段为 1 的 buf（就是其内容可以被后续的 filter 模块进行修改），可以直接使用函数 ngx_create_temp_buf 进行创建。

    ngx_buf_t *ngx_create_temp_buf(ngx_pool_t *pool, size_t size);
该函数创建一个 ngx_buf_t 类型的对象，并返回指向这个对象的指针，创建失败返回 NULL。

对于创建的这个对象，它的 start 和 end 指向新分配内存开始和结束的地方。pos 和 last 都指向这块新分配内存的开始处，这样，后续的操作可以在这块新分配的内存上存入数据。

pool: 分配该 buf 和 buf 使用的内存所使用的 pool。
size: 该 buf 使用的内存的大小。
为了配合对 ngx_buf_t 的使用，Nginx 定义了以下的宏方便操作。

    #define ngx_buf_in_memory(b)        (b->temporary || b->memory || b->mmap)
返回这个 buf 里面的内容是否在内存里。

    #define ngx_buf_in_memory_only(b)   (ngx_buf_in_memory(b) && !b->in_file)
返回这个 buf 里面的内容是否仅仅在内存里，并且没有在文件里。

    #define ngx_buf_special(b)                                                   \
        ((b->flush || b->last_buf || b->sync)                                    \
         && !ngx_buf_in_memory(b) && !b->in_file)
返回该 buf 是否是一个特殊的 buf，只含有特殊的标志和没有包含真正的数据。

    #define ngx_buf_sync_only(b)                                                 \
        (b->sync                                                                 \
         && !ngx_buf_in_memory(b) && !b->in_file && !b->flush && !b->last_buf)
返回该 buf 是否是一个只包含 sync 标志而不包含真正数据的特殊 buf。

    #define ngx_buf_size(b)                                                      \
        (ngx_buf_in_memory(b) ? (off_t) (b->last - b->pos):                      \
                                (b->file_last - b->file_pos))
返回该 buf 所含数据的大小，不管这个数据是在文件里还是在内存里。

ngx_list_t
ngx_list_t 顾名思义，看起来好像是一个 list 的数据结构。这样的说法，算对也不算对。因为它符合 list 类型数据结构的一些特点，比如可以添加元素，实现自增长，不会像数组类型的数据结构，受到初始设定的数组容量的限制，并且它跟我们常见的 list 型数据结构也是一样的，内部实现使用了一个链表。

那么它跟我们常见的链表实现的 list 有什么不同呢？不同点就在于它的节点，它的节点不像我们常见的 list 的节点，只能存放一个元素，ngx_list_t 的节点实际上是一个固定大小的数组。

在初始化的时候，我们需要设定元素需要占用的空间大小，每个节点数组的容量大小。在添加元素到这个 list 里面的时候，会在最尾部的节点里的数组上添加元素，如果这个节点的数组存满了，就再增加一个新的节点到这个 list 里面去。

好了，看到这里，大家应该基本上明白这个 list 结构了吧？还不明白也没有关系，下面我们来具体看一下它的定义，这些定义和相关的操作函数定义在src/core/ngx_list.h|c文件中。

    typedef struct {
        ngx_list_part_t  *last;
        ngx_list_part_t   part;
        size_t            size;
        ngx_uint_t        nalloc;
        ngx_pool_t       *pool;
    } ngx_list_t;
last: 指向该链表的最后一个节点。
part: 该链表的首个存放具体元素的节点。
size: 链表中存放的具体元素所需内存大小。
nalloc: 每个节点所含的固定大小的数组的容量。
pool: 该 list 使用的分配内存的 pool。
好，我们在看一下每个节点的定义。

    typedef struct ngx_list_part_s  ngx_list_part_t;
    struct ngx_list_part_s {
        void             *elts;
        ngx_uint_t        nelts;
        ngx_list_part_t  *next;
    };
elts: 节点中存放具体元素的内存的开始地址。

nelts: 节点中已有元素个数。这个值是不能大于链表头节点 ngx_list_t 类型中的 nalloc 字段的。

next: 指向下一个节点。
我们来看一下提供的一个操作的函数。

    ngx_list_t *ngx_list_create(ngx_pool_t *pool, ngx_uint_t n, size_t size);
该函数创建一个 ngx_list_t 类型的对象，并对该 list 的第一个节点分配存放元素的内存空间。

pool: 分配内存使用的 pool。

n: 每个节点（ngx_list_part_t）固定长度的数组的长度，即最多可以存放的元素个数。

size: 每个元素所占用的内存大小。

返回值: 成功返回指向创建的 ngx_list_t 对象的指针，失败返回 NULL。
    void *ngx_list_push(ngx_list_t *list);
该函数在给定的 list 的尾部追加一个元素，并返回指向新元素存放空间的指针。如果追加失败，则返回 NULL。

    static ngx_inline ngx_int_t
    ngx_list_init(ngx_list_t *list, ngx_pool_t *pool, ngx_uint_t n, size_t size);
该函数是用于 ngx_list_t 类型的对象已经存在，但是其第一个节点存放元素的内存空间还未分配的情况下，可以调用此函数来给这个 list 的首节点来分配存放元素的内存空间。

那么什么时候会出现已经有了 ngx_list_t 类型的对象，而其首节点存放元素的内存尚未分配的情况呢？那就是这个 ngx_list_t 类型的变量并不是通过调用 ngx_list_create 函数创建的。例如：如果某个结构体的一个成员变量是 ngx_list_t 类型的，那么当这个结构体类型的对象被创建出来的时候，这个成员变量也被创建出来了，但是它的首节点的存放元素的内存并未被分配。

总之，如果这个 ngx_list_t 类型的变量，如果不是你通过调用函数 ngx_list_create 创建的，那么就必须调用此函数去初始化，否则，你往这个 list 里追加元素就可能引发不可预知的行为，亦或程序会崩溃!

ngx_queue_t
ngx_queue_t 是 Nginx 中的双向链表，在 Nginx 源码目录src/core下面的ngx_queue.h|c里面。它的原型如下：

    typedef struct ngx_queue_s ngx_queue_t;

    struct ngx_queue_s {
        ngx_queue_t  *prev;
        ngx_queue_t  *next;
    };
不同于教科书中将链表节点的数据成员声明在链表节点的结构体中，ngx_queue_t 只是声明了前向和后向指针。在使用的时候，我们首先需要定义一个哨兵节点(对于后续具体存放数据的节点，我们称之为数据节点)，比如：

    ngx_queue_t free;
接下来需要进行初始化，通过宏 ngx_queue_init()来实现：

    ngx_queue_init(&free);
ngx_queue_init()的宏定义如下：

    #define ngx_queue_init(q)     \
        (q)->prev = q;            \
        (q)->next = q
可见初始的时候哨兵节点的 prev 和 next 都指向自己，因此其实是一个空链表。ngx_queue_empty()可以用来判断一个链表是否为空，其实现也很简单，就是：

    #define ngx_queue_empty(h)    \
        (h == (h)->prev)
那么如何声明一个具有数据元素的链表节点呢？只要在相应的结构体中加上一个 ngx_queue_t 的成员就行了。比如 ngx_http_upstream_keepalive_module 中的 ngx_http_upstream_keepalive_cache_t：

    typedef struct {
        ngx_http_upstream_keepalive_srv_conf_t  *conf;

        ngx_queue_t                        queue;
        ngx_connection_t                  *connection;

        socklen_t                          socklen;
        u_char                             sockaddr[NGX_SOCKADDRLEN];
    } ngx_http_upstream_keepalive_cache_t;
对于每一个这样的数据节点，可以通过 ngx_queue_insert_head()来添加到链表中，第一个参数是哨兵节点，第二个参数是数据节点，比如：

    ngx_http_upstream_keepalive_cache_t cache;
    ngx_queue_insert_head(&free, &cache.queue);
相应的几个宏定义如下：

    #define ngx_queue_insert_head(h, x)                         \
        (x)->next = (h)->next;                                  \
        (x)->next->prev = x;                                    \
        (x)->prev = h;                                          \
        (h)->next = x

    #define ngx_queue_insert_after   ngx_queue_insert_head

    #define ngx_queue_insert_tail(h, x)                          \
        (x)->prev = (h)->prev;                                   \
        (x)->prev->next = x;                                     \
        (x)->next = h;                                           \
        (h)->prev = x
ngx_queue_insert_head() 和 ngx_queue_insert_after() 都是往头部添加节点，ngx_queue_insert_tail() 是往尾部添加节点。从代码可以看出哨兵节点的 prev 指向链表的尾数据节点，next 指向链表的头数据节点。另外 ngx_queue_head() 和 ngx_queue_last() 这两个宏分别可以得到头节点和尾节点。

那假如现在有一个 ngx_queue_t *q 指向的是链表中的数据节点的 queue 成员，如何得到ngx_http_upstream_keepalive_cache_t 的数据呢？ Nginx 提供了 ngx_queue_data() 宏来得到ngx_http_upstream_keepalive_cache_t 的指针，例如：

 ngx_http_upstream_keepalive_cache_t *cache = ngx_queue_data(q,
                              ngx_http_upstream_keepalive_cache_t,
                                                     queue);
也许您已经可以猜到 ngx_queue_data 是通过地址相减来得到的：

    #define ngx_queue_data(q, type, link)                        \
        (type *) ((u_char *) q - offsetof(type, link))
另外 Nginx 也提供了 ngx_queue_remove()宏来从链表中删除一个数据节点，以及 ngx_queue_add() 用来将一个链表添加到另一个链表。




Nginx 的配置系统

Nginx 的配置系统由一个主配置文件和其他一些辅助的配置文件构成。这些配置文件均是纯文本文件，全部位于Nginx 安装目录下的 conf 目录下。

配置文件中以#开始的行，或者是前面有若干空格或者 TAB，然后再跟#的行，都被认为是注释，也就是只对编辑查看文件的用户有意义，程序在读取这些注释行的时候，其实际的内容是被忽略的。

由于除主配置文件 nginx.conf 以外的文件都是在某些情况下才使用的，而只有主配置文件是在任何情况下都被使用的。所以在这里我们就以主配置文件为例，来解释 Nginx 的配置系统。

在 nginx.conf 中，包含若干配置项。每个配置项由配置指令和指令参数 2 个部分构成。指令参数也就是配置指令对应的配置值。

指令概述
配置指令是一个字符串，可以用单引号或者双引号括起来，也可以不括。但是如果配置指令包含空格，一定要引起来。

指令参数
指令的参数使用一个或者多个空格或者 TAB 字符与指令分开。指令的参数有一个或者多个 TOKEN 串组成。TOKEN 串之间由空格或者 TAB 键分隔。

TOKEN 串分为简单字符串或者是复合配置块。复合配置块即是由大括号括起来的一堆内容。一个复合配置块中可能包含若干其他的配置指令。

如果一个配置指令的参数全部由简单字符串构成，也就是不包含复合配置块，那么我们就说这个配置指令是一个简单配置项，否则称之为复杂配置项。例如下面这个是一个简单配置项：

    error_page   500 502 503 504  /50x.html;
对于简单配置，配置项的结尾使用分号结束。对于复杂配置项，包含多个 TOKEN 串的，一般都是简单 TOKEN 串放在前面，复合配置块一般位于最后，而且其结尾，并不需要再添加分号。例如下面这个复杂配置项：

        location / {
            root   /home/jizhao/nginx-book/build/html;
            index  index.html index.htm;
        }
指令上下文
nginx.conf 中的配置信息，根据其逻辑上的意义，对它们进行了分类，也就是分成了多个作用域，或者称之为配置指令上下文。不同的作用域含有一个或者多个配置项。

当前 Nginx 支持的几个指令上下文：

main: Nginx 在运行时与具体业务功能（比如http服务或者email服务代理）无关的一些参数，比如工作进程数，运行的身份等。
http: 与提供 http 服务相关的一些配置参数。例如：是否使用 keepalive 啊，是否使用gzip进行压缩等。
server: http 服务上支持若干虚拟主机。每个虚拟主机一个对应的 server 配置项，配置项里面包含该虚拟主机相关的配置。在提供 mail 服务的代理时，也可以建立若干 server，每个 server 通过监听的地址来区分。
location: http 服务中，某些特定的URL对应的一系列配置项。
mail: 实现 email 相关的 SMTP/IMAP/POP3 代理时，共享的一些配置项（因为可能实现多个代理，工作在多个监听地址上）。
指令上下文，可能有包含的情况出现。例如：通常 http 上下文和 mail 上下文一定是出现在 main 上下文里的。在一个上下文里，可能包含另外一种类型的上下文多次。例如：如果 http 服务，支持了多个虚拟主机，那么在 http 上下文里，就会出现多个 server 上下文。

我们来看一个示例配置：

    user  nobody;
    worker_processes  1;
    error_log  logs/error.log  info;

    events {
        worker_connections  1024;
    }

    http {  
        server {  
            listen          80;  
            server_name     www.linuxidc.com;  
            access_log      logs/linuxidc.access.log main;  
            location / {  
                index index.html;  
                root  /var/www/linuxidc.com/htdocs;  
            }  
        }  

        server {  
            listen          80;  
            server_name     www.Androidj.com;  
            access_log      logs/androidj.access.log main;  
            location / {  
                index index.html;  
                root  /var/www/androidj.com/htdocs;  
            }  
        }  
    }

    mail {
        auth_http  127.0.0.1:80/auth.php;
        pop3_capabilities  "TOP"  "USER";
        imap_capabilities  "IMAP4rev1"  "UIDPLUS";

        server {
            listen     110;
            protocol   pop3;
            proxy      on;
        }
        server {
            listen      25;
            protocol    smtp;
            proxy       on;
            smtp_auth   login plain;
            xclient     off;
        }
    }
在这个配置中，上面提到个五种配置指令上下文都存在。

存在于 main 上下文中的配置指令如下:

user
worker_processes
error_log
events
http
mail
存在于 http 上下文中的指令如下:

server
存在于 mail 上下文中的指令如下：

server
auth_http
imap_capabilities
存在于 server 上下文中的配置指令如下：

listen
server_name
access_log
location
protocol
proxy
smtp_auth
xclient
存在于 location 上下文中的指令如下：

index
root
当然，这里只是一些示例。具体有哪些配置指令，以及这些配置指令可以出现在什么样的上下文中，需要参考 Nginx 的使用文档。




Nginx 的模块化体系结构

Nginx 的内部结构是由核心部分和一系列的功能模块所组成。这样划分是为了使得每个模块的功能相对简单，便于开发，同时也便于对系统进行功能扩展。为了便于描述，下文中我们将使用 Nginx core 来称呼 Nginx 的核心功能部分。

Nginx 提供了 Web 服务器的基础功能，同时提供了 Web 服务反向代理，Email 服务反向代理功能。Nginx core实现了底层的通讯协议，为其他模块和 Nginx 进程构建了基本的运行时环境，并且构建了其他各模块的协作基础。除此之外，或者说大部分与协议相关的，或者应用相关的功能都是在这些模块中所实现的。

模块概述
Nginx 将各功能模块组织成一条链，当有请求到达的时候，请求依次经过这条链上的部分或者全部模块，进行处理。每个模块实现特定的功能。例如，实现对请求解压缩的模块，实现 SSI 的模块，实现与上游服务器进行通讯的模块，实现与 FastCGI 服务进行通讯的模块。

有两个模块比较特殊，他们居于 Nginx core 和各功能模块的中间。这两个模块就是 http 模块和 mail 模块。这 2 个模块在 Nginx core 之上实现了另外一层抽象，处理与 HTTP 协议和 Email 相关协议（SMTP/POP3/IMAP）有关的事件，并且确保这些事件能被以正确的顺序调用其他的一些功能模块。

目前 HTTP 协议是被实现在 http 模块中的，但是有可能将来被剥离到一个单独的模块中，以扩展 Nginx 支持 SPDY 协议。

模块的分类
Nginx 的模块根据其功能基本上可以分为以下几种类型：

event module: 搭建了独立于操作系统的事件处理机制的框架，及提供了各具体事件的处理。包括 ngx_events_module， ngx_event_core_module和ngx_epoll_module 等。Nginx 具体使用何种事件处理模块，这依赖于具体的操作系统和编译选项。

phase handler: 此类型的模块也被直接称为 handler 模块。主要负责处理客户端请求并产生待响应内容，比如 ngx_http_static_module 模块，负责客户端的静态页面请求处理并将对应的磁盘文件准备为响应内容输出。

output filter: 也称为 filter 模块，主要是负责对输出的内容进行处理，可以对输出进行修改。例如，可以实现对输出的所有 html 页面增加预定义的 footbar 一类的工作，或者对输出的图片的 URL 进行替换之类的工作。

upstream: upstream 模块实现反向代理的功能，将真正的请求转发到后端服务器上，并从后端服务器上读取响应，发回客户端。upstream 模块是一种特殊的 handler，只不过响应内容不是真正由自己产生的，而是从后端服务器上读取的。

load-balancer: 负载均衡模块，实现特定的算法，在众多的后端服务器中，选择一个服务器出来作为某个请求的转发服务器。



Nginx 的请求处理

Nginx 使用一个多进程模型来对外提供服务，其中一个 master 进程，多个 worker 进程。master 进程负责管理 Nginx 本身和其他 worker 进程。

所有实际上的业务处理逻辑都在 worker 进程。worker 进程中有一个函数，执行无限循环，不断处理收到的来自客户端的请求，并进行处理，直到整个 Nginx 服务被停止。

worker 进程中，ngx_worker_process_cycle()函数就是这个无限循环的处理函数。在这个函数中，一个请求的简单处理流程如下：

操作系统提供的机制（例如 epoll, kqueue 等）产生相关的事件。
接收和处理这些事件，如是接受到数据，则产生更高层的 request 对象。
处理 request 的 header 和 body。
产生响应，并发送回客户端。
完成 request 的处理。
重新初始化定时器及其他事件。
请求的处理流程
为了让大家更好的了解 Nginx 中请求处理过程，我们以 HTTP Request 为例，来做一下详细地说明。

从 Nginx 的内部来看，一个 HTTP Request 的处理过程涉及到以下几个阶段。

初始化 HTTP Request（读取来自客户端的数据，生成 HTTP Request 对象，该对象含有该请求所有的信息）。
处理请求头。
处理请求体。
如果有的话，调用与此请求（URL 或者 Location）关联的 handler。
依次调用各 phase handler 进行处理。
在这里，我们需要了解一下 phase handler 这个概念。phase 字面的意思，就是阶段。所以 phase handlers 也就好理解了，就是包含若干个处理阶段的一些 handler。

在每一个阶段，包含有若干个 handler，再处理到某个阶段的时候，依次调用该阶段的 handler 对 HTTP Request 进行处理。

通常情况下，一个 phase handler 对这个 request 进行处理，并产生一些输出。通常 phase handler 是与定义在配置文件中的某个 location 相关联的。

一个 phase handler 通常执行以下几项任务：

获取 location 配置。
产生适当的响应。
发送 response header。
发送 response body。
当 Nginx 读取到一个 HTTP Request 的 header 的时候，Nginx 首先查找与这个请求关联的虚拟主机的配置。如果找到了这个虚拟主机的配置，那么通常情况下，这个 HTTP Request 将会经过以下几个阶段的处理（phase handlers）：

NGX_HTTP_POST_READ_PHASE: 读取请求内容阶段
NGX_HTTP_SERVER_REWRITE_PHASE: Server 请求地址重写阶段
NGX_HTTP_FIND_CONFIG_PHASE: 配置查找阶段:
NGX_HTTP_REWRITE_PHASE: Location请求地址重写阶段
NGX_HTTP_POST_REWRITE_PHASE: 请求地址重写提交阶段
NGX_HTTP_PREACCESS_PHASE: 访问权限检查准备阶段
NGX_HTTP_ACCESS_PHASE: 访问权限检查阶段
NGX_HTTP_POST_ACCESS_PHASE: 访问权限检查提交阶段
NGX_HTTP_TRY_FILES_PHASE: 配置项 try_files 处理阶段
NGX_HTTP_CONTENT_PHASE: 内容产生阶段
NGX_HTTP_LOG_PHASE: 日志模块处理阶段
在内容产生阶段，为了给一个 request 产生正确的响应，Nginx 必须把这个 request 交给一个合适的 content handler 去处理。如果这个 request 对应的 location 在配置文件中被明确指定了一个 content handler，那么Nginx 就可以通过对 location 的匹配，直接找到这个对应的 handler，并把这个 request 交给这个 content handler 去处理。这样的配置指令包括像，perl，flv，proxy_pass，mp4等。

如果一个 request 对应的 location 并没有直接有配置的 content handler，那么 Nginx 依次尝试:

如果一个 location 里面有配置 random_index on，那么随机选择一个文件，发送给客户端。
如果一个 location 里面有配置 index 指令，那么发送 index 指令指明的文件，给客户端。
如果一个 location 里面有配置 autoindex on，那么就发送请求地址对应的服务端路径下的文件列表给客户端。
如果这个 request 对应的 location 上有设置 gzip_static on，那么就查找是否有对应的.gz文件存在，有的话，就发送这个给客户端（客户端支持 gzip 的情况下）。
请求的 URI 如果对应一个静态文件，static module 就发送静态文件的内容到客户端。
内容产生阶段完成以后，生成的输出会被传递到 filter 模块去进行处理。filter 模块也是与 location 相关的。所有的 fiter 模块都被组织成一条链。输出会依次穿越所有的 filter，直到有一个 filter 模块的返回值表明已经处理完成。

这里列举几个常见的 filter 模块，例如：

server-side includes。
XSLT filtering。
图像缩放之类的。
gzip 压缩。
在所有的 filter 中，有几个 filter 模块需要关注一下。按照调用的顺序依次说明如下：

write: 写输出到客户端，实际上是写到连接对应的 socket 上。
postpone: 这个 filter 是负责 subrequest 的，也就是子请求的。
copy: 将一些需要复制的 buf(文件或者内存)重新复制一份然后交给剩余的 body filter 处理。



handler 模块简介

相信大家在看了前一章的模块概述以后，都对 Nginx 的模块有了一个基本的认识。基本上作为第三方开发者最可能开发的就是三种类型的模块，即 handler，filter 和 load-balancer。Handler 模块就是接受来自客户端的请求并产生输出的模块。有些地方说 upstream 模块实际上也是一种 handler 模块，只不过它产生的内容来自于从后端服务器获取的，而非在本机产生的。

在上一章提到，配置文件中使用 location 指令可以配置 content handler 模块，当 Nginx 系统启动的时候，每个 handler 模块都有一次机会把自己关联到对应的 location上。如果有多个 handler 模块都关联了同一个 location，那么实际上只有一个 handler 模块真正会起作用。当然大多数情况下，模块开发人员都会避免出现这种情况。

handler 模块处理的结果通常有三种情况: 处理成功，处理失败（处理的时候发生了错误）或者是拒绝去处理。在拒绝处理的情况下，这个 location 的处理就会由默认的 handler 模块来进行处理。例如，当请求一个静态文件的时候，如果关联到这个 location 上的一个 handler 模块拒绝处理，就会由默认的 ngx_http_static_module 模块进行处理，该模块是一个典型的 handler 模块。

本章主要讲述的是如何编写 handler 模块，在研究 handler 模块编写之前先来了解一下模块的一些基本数据结构。




模块的基本结构

在这一节我们将会对通常的模块开发过程中，每个模块所包含的一些常用的部分进行说明。这些部分有些是必须的，有些不是必须的。同时这里所列出的这些东西对于其他类型的模块，例如 filter 模块等也都是相同的。

模块配置结构
基本上每个模块都会提供一些配置指令，以便于用户可以通过配置来控制该模块的行为。那么这些配置信息怎么存储呢？那就需要定义该模块的配置结构来进行存储。

大家都知道 Nginx 的配置信息分成了几个作用域(scope,有时也称作上下文)，这就是 main，server 以及 location。同样的每个模块提供的配置指令也可以出现在这几个作用域里。那对于这三个作用域的配置信息，每个模块就需要定义三个不同的数据结构去进行存储。当然，不是每个模块都会在这三个作用域都提供配置指令的。那么也就不一定每个模块都需要定义三个数据结构去存储这些配置信息了。视模块的实现而言，需要几个就定义几个。

有一点需要特别注意的就是，在模块的开发过程中，我们最好使用 Nginx 原有的命名习惯。这样跟原代码的契合度更高，看起来也更舒服。

对于模块配置信息的定义，命名习惯是ngx_http_<module name>_(main|srv|loc)_conf_t。这里有个例子，就是从我们后面将要展示给大家的 hello module 中截取的。

    typedef struct
    {
        ngx_str_t hello_string;
        ngx_int_t hello_counter;
    }ngx_http_hello_loc_conf_t;
模块配置指令
一个模块的配置指令是定义在一个静态数组中的。同样地，我们来看一下从 hello module 中截取的模块配置指令的定义。

    static ngx_command_t ngx_http_hello_commands[] = {
       { 
            ngx_string("hello_string"),
            NGX_HTTP_LOC_CONF|NGX_CONF_NOARGS|NGX_CONF_TAKE1,
            ngx_http_hello_string,
            NGX_HTTP_LOC_CONF_OFFSET,
            offsetof(ngx_http_hello_loc_conf_t, hello_string),
            NULL },

        { 
            ngx_string("hello_counter"),
            NGX_HTTP_LOC_CONF|NGX_CONF_FLAG,
            ngx_http_hello_counter,
            NGX_HTTP_LOC_CONF_OFFSET,
            offsetof(ngx_http_hello_loc_conf_t, hello_counter),
            NULL },               

        ngx_null_command
    };
其实看这个定义，就基本能看出来一些信息。例如，我们是定义了两个配置指令，一个是叫 hello_string，可以接受一个参数，或者是没有参数。另外一个命令是 hello_counter，接受一个 NGX_CONF_FLAG 类型的参数。除此之外，似乎看起来有点迷惑。没有关系，我们来详细看一下 ngx_command_t，一旦我们了解这个结构的详细信息，那么我相信上述这个定义所表达的所有信息就不言自明了。

ngx_command_t 的定义，位于src/core/ngx_conf_file.h中。

    struct ngx_command_s {
        ngx_str_t             name;
        ngx_uint_t            type;
        char               *(*set)(ngx_conf_t *cf, ngx_command_t *cmd, void *conf);
        ngx_uint_t            conf;
        ngx_uint_t            offset;
        void                 *post;
    };
name: 配置指令的名称。

type: 该配置的类型，其实更准确一点说，是该配置指令属性的集合。Nginx 提供了很多预定义的属性值（一些宏定义），通过逻辑或运算符可组合在一起，形成对这个配置指令的详细的说明。下面列出可在这里使用的预定义属性值及说明。

NGX_CONF_NOARGS：配置指令不接受任何参数。
NGX_CONF_TAKE1：配置指令接受 1 个参数。
NGX_CONF_TAKE2：配置指令接受 2 个参数。
NGX_CONF_TAKE3：配置指令接受 3 个参数。
NGX_CONF_TAKE4：配置指令接受 4 个参数。
NGX_CONF_TAKE5：配置指令接受 5 个参数。
NGX_CONF_TAKE6：配置指令接受 6 个参数。
NGX_CONF_TAKE7：配置指令接受 7 个参数。
可以组合多个属性，比如一个指令即可以不填参数，也可以接受1个或者2个参数。那么就是NGX_CONF_NOARGS|NGX_CONF_TAKE1|NGX_CONF_TAKE2。如果写上面三个属性在一起，你觉得麻烦，那么没有关系，Nginx 提供了一些定义，使用起来更简洁。

NGX_CONF_TAKE12：配置指令接受 1 个或者 2 个参数。
NGX_CONF_TAKE13：配置指令接受 1 个或者 3 个参数。
NGX_CONF_TAKE23：配置指令接受 2 个或者 3 个参数。
NGX_CONF_TAKE123：配置指令接受 1 个或者 2 个或者 3 参数。
NGX_CONF_TAKE1234：配置指令接受 1 个或者 2 个或者 3 个或者 4 个参数。
NGX_CONF_1MORE：配置指令接受至少一个参数。
NGX_CONF_2MORE：配置指令接受至少两个参数。
NGX_CONF_MULTI: 配置指令可以接受多个参数，即个数不定。
NGX_CONF_BLOCK：配置指令可以接受的值是一个配置信息块。也就是一对大括号括起来的内容。里面可以再包括很多的配置指令。比如常见的 server 指令就是这个属性的。
NGX_CONF_FLAG：配置指令可以接受的值是"on"或者"off"，最终会被转成 bool 值。
NGX_CONF_ANY：配置指令可以接受的任意的参数值。一个或者多个，或者"on"或者"off"，或者是配置块。
最后要说明的是，无论如何，Nginx 的配置指令的参数个数不可以超过 NGX_CONF_MAX_ARGS 个。目前这个值被定义为 8，也就是不能超过 8 个参数值。

下面介绍一组说明配置指令可以出现的位置的属性。

NGX_DIRECT_CONF：可以出现在配置文件中最外层。例如已经提供的配置指令 daemon，master_process 等。
NGX_MAIN_CONF: http、mail、events、error_log 等。
NGX_ANY_CONF: 该配置指令可以出现在任意配置级别上。
对于我们编写的大多数模块而言，都是在处理http相关的事情，也就是所谓的都是NGX_HTTP_MODULE，对于这样类型的模块，其配置可能出现的位置也是分为直接出现在http里面，以及其他位置。

NGX_HTTP_MAIN_CONF: 可以直接出现在 http 配置指令里。
NGX_HTTP_SRV_CONF: 可以出现在 http 里面的 server 配置指令里。
NGX_HTTP_LOC_CONF: 可以出现在 http server 块里面的 location 配置指令里。
NGX_HTTP_UPS_CONF: 可以出现在 http 里面的 upstream 配置指令里。
NGX_HTTP_SIF_CONF: 可以出现在 http 里面的 server 配置指令里的 if 语句所在的 block 中。
NGX_HTTP_LMT_CONF: 可以出现在 http 里面的 limit_except 指令的 block 中。
NGX_HTTP_LIF_CONF: 可以出现在 http server 块里面的 location 配置指令里的 if 语句所在的 block 中。
set: 这是一个函数指针，当 Nginx 在解析配置的时候，如果遇到这个配置指令，将会把读取到的值传递给这个函数进行分解处理。因为具体每个配置指令的值如何处理，只有定义这个配置指令的人是最清楚的。来看一下这个函数指针要求的函数原型。

char *(*set)(ngx_conf_t *cf, ngx_command_t *cmd, void *conf);
先看该函数的返回值，处理成功时，返回 NGX_OK，否则返回 NGX_CONF_ERROR 或者是一个自定义的错误信息的字符串。

再看一下这个函数被调用的时候，传入的三个参数。

cf: 该参数里面保存从配置文件读取到的原始字符串以及相关的一些信息。特别注意的是这个参数的args字段是一个 ngx_str_t类型的数组，该数组的首个元素是这个配置指令本身，第二个元素是指令的第一个参数，第三个元素是第二个参数，依次类推。

cmd: 这个配置指令对应的 ngx_command_t 结构。

conf: 就是定义的存储这个配置值的结构体，比如在上面展示的那个 ngx_http_hello_loc_conf_t。当解析这个 hello_string 变量的时候，传入的 conf 就指向一个 ngx_http_hello_loc_conf_t 类型的变量。用户在处理的时候可以使用类型转换，转换成自己知道的类型，再进行字段的赋值。
为了更加方便的实现对配置指令参数的读取，Nginx 已经默认提供了对一些标准类型的参数进行读取的函数，可以直接赋值给 set 字段使用。下面来看一下这些已经实现的 set 类型函数。

ngx_conf_set_flag_slot： 读取 NGX_CONF_FLAG 类型的参数。
ngx_conf_set_str_slot:读取字符串类型的参数。
ngx_conf_set_str_array_slot: 读取字符串数组类型的参数。
ngx_conf_set_keyval_slot： 读取键值对类型的参数。
ngx_conf_set_num_slot: 读取整数类型(有符号整数 ngx_int_t)的参数。
ngx_conf_set_size_slot:读取 size_t 类型的参数，也就是无符号数。
ngx_conf_set_off_slot: 读取 off_t 类型的参数。
ngx_conf_set_msec_slot: 读取毫秒值类型的参数。
ngx_conf_set_sec_slot: 读取秒值类型的参数。
ngx_conf_set_bufs_slot： 读取的参数值是 2 个，一个是 buf 的个数，一个是 buf 的大小。例如： output_buffers 1 128k;
ngx_conf_set_enum_slot: 读取枚举类型的参数，将其转换成整数 ngx_uint_t 类型。
ngx_conf_set_bitmask_slot: 读取参数的值，并将这些参数的值以 bit 位的形式存储。例如：HttpDavModule 模块的 dav_methods 指令。
conf: 该字段被 NGX_HTTP_MODULE 类型模块所用 (我们编写的基本上都是 NGX_HTTP_MOUDLE，只有一些 Nginx 核心模块是非 NGX_HTTP_MODULE)，该字段指定当前配置项存储的内存位置。实际上是使用哪个内存池的问题。因为 http 模块对所有 http 模块所要保存的配置信息，划分了 main, server 和 location 三个地方进行存储，每个地方都有一个内存池用来分配存储这些信息的内存。这里可能的值为 NGX_HTTP_MAIN_CONF_OFFSET、NGX_HTTP_SRV_CONF_OFFSET 或 NGX_HTTP_LOC_CONF_OFFSET。当然也可以直接置为 0，就是 NGX_HTTP_MAIN_CONF_OFFSET。

offset: 指定该配置项值的精确存放位置，一般指定为某一个结构体变量的字段偏移。因为对于配置信息的存储，一般我们都是定义个结构体来存储的。那么比如我们定义了一个结构体 A，该项配置的值需要存储到该结构体的 b 字段。那么在这里就可以填写为 offsetof(A, b)。对于有些配置项，它的值不需要保存或者是需要保存到更为复杂的结构中时，这里可以设置为 0。

post: 该字段存储一个指针。可以指向任何一个在读取配置过程中需要的数据，以便于进行配置读取的处理。大多数时候，都不需要，所以简单地设为 0 即可。

看到这里，应该就比较清楚了。ngx_http_hello_commands 这个数组每 5 个元素为一组，用来描述一个配置项的所有情况。那么如果有多个配置项，只要按照需要再增加 5 个对应的元素对新的配置项进行说明。

需要注意的是，就是在ngx_http_hello_commands这个数组定义的最后，都要加一个ngx_null_command作为结尾。

模块上下文结构
这是一个 ngx_http_module_t 类型的静态变量。这个变量实际上是提供一组回调函数指针，这些函数有在创建存储配置信息的对象的函数，也有在创建前和创建后会调用的函数。这些函数都将被 Nginx 在合适的时间进行调用。

    typedef struct {
        ngx_int_t   (*preconfiguration)(ngx_conf_t *cf);
        ngx_int_t   (*postconfiguration)(ngx_conf_t *cf);

        void       *(*create_main_conf)(ngx_conf_t *cf);
        char       *(*init_main_conf)(ngx_conf_t *cf, void *conf);

        void       *(*create_srv_conf)(ngx_conf_t *cf);
        char       *(*merge_srv_conf)(ngx_conf_t *cf, void *prev, void *conf);

        void       *(*create_loc_conf)(ngx_conf_t *cf);
        char       *(*merge_loc_conf)(ngx_conf_t *cf, void *prev, void *conf);
    } ngx_http_module_t; 
preconfiguration: 在创建和读取该模块的配置信息之前被调用。

postconfiguration: 在创建和读取该模块的配置信息之后被调用。

create_main_conf: 调用该函数创建本模块位于 http block 的配置信息存储结构。该函数成功的时候，返回创建的配置对象。失败的话，返回 NULL。

init_main_conf: 调用该函数初始化本模块位于 http block 的配置信息存储结构。该函数成功的时候，返回 NGX_CONF_OK。失败的话，返回 NGX_CONF_ERROR 或错误字符串。

create_srv_conf: 调用该函数创建本模块位于 http server block 的配置信息存储结构，每个 server block 会创建一个。该函数成功的时候，返回创建的配置对象。失败的话，返回 NULL。

merge_srv_conf: 因为有些配置指令既可以出现在 http block，也可以出现在 http server block 中。那么遇到这种情况，每个 server 都会有自己存储结构来存储该 server 的配置，但是在这种情况下 http block 中的配置与 server block 中的配置信息发生冲突的时候，就需要调用此函数进行合并，该函数并非必须提供，当预计到绝对不会发生需要合并的情况的时候，就无需提供。当然为了安全起见还是建议提供。该函数执行成功的时候，返回 NGX_CONF_OK。失败的话，返回 NGX_CONF_ERROR 或错误字符串。

create_loc_conf: 调用该函数创建本模块位于 location block 的配置信息存储结构。每个在配置中指明的 location 创建一个。该函数执行成功，返回创建的配置对象。失败的话，返回 NULL。

merge_loc_conf: 与 merge_srv_conf 类似，这个也是进行配置值合并的地方。该函数成功的时候，返回 NGX_CONF_OK。失败的话，返回 NGX_CONF_ERROR 或错误字符串。
Nginx 里面的配置信息都是上下一层层的嵌套的，对于具体某个 location 的话，对于同一个配置，如果当前层次没有定义，那么就使用上层的配置，否则使用当前层次的配置。

这些配置信息一般默认都应该设为一个未初始化的值，针对这个需求，Nginx 定义了一系列的宏定义来代表各种配置所对应数据类型的未初始化值，如下：

    #define NGX_CONF_UNSET       -1
    #define NGX_CONF_UNSET_UINT  (ngx_uint_t) -1
    #define NGX_CONF_UNSET_PTR   (void *) -1
    #define NGX_CONF_UNSET_SIZE  (size_t) -1
    #define NGX_CONF_UNSET_MSEC  (ngx_msec_t) -1
又因为对于配置项的合并，逻辑都类似，也就是前面已经说过的，如果在本层次已经配置了，也就是配置项的值已经被读取进来了（那么这些配置项的值就不会等于上面已经定义的那些 UNSET 的值），就使用本层次的值作为定义合并的结果，否则，使用上层的值，如果上层的值也是这些UNSET类的值，那就赋值为默认值，否则就使用上层的值作为合并的结果。对于这样类似的操作，Nginx 定义了一些宏操作来做这些事情，我们来看其中一个的定义。

    #define ngx_conf_merge_uint_value(conf, prev, default) \
        if (conf == NGX_CONF_UNSET_UINT) {      \
            conf = (prev == NGX_CONF_UNSET_UINT) ? default : prev; \
        }
显而易见，这个逻辑确实比较简单，所以其它的宏定义也类似，我们就列具其中的一部分吧。

    ngx_conf_merge_value
    ngx_conf_merge_ptr_value
    ngx_conf_merge_uint_value
    ngx_conf_merge_msec_value
    ngx_conf_merge_sec_value
等等。

下面来看一下 hello 模块的模块上下文的定义，加深一下印象。

    static ngx_http_module_t ngx_http_hello_module_ctx = {
        NULL,                          /* preconfiguration */
        ngx_http_hello_init,           /* postconfiguration */

        NULL,                          /* create main configuration */
        NULL,                          /* init main configuration */

        NULL,                          /* create server configuration */
        NULL,                          /* merge server configuration */

        ngx_http_hello_create_loc_conf, /* create location configuration */
        NULL                        /* merge location configuration */
    };
注意：这里并没有提供 merge_loc_conf 函数，因为我们这个模块的配置指令已经确定只出现在 NGX_HTTP_LOC_CONF 中这一个层次上，不会发生需要合并的情况。

模块的定义
对于开发一个模块来说，我们都需要定义一个 ngx_module_t 类型的变量来说明这个模块本身的信息，从某种意义上来说，这是这个模块最重要的一个信息，它告诉了 Nginx 这个模块的一些信息，上面定义的配置信息，还有模块上下文信息，都是通过这个结构来告诉 Nginx 系统的，也就是加载模块的上层代码，都需要通过定义的这个结构，来获取这些信息。

我们先来看下 ngx_module_t 的定义

    typedef struct ngx_module_s      ngx_module_t;
    struct ngx_module_s {
        ngx_uint_t            ctx_index;
        ngx_uint_t            index;
        ngx_uint_t            spare0;
        ngx_uint_t            spare1;
        ngx_uint_t            abi_compatibility;
        ngx_uint_t            major_version;
        ngx_uint_t            minor_version;
        void                 *ctx;
        ngx_command_t        *commands;
        ngx_uint_t            type;
        ngx_int_t           (*init_master)(ngx_log_t *log);
        ngx_int_t           (*init_module)(ngx_cycle_t *cycle);
        ngx_int_t           (*init_process)(ngx_cycle_t *cycle);
        ngx_int_t           (*init_thread)(ngx_cycle_t *cycle);
        void                (*exit_thread)(ngx_cycle_t *cycle);
        void                (*exit_process)(ngx_cycle_t *cycle);
        void                (*exit_master)(ngx_cycle_t *cycle);
        uintptr_t             spare_hook0;
        uintptr_t             spare_hook1;
        uintptr_t             spare_hook2;
        uintptr_t             spare_hook3;
        uintptr_t             spare_hook4;
        uintptr_t             spare_hook5;
        uintptr_t             spare_hook6;
        uintptr_t             spare_hook7;
    };

    #define NGX_NUMBER_MAJOR  3
    #define NGX_NUMBER_MINOR  1
    #define NGX_MODULE_V1          0, 0, 0, 0,                              \
        NGX_DSO_ABI_COMPATIBILITY, NGX_NUMBER_MAJOR, NGX_NUMBER_MINOR
    #define NGX_MODULE_V1_PADDING  0, 0, 0, 0, 0, 0, 0, 0
再看一下 hello 模块的模块定义。

    ngx_module_t ngx_http_hello_module = {
        NGX_MODULE_V1,
        &ngx_http_hello_module_ctx,    /* module context */
        ngx_http_hello_commands,       /* module directives */
        NGX_HTTP_MODULE,               /* module type */
        NULL,                          /* init master */
        NULL,                          /* init module */
        NULL,                          /* init process */
        NULL,                          /* init thread */
        NULL,                          /* exit thread */
        NULL,                          /* exit process */
        NULL,                          /* exit master */
        NGX_MODULE_V1_PADDING
    };
模块可以提供一些回调函数给 Nginx，当 Nginx 在创建进程线程或者结束进程线程时进行调用。但大多数模块在这些时刻并不需要做什么，所以都简单赋值为 NULL。



handler 模块的基本结构

除了上一节介绍的模块的基本结构以外，handler 模块必须提供一个真正的处理函数，这个函数负责对来自客户端请求的真正处理。这个函数的处理，既可以选择自己直接生成内容，也可以选择拒绝处理，由后续的 handler 去进行处理，或者是选择丢给后续的 filter 进行处理。来看一下这个函数的原型申明。

typedef ngx_int_t (*ngx_http_handler_pt)(ngx_http_request_t *r);
r 是 http 请求。里面包含请求所有的信息，这里不详细说明了，可以参考别的章节的介绍。 该函数处理成功返回 NGX_OK，处理发生错误返回 NGX_ERROR，拒绝处理（留给后续的 handler 进行处理）返回 NGX_DECLINE。 返回 NGX_OK 也就代表给客户端的响应已经生成好了，否则返回 NGX_ERROR 就发生错误了。




handler 模块的挂载

handler 模块真正的处理函数通过两种方式挂载到处理过程中，一种方式就是按处理阶段挂载;另外一种挂载方式就是按需挂载。

按处理阶段挂载
为了更精细地控制对于客户端请求的处理过程，Nginx 把这个处理过程划分成了 11 个阶段。他们从前到后，依次列举如下：

NGX_HTTP_POST_READ_PHASE: 读取请求内容阶段
NGX_HTTP_SERVER_REWRITE_PHASE: Server 请求地址重写阶段
NGX_HTTP_FIND_CONFIG_PHASE: 配置查找阶段:
NGX_HTTP_REWRITE_PHASE: Location 请求地址重写阶段
NGX_HTTP_POST_REWRITE_PHASE: 请求地址重写提交阶段
NGX_HTTP_PREACCESS_PHASE: 访问权限检查准备阶段
NGX_HTTP_ACCESS_PHASE: 访问权限检查阶段
NGX_HTTP_POST_ACCESS_PHASE: 访问权限检查提交阶段
NGX_HTTP_TRY_FILES_PHASE: 配置项 try_files 处理阶段
NGX_HTTP_CONTENT_PHASE: 内容产生阶段
NGX_HTTP_LOG_PHASE: 日志模块处理阶段
一般情况下，我们自定义的模块，大多数是挂载在 NGX_HTTP_CONTENT_PHASE 阶段的。挂载的动作一般是在模块上下文调用的 postconfiguration 函数中。

注意：有几个阶段是特例，它不调用挂载地任何的handler，也就是你就不用挂载到这几个阶段了：

NGX_HTTP_FIND_CONFIG_PHASE
NGX_HTTP_POST_ACCESS_PHASE
NGX_HTTP_POST_REWRITE_PHASE
NGX_HTTP_TRY_FILES_PHASE
所以其实真正是有 7 个 phase 你可以去挂载 handler。

挂载的代码如下（摘自 hello module）:

    static ngx_int_t
    ngx_http_hello_init(ngx_conf_t *cf)
    {
        ngx_http_handler_pt        *h;
        ngx_http_core_main_conf_t  *cmcf;

        cmcf = ngx_http_conf_get_module_main_conf(cf, ngx_http_core_module);

        h = ngx_array_push(&cmcf->phases[NGX_HTTP_CONTENT_PHASE].handlers);
        if (h == NULL) {
            return NGX_ERROR;
        }

        *h = ngx_http_hello_handler;

        return NGX_OK;
    }
使用这种方式挂载的 handler 也被称为 content phase handlers。

按需挂载
以这种方式挂载的 handler 也被称为 content handler。

当一个请求进来以后，Nginx 从 NGX_HTTP_POST_READ_PHASE 阶段开始依次执行每个阶段中所有 handler。执行到 NGX_HTTP_CONTENT_PHASE 阶段的时候，如果这个 location 有一个对应的 content handler 模块，那么就去执行这个 content handler 模块真正的处理函数。否则继续依次执行 NGX_HTTP_CONTENT_PHASE 阶段中所有 content phase handlers，直到某个函数处理返回 NGX_OK 或者 NGX_ERROR。

换句话说，当某个 location 处理到 NGX_HTTP_CONTENT_PHASE 阶段时，如果有 content handler 模块，那么 NGX_HTTP_CONTENT_PHASE 挂载的所有 content phase handlers 都不会被执行了。

但是使用这个方法挂载上去的 handler 有一个特点是必须在 NGX_HTTP_CONTENT_PHASE 阶段才能执行到。如果你想自己的 handler 在更早的阶段执行，那就不要使用这种挂载方式。

那么在什么情况会使用这种方式来挂载呢？一般情况下，某个模块对某个 location 进行了处理以后，发现符合自己处理的逻辑，而且也没有必要再调用 NGX_HTTP_CONTENT_PHASE 阶段的其它 handler 进行处理的时候，就动态挂载上这个 handler。

下面来看一下使用这种挂载方式的具体例子（摘自 Emiller's Guide To Nginx Module Development）。

    static char *
    ngx_http_circle_gif(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)
    {
        ngx_http_core_loc_conf_t  *clcf;

        clcf = ngx_http_conf_get_module_loc_conf(cf, ngx_http_core_module);
        clcf->handler = ngx_http_circle_gif_handler;

        return NGX_CONF_OK;
    }




handler 的编写步骤

好，到了这里，让我们稍微整理一下思路，回顾一下实现一个 handler 的步骤:

编写模块基本结构。包括模块的定义，模块上下文结构，模块的配置结构等。
实现 handler 的挂载函数。根据模块的需求选择正确的挂载方式。
编写 handler 处理函数。模块的功能主要通过这个函数来完成。
看起来不是那么难，对吧？还是那句老话，世上无难事，只怕有心人! 现在我们来完整的分析前面提到的 hello handler module 示例的功能和代码。



示例: hello handler 模块

在前面已经看到了这个 hello handler module 的部分重要的结构。该模块提供了 2 个配置指令，仅可以出现在 location 指令的作用域中。这两个指令是 hello_string, 该指令接受一个参数来设置显示的字符串。如果没有跟参数，那么就使用默认的字符串作为响应字符串。

另一个指令是 hello_counter，如果设置为 on，则会在响应的字符串后面追加 Visited Times:的字样，以统计请求的次数。

这里有两点注意一下：

对于 flag 类型的配置指令，当值为 off 的时候，使用 ngx_conf_set_flag_slot 函数，会转化为 0，为on，则转化为非 0。
另外一个是，我提供了 merge_loc_conf 函数，但是却没有设置到模块的上下文定义中。这样有一个缺点，就是如果一个指令没有出现在配置文件中的时候，配置信息中的值，将永远会保持在 create_loc_conf 中的初始化的值。那如果，在类似 create_loc_conf 这样的函数中，对创建出来的配置信息的值，没有设置为合理的值的话，后面用户又没有配置，就会出现问题。
下面来完整的给出 ngx_http_hello_module 模块的完整代码。

    #include <ngx_config.h>
    #include <ngx_core.h>
    #include <ngx_http.h>

    typedef struct
    {
        ngx_str_t hello_string;
        ngx_int_t hello_counter;
    }ngx_http_hello_loc_conf_t;

    static ngx_int_t ngx_http_hello_init(ngx_conf_t *cf);

    static void *ngx_http_hello_create_loc_conf(ngx_conf_t *cf);

    static char *ngx_http_hello_string(ngx_conf_t *cf, ngx_command_t *cmd,
        void *conf);
    static char *ngx_http_hello_counter(ngx_conf_t *cf, ngx_command_t *cmd,
        void *conf);

    static ngx_command_t ngx_http_hello_commands[] = {
       { 
            ngx_string("hello_string"),
            NGX_HTTP_LOC_CONF|NGX_CONF_NOARGS|NGX_CONF_TAKE1,
            ngx_http_hello_string,
            NGX_HTTP_LOC_CONF_OFFSET,
            offsetof(ngx_http_hello_loc_conf_t, hello_string),
            NULL },

        { 
            ngx_string("hello_counter"),
            NGX_HTTP_LOC_CONF|NGX_CONF_FLAG,
            ngx_http_hello_counter,
            NGX_HTTP_LOC_CONF_OFFSET,
            offsetof(ngx_http_hello_loc_conf_t, hello_counter),
            NULL },               

        ngx_null_command
    };

    /* 
    static u_char ngx_hello_default_string[] = "Default String: Hello, world!";
    */
    static int ngx_hello_visited_times = 0; 

    static ngx_http_module_t ngx_http_hello_module_ctx = {
        NULL,                          /* preconfiguration */
        ngx_http_hello_init,           /* postconfiguration */

        NULL,                          /* create main configuration */
        NULL,                          /* init main configuration */

        NULL,                          /* create server configuration */
        NULL,                          /* merge server configuration */

        ngx_http_hello_create_loc_conf, /* create location configuration */
        NULL                            /* merge location configuration */
    };

    ngx_module_t ngx_http_hello_module = {
        NGX_MODULE_V1,
        &ngx_http_hello_module_ctx,    /* module context */
        ngx_http_hello_commands,       /* module directives */
        NGX_HTTP_MODULE,               /* module type */
        NULL,                          /* init master */
        NULL,                          /* init module */
        NULL,                          /* init process */
        NULL,                          /* init thread */
        NULL,                          /* exit thread */
        NULL,                          /* exit process */
        NULL,                          /* exit master */
        NGX_MODULE_V1_PADDING
    };

    static ngx_int_t
    ngx_http_hello_handler(ngx_http_request_t *r)
    {
        ngx_int_t    rc;
        ngx_buf_t   *b;
        ngx_chain_t  out;
        ngx_http_hello_loc_conf_t* my_conf;
        u_char ngx_hello_string[1024] = {0};
        ngx_uint_t content_length = 0;

        ngx_log_error(NGX_LOG_EMERG, r->connection->log, 0, "ngx_http_hello_handler is called!");

        my_conf = ngx_http_get_module_loc_conf(r, ngx_http_hello_module);
        if (my_conf->hello_string.len == 0 )
        {
            ngx_log_error(NGX_LOG_EMERG, r->connection->log, 0, "hello_string is empty!");
            return NGX_DECLINED;
        }

        if (my_conf->hello_counter == NGX_CONF_UNSET
            || my_conf->hello_counter == 0)
        {
            ngx_sprintf(ngx_hello_string, "%s", my_conf->hello_string.data);
        }
        else
        {
            ngx_sprintf(ngx_hello_string, "%s Visited Times:%d", my_conf->hello_string.data, 
                ++ngx_hello_visited_times);
        }
        ngx_log_error(NGX_LOG_EMERG, r->connection->log, 0, "hello_string:%s", ngx_hello_string);
        content_length = ngx_strlen(ngx_hello_string);

        /* we response to 'GET' and 'HEAD' requests only */
        if (!(r->method & (NGX_HTTP_GET|NGX_HTTP_HEAD))) {
            return NGX_HTTP_NOT_ALLOWED;
        }

        /* discard request body, since we don't need it here */
        rc = ngx_http_discard_request_body(r);

        if (rc != NGX_OK) {
            return rc;
        }

        /* set the 'Content-type' header */
        /*
         *r->headers_out.content_type.len = sizeof("text/html") - 1;
         *r->headers_out.content_type.data = (u_char *)"text/html";
                 */
        ngx_str_set(&r->headers_out.content_type, "text/html");

        /* send the header only, if the request type is http 'HEAD' */
        if (r->method == NGX_HTTP_HEAD) {
            r->headers_out.status = NGX_HTTP_OK;
            r->headers_out.content_length_n = content_length;

            return ngx_http_send_header(r);
        }

        /* allocate a buffer for your response body */
        b = ngx_pcalloc(r->pool, sizeof(ngx_buf_t));
        if (b == NULL) {
            return NGX_HTTP_INTERNAL_SERVER_ERROR;
        }

        /* attach this buffer to the buffer chain */
        out.buf = b;
        out.next = NULL;

        /* adjust the pointers of the buffer */
        b->pos = ngx_hello_string;
        b->last = ngx_hello_string + content_length;
        b->memory = 1;    /* this buffer is in memory */
        b->last_buf = 1;  /* this is the last buffer in the buffer chain */

        /* set the status line */
        r->headers_out.status = NGX_HTTP_OK;
        r->headers_out.content_length_n = content_length;

        /* send the headers of your response */
        rc = ngx_http_send_header(r);

        if (rc == NGX_ERROR || rc > NGX_OK || r->header_only) {
            return rc;
        }

        /* send the buffer chain of your response */
        return ngx_http_output_filter(r, &out);
    }

    static void *ngx_http_hello_create_loc_conf(ngx_conf_t *cf)
    {
        ngx_http_hello_loc_conf_t* local_conf = NULL;
        local_conf = ngx_pcalloc(cf->pool, sizeof(ngx_http_hello_loc_conf_t));
        if (local_conf == NULL)
        {
            return NULL;
        }

        ngx_str_null(&local_conf->hello_string);
        local_conf->hello_counter = NGX_CONF_UNSET;

        return local_conf;
    } 

    /*
    static char *ngx_http_hello_merge_loc_conf(ngx_conf_t *cf, void *parent, void *child)
    {
        ngx_http_hello_loc_conf_t* prev = parent;
        ngx_http_hello_loc_conf_t* conf = child;

        ngx_conf_merge_str_value(conf->hello_string, prev->hello_string, ngx_hello_default_string);
        ngx_conf_merge_value(conf->hello_counter, prev->hello_counter, 0);

        return NGX_CONF_OK;
    }*/

    static char *
    ngx_http_hello_string(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)
    {

        ngx_http_hello_loc_conf_t* local_conf;

        local_conf = conf;
        char* rv = ngx_conf_set_str_slot(cf, cmd, conf);

        ngx_conf_log_error(NGX_LOG_EMERG, cf, 0, "hello_string:%s", local_conf->hello_string.data);

        return rv;
    }

    static char *ngx_http_hello_counter(ngx_conf_t *cf, ngx_command_t *cmd,
        void *conf)
    {
        ngx_http_hello_loc_conf_t* local_conf;

        local_conf = conf;

        char* rv = NULL;

        rv = ngx_conf_set_flag_slot(cf, cmd, conf);

        ngx_conf_log_error(NGX_LOG_EMERG, cf, 0, "hello_counter:%d", local_conf->hello_counter);
        return rv;    
    }

    static ngx_int_t
    ngx_http_hello_init(ngx_conf_t *cf)
    {
        ngx_http_handler_pt        *h;
        ngx_http_core_main_conf_t  *cmcf;

        cmcf = ngx_http_conf_get_module_main_conf(cf, ngx_http_core_module);

        h = ngx_array_push(&cmcf->phases[NGX_HTTP_CONTENT_PHASE].handlers);
        if (h == NULL) {
            return NGX_ERROR;
        }

        *h = ngx_http_hello_handler;

        return NGX_OK;
    }
通过上面一些介绍，我相信大家都能对整个示例模块有一个比较好的理解。唯一可能感觉有些理解困难的地方在于ngx_http_hello_handler 函数里面产生和设置输出。但其实大家在本书的前面的相关章节都可以看到对 ngx_buf_t 和 request 等相关数据结构的说明。如果仔细看了这些地方的说明的话，应该对这里代码的实现就比较容易理解了。因此，这里不再赘述解释。




handler 模块的编译和使用

模块的功能开发完了之后，模块的使用还需要编译才能够执行，下面我们来看下模块的编译和使用。

config 文件的编写
对于开发一个模块，我们是需要把这个模块的 C 代码组织到一个目录里，同时需要编写一个 config 文件。这个 config 文件的内容就是告诉 Nginx 的编译脚本，该如何进行编译。我们来看一下 hello handler module 的 config 文件的内容，然后再做解释。

    ngx_addon_name=ngx_http_hello_module
    HTTP_MODULES="$HTTP_MODULES ngx_http_hello_module"
    NGX_ADDON_SRCS="$NGX_ADDON_SRCS $ngx_addon_dir/ngx_http_hello_module.c"
其实文件很简单，几乎不需要做什么解释。大家一看都懂了。唯一需要说明的是，如果这个模块的实现有多个源文件，那么都在 NGX_ADDON_SRCS 这个变量里，依次写进去就可以。

编译
对于模块的编译，Nginx 并不像 apache 一样，提供了单独的编译工具，可以在没有 apache 源代码的情况下来单独编译一个模块的代码。Nginx 必须去到 Nginx 的源代码目录里，通过 configure 指令的参数，来进行编译。下面看一下 hello module 的 configure 指令：

./configure --prefix=/usr/local/nginx-1.3.1 --add-module=/home/jizhao/open_source/book_module
我写的这个示例模块的代码和 config 文件都放在/home/jizhao/open_source/book_module这个目录下。所以一切都很明了，也没什么好说的了。

使用
使用一个模块需要根据这个模块定义的配置指令来做。比如我们这个简单的 hello handler module 的使用就很简单。在我的测试服务器的配置文件里，就是在 http 里面的默认的 server 里面加入如下的配置：

    location /test {
            hello_string jizhao;
            hello_counter on;
    }
当我们访问这个地址的时候, lynx http://127.0.0.1/test 的时候，就可以看到返回的结果。

jizhao Visited Times:1
当然你访问多次，这个次数是会增加的。




更多 handler 模块示例分析

http access module
该模块的代码位于src/http/modules/ngx_http_access_module.c中。该模块的作用是提供对于特定 host 的客户端的访问控制。可以限定特定 host 的客户端对于服务端全部，或者某个 server，或者是某个 location 的访问。

该模块的实现非常简单，总共也就只有几个函数。

    static ngx_int_t ngx_http_access_handler(ngx_http_request_t *r);
    static ngx_int_t ngx_http_access_inet(ngx_http_request_t *r,
        ngx_http_access_loc_conf_t *alcf, in_addr_t addr);
    #if (NGX_HAVE_INET6)
    static ngx_int_t ngx_http_access_inet6(ngx_http_request_t *r,
        ngx_http_access_loc_conf_t *alcf, u_char *p);
    #endif
    static ngx_int_t ngx_http_access_found(ngx_http_request_t *r, ngx_uint_t deny);
    static char *ngx_http_access_rule(ngx_conf_t *cf, ngx_command_t *cmd,
        void *conf);
    static void *ngx_http_access_create_loc_conf(ngx_conf_t *cf);
    static char *ngx_http_access_merge_loc_conf(ngx_conf_t *cf,
        void *parent, void *child);
    static ngx_int_t ngx_http_access_init(ngx_conf_t *cf);
对于与配置相关的几个函数都不需要做解释了，需要提一下的是函数 ngx_http_access_init，该函数在实现上把本模块挂载到了 NGX_HTTP_ACCESS_PHASE 阶段的 handler 上，从而使自己的被调用时机发生在了 NGX_HTTP_CONTENT_PHASE 等阶段前。因为进行客户端地址的限制检查，根本不需要等到这么后面。

另外看一下这个模块的主处理函数 ngx_http_access_handler。这个函数的逻辑也非常简单，主要是根据客户端地址的类型，来分别选择 ipv4 类型的处理函数 ngx_http_access_inet 还是 ipv6 类型的处理函数 ngx_http_access_inet6。

而这个两个处理函数内部也非常简单，就是循环检查每个规则，检查是否有匹配的规则，如果有就返回匹配的结果，如果都没有匹配，就默认拒绝。

http static module
从某种程度上来说，此模块可以算的上是“最正宗的”，“最古老”的 content handler。因为本模块的作用就是读取磁盘上的静态文件，并把文件内容作为产生的输出。在Web技术发展的早期，只有静态页面，没有服务端脚本来动态生成 HTML 的时候。恐怕开发个 Web 服务器的时候，第一个要开发就是这样一个 content handler。

http static module 的代码位于src/http/modules/ngx_http_static_module.c中，总共只有两百多行近三百行。可以说是非常短小。

我们首先来看一下该模块的模块上下文的定义。

    ngx_http_module_t  ngx_http_static_module_ctx = {
        NULL,                                  /* preconfiguration */
        ngx_http_static_init,                  /* postconfiguration */

        NULL,                                  /* create main configuration */
        NULL,                                  /* init main configuration */

        NULL,                                  /* create server configuration */
        NULL,                                  /* merge server configuration */

        NULL,                                  /* create location configuration */
        NULL                                   /* merge location configuration */
    };
是非常的简洁吧，连任何与配置相关的函数都没有。对了，因为该模块没有提供任何配置指令。大家想想也就知道了，这个模块做的事情实在是太简单了，也确实没什么好配置的。唯一需要调用的函数是一个 ngx_http_static_init 函数。好了，来看一下这个函数都干了写什么。

    static ngx_int_t
    ngx_http_static_init(ngx_conf_t *cf)
    {
        ngx_http_handler_pt        *h;
        ngx_http_core_main_conf_t  *cmcf;

        cmcf = ngx_http_conf_get_module_main_conf(cf, ngx_http_core_module);

        h = ngx_array_push(&cmcf->phases[NGX_HTTP_CONTENT_PHASE].handlers);
        if (h == NULL) {
            return NGX_ERROR;
        }

        *h = ngx_http_static_handler;

        return NGX_OK;
    }
仅仅是挂载这个 handler 到 NGX_HTTP_CONTENT_PHASE 处理阶段。简单吧？

下面我们就看一下这个模块最核心的处理逻辑所在的 ngx_http_static_handler 函数。该函数大概占了这个模块代码量的百分之八九十。

    static ngx_int_t
    ngx_http_static_handler(ngx_http_request_t *r)
    {
        u_char                    *last, *location;
        size_t                     root, len;
        ngx_str_t                  path;
        ngx_int_t                  rc;
        ngx_uint_t                 level;
        ngx_log_t                 *log;
        ngx_buf_t                 *b;
        ngx_chain_t                out;
        ngx_open_file_info_t       of;
        ngx_http_core_loc_conf_t  *clcf;

        if (!(r->method & (NGX_HTTP_GET|NGX_HTTP_HEAD|NGX_HTTP_POST))) {
            return NGX_HTTP_NOT_ALLOWED;
        }

        if (r->uri.data[r->uri.len - 1] == '/') {
            return NGX_DECLINED;
        }

        log = r->connection->log;

        /*
         * ngx_http_map_uri_to_path() allocates memory for terminating '\0'
         * so we do not need to reserve memory for '/' for possible redirect
         */

        last = ngx_http_map_uri_to_path(r, &path, &root, 0);
        if (last == NULL) {
            return NGX_HTTP_INTERNAL_SERVER_ERROR;
        }

        path.len = last - path.data;

        ngx_log_debug1(NGX_LOG_DEBUG_HTTP, log, 0,
                       "http filename: \"%s\"", path.data);

        clcf = ngx_http_get_module_loc_conf(r, ngx_http_core_module);

        ngx_memzero(&of, sizeof(ngx_open_file_info_t));

        of.read_ahead = clcf->read_ahead;
        of.directio = clcf->directio;
        of.valid = clcf->open_file_cache_valid;
        of.min_uses = clcf->open_file_cache_min_uses;
        of.errors = clcf->open_file_cache_errors;
        of.events = clcf->open_file_cache_events;

        if (ngx_http_set_disable_symlinks(r, clcf, &path, &of) != NGX_OK) {
            return NGX_HTTP_INTERNAL_SERVER_ERROR;
        }

        if (ngx_open_cached_file(clcf->open_file_cache, &path, &of, r->pool)
            != NGX_OK)
        {
            switch (of.err) {

            case 0:
                return NGX_HTTP_INTERNAL_SERVER_ERROR;

            case NGX_ENOENT:
            case NGX_ENOTDIR:
            case NGX_ENAMETOOLONG:

                level = NGX_LOG_ERR;
                rc = NGX_HTTP_NOT_FOUND;
                break;

            case NGX_EACCES:
    #if (NGX_HAVE_OPENAT)
            case NGX_EMLINK:
            case NGX_ELOOP:
    #endif

                level = NGX_LOG_ERR;
                rc = NGX_HTTP_FORBIDDEN;
                break;

            default:

                level = NGX_LOG_CRIT;
                rc = NGX_HTTP_INTERNAL_SERVER_ERROR;
                break;
            }

            if (rc != NGX_HTTP_NOT_FOUND || clcf->log_not_found) {
                ngx_log_error(level, log, of.err,
                              "%s \"%s\" failed", of.failed, path.data);
            }

            return rc;
        }

        r->root_tested = !r->error_page;

        ngx_log_debug1(NGX_LOG_DEBUG_HTTP, log, 0, "http static fd: %d", of.fd);

        if (of.is_dir) {

            ngx_log_debug0(NGX_LOG_DEBUG_HTTP, log, 0, "http dir");

            ngx_http_clear_location(r);

            r->headers_out.location = ngx_palloc(r->pool, sizeof(ngx_table_elt_t));
            if (r->headers_out.location == NULL) {
                return NGX_HTTP_INTERNAL_SERVER_ERROR;
            }

            len = r->uri.len + 1;

            if (!clcf->alias && clcf->root_lengths == NULL && r->args.len == 0) {
                location = path.data + clcf->root.len;

                *last = '/';

            } else {
                if (r->args.len) {
                    len += r->args.len + 1;
                }

                location = ngx_pnalloc(r->pool, len);
                if (location == NULL) {
                    return NGX_HTTP_INTERNAL_SERVER_ERROR;
                }

                last = ngx_copy(location, r->uri.data, r->uri.len);

                *last = '/';

                if (r->args.len) {
                    *++last = '?';
                    ngx_memcpy(++last, r->args.data, r->args.len);
                }
            }

            /*
             * we do not need to set the r->headers_out.location->hash and
             * r->headers_out.location->key fields
             */

            r->headers_out.location->value.len = len;
            r->headers_out.location->value.data = location;

            return NGX_HTTP_MOVED_PERMANENTLY;
        }

    #if !(NGX_WIN32) /* the not regular files are probably Unix specific */

        if (!of.is_file) {
            ngx_log_error(NGX_LOG_CRIT, log, 0,
                          "\"%s\" is not a regular file", path.data);

            return NGX_HTTP_NOT_FOUND;
        }

    #endif

        if (r->method & NGX_HTTP_POST) {
            return NGX_HTTP_NOT_ALLOWED;
        }

        rc = ngx_http_discard_request_body(r);

        if (rc != NGX_OK) {
            return rc;
        }

        log->action = "sending response to client";

        r->headers_out.status = NGX_HTTP_OK;
        r->headers_out.content_length_n = of.size;
        r->headers_out.last_modified_time = of.mtime;

        if (ngx_http_set_content_type(r) != NGX_OK) {
            return NGX_HTTP_INTERNAL_SERVER_ERROR;
        }

        if (r != r->main && of.size == 0) {
            return ngx_http_send_header(r);
        }

        r->allow_ranges = 1;

        /* we need to allocate all before the header would be sent */

        b = ngx_pcalloc(r->pool, sizeof(ngx_buf_t));
        if (b == NULL) {
            return NGX_HTTP_INTERNAL_SERVER_ERROR;
        }

        b->file = ngx_pcalloc(r->pool, sizeof(ngx_file_t));
        if (b->file == NULL) {
            return NGX_HTTP_INTERNAL_SERVER_ERROR;
        }

        rc = ngx_http_send_header(r);

        if (rc == NGX_ERROR || rc > NGX_OK || r->header_only) {
            return rc;
        }

        b->file_pos = 0;
        b->file_last = of.size;

        b->in_file = b->file_last ? 1: 0;
        b->last_buf = (r == r->main) ? 1: 0;
        b->last_in_chain = 1;

        b->file->fd = of.fd;
        b->file->name = path;
        b->file->log = log;
        b->file->directio = of.is_directio;

        out.buf = b;
        out.next = NULL;

        return ngx_http_output_filter(r, &out);
    }
首先是检查客户端的 http 请求类型（r->method），如果请求类型为NGX_HTTP_GET|NGX_HTTP_HEAD|NGX_HTTP_POST，则继续进行处理，否则一律返回 NGX_HTTP_NOT_ALLOWED 从而拒绝客户端的发起的请求。

其次是检查请求的 url 的结尾字符是不是斜杠/，如果是说明请求的不是一个文件，给后续的 handler 去处理，比如后续的 ngx_http_autoindex_handler（如果是请求的是一个目录下面，可以列出这个目录的文件），或者是 ngx_http_index_handler（如果请求的路径下面有个默认的 index 文件，直接返回 index 文件的内容）。

然后接下来调用了一个 ngx_http_map_uri_to_path 函数，该函数的作用是把请求的 http 协议的路径转化成一个文件系统的路径。

然后根据转化出来的具体路径，去打开文件，打开文件的时候做了 2 种检查，一种是，如果请求的文件是个 symbol link，根据配置，是否允许符号链接，不允许返回错误。还有一个检查是，如果请求的是一个名称，是一个目录的名字，也返回错误。如果都没有错误，就读取文件，返回内容。其实说返回内容可能不是特别准确，比较准确的说法是，把产生的内容传递给后续的 filter 去处理。

http log module
该模块提供了对于每一个 http 请求进行记录的功能，也就是我们见到的 access.log。当然这个模块对于 log 提供了一些配置指令，使得可以比较方便的定制 access.log。

这个模块的代码位于src/http/modules/ngx_http_log_module.c，虽然这个模块的代码有接近 1400 行，但是主要的逻辑在于对日志本身格式啊，等细节的处理。我们在这里进行分析主要是关注，如何编写一个 log handler 的问题。

由于 log handler 的时候，拿到的参数也是 request 这个东西，那么也就意味着我们如果需要，可以好好研究下这个结构，把我们需要的所有信息都记录下来。

对于 log handler，有一点特别需要注意的就是，log handler 是无论如何都会被调用的，就是只要服务端接受到了一个客户端的请求，也就是产生了一个 request 对象，那么这些个 log handler 的处理函数都会被调用的，就是在释放 request 的时候被调用的（ngx_http_free_request函数）。

那么当然绝对不能忘记的就是 log handler 最好，也是建议被挂载在 NGX_HTTP_LOG_PHASE 阶段。因为挂载在其他阶段，有可能在某些情况下被跳过，而没有执行到，导致你的 log 模块记录的信息不全。

还有一点要说明的是，由于 Nginx 是允许在某个阶段有多个 handler 模块存在的，根据其处理结果，确定是否要调用下一个 handler。但是对于挂载在 NGX_HTTP_LOG_PHASE 阶段的 handler，则根本不关注这里 handler 的具体处理函数的返回值，所有的都被调用。如下，位于src/http/ngx_http_request.c中的 ngx_http_log_request 函数。

    static void
    ngx_http_log_request(ngx_http_request_t *r)
    {
        ngx_uint_t                  i, n;
        ngx_http_handler_pt        *log_handler;
        ngx_http_core_main_conf_t  *cmcf;

        cmcf = ngx_http_get_module_main_conf(r, ngx_http_core_module);

        log_handler = cmcf->phases[NGX_HTTP_LOG_PHASE].handlers.elts;
        n = cmcf->phases[NGX_HTTP_LOG_PHASE].handlers.nelts;

        for (i = 0; i < n; i++) {
            log_handler[i](r);
        }
    }





过滤模块简介

执行时间和内容
过滤（filter）模块是过滤响应头和内容的模块，可以对回复的头和内容进行处理。它的处理时间在获取回复内容之后，向用户发送响应之前。它的处理过程分为两个阶段，过滤 HTTP 回复的头部和主体，在这两个阶段可以分别对头部和主体进行修改。

在代码中有类似的函数：

ngx_http_top_header_filter(r);
ngx_http_top_body_filter(r, in);
就是分别对头部和主体进行过滤的函数。所有模块的响应内容要返回给客户端，都必须调用这两个接口。

执行顺序
过滤模块的调用是有顺序的，它的顺序在编译的时候就决定了。控制编译的脚本位于 auto/modules 中，当你编译完 Nginx 以后，可以在 objs 目录下面看到一个 ngx_modules.c 的文件。打开这个文件，有类似的代码：

        ngx_module_t *ngx_modules[] = {
            ...
            &ngx_http_write_filter_module,
            &ngx_http_header_filter_module,
            &ngx_http_chunked_filter_module,
            &ngx_http_range_header_filter_module,
            &ngx_http_gzip_filter_module,
            &ngx_http_postpone_filter_module,
            &ngx_http_ssi_filter_module,
            &ngx_http_charset_filter_module,
            &ngx_http_userid_filter_module,
            &ngx_http_headers_filter_module,
            &ngx_http_copy_filter_module,
            &ngx_http_range_body_filter_module,
            &ngx_http_not_modified_filter_module,
            NULL
        };
从 write_filter 到 not_modified_filter，模块的执行顺序是反向的。也就是说最早执行的是 not_modified_filter，然后各个模块依次执行。一般情况下，第三方过滤模块的 config 文件会将模块名追加到变量 HTTP_AUX_FILTER_MODULES 中，此时该模块只能加入到 copy_filter 和 headers_filter 模块之间执行。

Nginx 执行的时候是怎么按照次序依次来执行各个过滤模块呢？它采用了一种很隐晦的方法，即通过局部的全局变量。比如，在每个 filter 模块，很可能看到如下代码：

        static ngx_http_output_header_filter_pt  ngx_http_next_header_filter;
        static ngx_http_output_body_filter_pt    ngx_http_next_body_filter;

        ...

        ngx_http_next_header_filter = ngx_http_top_header_filter;
        ngx_http_top_header_filter = ngx_http_example_header_filter;

        ngx_http_next_body_filter = ngx_http_top_body_filter;
        ngx_http_top_body_filter = ngx_http_example_body_filter;
ngx_http_top_header_filter 是一个全局变量。当编译进一个 filter 模块的时候，就被赋值为当前 filter 模块的处理函数。而 ngx_http_next_header_filter 是一个局部全局变量，它保存了编译前上一个 filter 模块的处理函数。所以整体看来，就像用全局变量组成的一条单向链表。

每个模块想执行下一个过滤函数，只要调用一下 ngx_http_next_header_filter 这个局部变量。而整个过滤模块链的入口，需要调用 ngx_http_top_header_filter 这个全局变量。ngx_http_top_body_filter 的行为与 header fitler 类似。

响应头和响应体过滤函数的执行顺序如下所示：



这图只表示了 head_filter 和 body_filter 之间的执行顺序，在 header_filter 和 body_filter 处理函数之间，在 body_filter 处理函数之间，可能还有其他执行代码。

模块编译
Nginx 可以方便的加入第三方的过滤模块。在过滤模块的目录里，首先需要加入 config 文件，文件的内容如下：

ngx_addon_name=ngx_http_example_filter_module
HTTP_AUX_FILTER_MODULES="$HTTP_AUX_FILTER_MODULES ngx_http_example_filter_module"
NGX_ADDON_SRCS="$NGX_ADDON_SRCS $ngx_addon_dir/ngx_http_example_filter_module.c"
说明把这个名为 ngx_http_example_filter_module 的过滤模块加入，ngx_http_example_filter_module.c 是该模块的源代码。

注意 HTTP_AUX_FILTER_MODULES 这个变量与一般的内容处理模块不同。



过滤模块的分析

相关结构体
ngx_chain_t 结构非常简单，是一个单向链表：

        typedef struct ngx_chain_s ngx_chain_t;

        struct ngx_chain_s {
            ngx_buf_t    *buf;
            ngx_chain_t  *next;
        };
在过滤模块中，所有输出的内容都是通过一条单向链表所组成。这种单向链表的设计，正好应和了 Nginx 流式的输出模式。每次 Nginx 都是读到一部分的内容，就放到链表，然后输出出去。这种设计的好处是简单，非阻塞，但是相应的问题就是跨链表的内容操作非常麻烦，如果需要跨链表，很多时候都只能缓存链表的内容。

单链表负载的就是 ngx_buf_t，这个结构体使用非常广泛，先让我们看下该结构体的代码：

        struct ngx_buf_s {
            u_char          *pos;       /* 当前buffer真实内容的起始位置 */
            u_char          *last;      /* 当前buffer真实内容的结束位置 */
            off_t            file_pos;  /* 在文件中真实内容的起始位置   */
            off_t            file_last; /* 在文件中真实内容的结束位置   */

            u_char          *start;    /* buffer内存的开始分配的位置 */
            u_char          *end;      /* buffer内存的结束分配的位置 */
            ngx_buf_tag_t    tag;      /* buffer属于哪个模块的标志 */
            ngx_file_t      *file;     /* buffer所引用的文件 */

            /* 用来引用替换过后的buffer，以便当所有buffer输出以后，
             * 这个影子buffer可以被释放。
             */
            ngx_buf_t       *shadow; 

            /* the buf's content could be changed */
            unsigned         temporary:1;

            /*
             * the buf's content is in a memory cache or in a read only memory
             * and must not be changed
             */
            unsigned         memory:1;

            /* the buf's content is mmap()ed and must not be changed */
            unsigned         mmap:1;

            unsigned         recycled:1; /* 内存可以被输出并回收 */
            unsigned         in_file:1;  /* buffer的内容在文件中 */
            /* 马上全部输出buffer的内容, gzip模块里面用得比较多 */
            unsigned         flush:1;
            /* 基本上是一段输出链的最后一个buffer带的标志，标示可以输出，
             * 有些零长度的buffer也可以置该标志
             */
            unsigned         sync:1;
            /* 所有请求里面最后一块buffer，包含子请求 */
            unsigned         last_buf:1;
            /* 当前请求输出链的最后一块buffer         */
            unsigned         last_in_chain:1;
            /* shadow链里面的最后buffer，可以释放buffer了 */
            unsigned         last_shadow:1;
            /* 是否是暂存文件 */
            unsigned         temp_file:1;

            /* 统计用，表示使用次数 */
            /* STUB */ int   num;
        };
一般 buffer 结构体可以表示一块内存，内存的起始和结束地址分别用 start 和 end 表示，pos 和 last 表示实际的内容。如果内容已经处理过了，pos 的位置就可以往后移动。如果读取到新的内容，last 的位置就会往后移动。所以 buffer 可以在多次调用过程中使用。如果 last 等于 end，就说明这块内存已经用完了。如果 pos 等于 last，说明内存已经处理完了。下面是一个简单的示意图，说明 buffer 中指针的用法：



响应头过滤函数
响应头过滤函数主要的用处就是处理 HTTP 响应的头，可以根据实际情况对于响应头进行修改或者添加删除。响应头过滤函数先于响应体过滤函数，而且只调用一次，所以一般可作过滤模块的初始化工作。

响应头过滤函数的入口只有一个：

        ngx_int_t
        ngx_http_send_header(ngx_http_request_t *r)
        {
            ...

            return ngx_http_top_header_filter(r);
        }
该函数向客户端发送回复的时候调用，然后按前一节所述的执行顺序。该函数的返回值一般是 NGX_OK，NGX_ERROR 和 NGX_AGAIN，分别表示处理成功，失败和未完成。

你可以把 HTTP 响应头的存储方式想象成一个 hash 表，在 Nginx 内部可以很方便地查找和修改各个响应头部，ngx_http_header_filter_module 过滤模块把所有的 HTTP 头组合成一个完整的 buffer，最终 ngx_http_write_filter_module 过滤模块把 buffer 输出。

按照前一节过滤模块的顺序，依次讲解如下：

filter module	description
ngx_http_not_modified_filter_module	默认打开，如果请求的 if-modified-since 等于回复的 last-modified 间值，说明回复没有变化，清空所有回复的内容，返回 304。
ngx_http_range_body_filter_module	默认打开，只是响应体过滤函数，支持 range 功能，如果请求包含range请求，那就只发送range请求的一段内容。
ngx_http_copy_filter_module	始终打开，只是响应体过滤函数， 主要工作是把文件中内容读到内存中，以便进行处理。
ngx_http_headers_filter_module	始终打开，可以设置 expire 和 Cache-control 头，可以添加任意名称的头
ngx_http_userid_filter_module	默认关闭，可以添加统计用的识别用户的 cookie。
ngx_http_charset_filter_module	默认关闭，可以添加 charset，也可以将内容从一种字符集转换到另外一种字符集，不支持多字节字符集。
ngx_http_ssi_filter_module	默认关闭，过滤 SSI 请求，可以发起子请求，去获取include进来的文件
ngx_http_postpone_filter_module	始终打开，用来将子请求和主请求的输出链合并
ngx_http_gzip_filter_module	默认关闭，支持流式的压缩内容
ngx_http_range_header_filter_module	默认打开，只是响应头过滤函数，用来解析range头，并产生range响应的头。
ngx_http_chunked_filter_module	默认打开，对于 HTTP/1.1 和缺少 content-length 的回复自动打开。
ngx_http_header_filter_module	始终打开，用来将所有 header 组成一个完整的 HTTP 头。
ngx_http_write_filter_module	始终打开，将输出链拷贝到 r->out中，然后输出内容。
响应体过滤函数
响应体过滤函数是过滤响应主体的函数。ngx_http_top_body_filter 这个函数每个请求可能会被执行多次，它的入口函数是 ngx_http_output_filter，比如：

        ngx_int_t
        ngx_http_output_filter(ngx_http_request_t *r, ngx_chain_t *in)
        {
            ngx_int_t          rc;
            ngx_connection_t  *c;

            c = r->connection;

            rc = ngx_http_top_body_filter(r, in);

            if (rc == NGX_ERROR) {
                /* NGX_ERROR may be returned by any filter */
                c->error = 1;
            }

            return rc;
        }
ngx_http_output_filter 可以被一般的静态处理模块调用，也有可能是在 upstream 模块里面被调用，对于整个请求的处理阶段来说，他们处于的用处都是一样的，就是把响应内容过滤，然后发给客户端。

具体模块的响应体过滤函数的格式类似这样：

        static int 
        ngx_http_example_body_filter(ngx_http_request_t *r, ngx_chain_t *in)
        {
            ...

            return ngx_http_next_body_filter(r, in);
        }
该函数的返回值一般是 NGX_OK，NGX_ERROR 和 NGX_AGAIN，分别表示处理成功，失败和未完成。

主要功能介绍
响应的主体内容就存于单链表 in，链表一般不会太长，有时 in 参数可能为 NULL。in中存有buf结构体中，对于静态文件，这个buf大小默认是 32K；对于反向代理的应用，这个buf可能是4k或者8k。为了保持内存的低消耗，Nginx一般不会分配过大的内存，处理的原则是收到一定的数据，就发送出去。一个简单的例子，可以看看Nginx的chunked_filter模块，在没有 content-length 的情况下，chunk 模块可以流式（stream）的加上长度，方便浏览器接收和显示内容。

在响应体过滤模块中，尤其要注意的是 buf 的标志位，完整描述可以在“相关结构体”这个节中看到。如果 buf 中包含 last 标志，说明是最后一块 buf，可以直接输出并结束请求了。如果有 flush 标志，说明这块 buf 需要马上输出，不能缓存。如果整块 buffer 经过处理完以后，没有数据了，你可以把 buffer 的 sync 标志置上，表示只是同步的用处。

当所有的过滤模块都处理完毕时，在最后的 write_fitler 模块中，Nginx 会将 in 输出链拷贝到 r->out 输出链的末尾，然后调用 sendfile 或者 writev 接口输出。由于 Nginx 是非阻塞的 socket 接口，写操作并不一定会成功，可能会有部分数据还残存在 r->out。在下次的调用中，Nginx 会继续尝试发送，直至成功。

发出子请求
Nginx 过滤模块一大特色就是可以发出子请求，也就是在过滤响应内容的时候，你可以发送新的请求，Nginx 会根据你调用的先后顺序，将多个回复的内容拼接成正常的响应主体。一个简单的例子可以参考 addition 模块。

Nginx 是如何保证父请求和子请求的顺序呢？当 Nginx 发出子请求时，就会调用 ngx_http_subrequest 函数，将子请求插入父请求的 r->postponed 链表中。子请求会在主请求执行完毕时获得依次调用。子请求同样会有一个请求所有的生存期和处理过程，也会进入过滤模块流程。

关键点是在 postpone_filter 模块中，它会拼接主请求和子请求的响应内容。r->postponed 按次序保存有父请求和子请求，它是一个链表，如果前面一个请求未完成，那后一个请求内容就不会输出。当前一个请求完成时并输出时，后一个请求才可输出，当所有的子请求都完成时，所有的响应内容也就输出完毕了。

一些优化措施
Nginx 过滤模块涉及到的结构体，主要就是 chain 和 buf，非常简单。在日常的过滤模块中，这两类结构使用非常频繁，Nginx采用类似 freelist 重复利用的原则，将使用完毕的 chain 或者 buf 结构体，放置到一个固定的空闲链表里，以待下次使用。

比如，在通用内存池结构体中，pool->chain 变量里面就保存着释放的 chain。而一般的 buf 结构体，没有模块间公用的空闲链表池，都是保存在各模块的缓存空闲链表池里面。对于 buf 结构体，还有一种 busy 链表，表示该链表中的 buf 都处于输出状态，如果 buf 输出完毕，这些 buf 就可以释放并重复利用了。

功能	函数名
chain 分配	ngx_alloc_chain_link
chain 释放	ngx_free_chain
buf 分配	ngx_chain_get_free_buf
buf 释放	ngx_chain_update_chains
过滤内容的缓存
由于 Nginx 设计流式的输出结构，当我们需要对响应内容作全文过滤的时候，必须缓存部分的 buf 内容。该类过滤模块往往比较复杂，比如 sub，ssi，gzip 等模块。这类模块的设计非常灵活，我简单讲一下设计原则：

输入链 in 需要拷贝操作，经过缓存的过滤模块，输入输出链往往已经完全不一样了，所以需要拷贝，通过 ngx_chain_add_copy 函数完成。

一般有自己的 free 和 busy 缓存链表池，可以提高 buf 分配效率。

如果需要分配大块内容，一般分配固定大小的内存卡，并设置 recycled 标志，表示可以重复利用。

原有的输入 buf 被替换缓存时，必须将其 buf->pos 设为 buf->last，表明原有的 buf 已经被输出完毕。或者在新建立的 buf，将 buf->shadow 指向旧的 buf，以便输出完毕时及时释放旧的 buf。




upstream 模块简介

Nginx 模块一般被分成三大类：handler、filter 和 upstream。前面的章节中，读者已经了解了 handler、filter。利用这两类模块，可以使 Nginx 轻松完成任何单机工作。而本章介绍的 upstream 模块，将使 Nginx 跨越单机的限制，完成网络数据的接收、处理和转发。

数据转发功能，为 Nginx 提供了跨越单机的横向处理能力，使 Nginx 摆脱只能为终端节点提供单一功能的限制，而使它具备了网路应用级别的拆分、封装和整合的战略功能。在云模型大行其道的今天，数据转发是 Nginx 有能力构建一个网络应用的关键组件。当然，鉴于开发成本的问题，一个网络应用的关键组件一开始往往会采用高级编程语言开发。但是当系统到达一定规模，并且需要更重视性能的时候，为了达到所要求的性能目标，高级语言开发出的组件必须进行结构化修改。此时，对于修改代价而言，Nginx 的 upstream 模块呈现出极大的吸引力，因为它天生就快。作为附带，Nginx 的配置系统提供的层次化和松耦合使得系统的扩展性也达到比较高的程度。

言归正传，下面介绍 upstream 的写法。

upstream 模块接口
从本质上说，upstream 属于 handler，只是他不产生自己的内容，而是通过请求后端服务器得到内容，所以才称为 upstream（上游）。请求并取得响应内容的整个过程已经被封装到 Nginx 内部，所以 upstream 模块只需要开发若干回调函数，完成构造请求和解析响应等具体的工作。

这些回调函数如下表所示：

SN	描述
create_request	生成发送到后端服务器的请求缓冲（缓冲链），在初始化 upstream 时使用。
reinit_request	在某台后端服务器出错的情况，Nginx会尝试另一台后端服务器。Nginx 选定新的服务器以后，会先调用此函数，以重新初始化 upstream 模块的工作状态，然后再次进行 upstream 连接。
process_header	处理后端服务器返回的信息头部。所谓头部是与 upstreamserver 通信的协议规定的，比如 HTTP 协议的 header 部分，或者 memcached 协议的响应状态部分。
abort_request	在客户端放弃请求时被调用。不需要在函数中实现关闭后端服务器连接的功能，系统会自动完成关闭连接的步骤，所以一般此函数不会进行任何具体工作。
finalize_request	正常完成与后端服务器的请求后调用该函数，与 abort_request 相同，一般也不会进行任何具体工作。
input_filter	处理后端服务器返回的响应正文。Nginx 默认的 input_filter 会将收到的内容封装成为缓冲区链 ngx_chain。该链由 upstream 的 out_bufs 指针域定位，所以开发人员可以在模块以外通过该指针 得到后端服务器返回的正文数据。memcached 模块实现了自己的 input_filter，在后面会具体分析这个模块。
input_filter_init	初始化 input filter 的上下文。Nginx 默认的 input_filter_init 直接返回。
memcached 模块分析
memcache 是一款高性能的分布式 cache 系统，得到了非常广泛的应用。memcache 定义了一套私有通信协议，使得不能通过 HTTP 请求来访问 memcache。但协议本身简单高效，而且 memcache 使用广泛，所以大部分现代开发语言和平台都提供了 memcache 支持，方便开发者使用 memcache。

Nginx 提供了 ngx_http_memcached 模块，提供从 memcache 读取数据的功能，而不提供向 memcache 写数据的功能。作为 Web 服务器，这种设计是可以接受的。

下面，我们开始分析 ngx_http_memcached 模块，一窥 upstream 的奥秘。

Handler 模块？
初看 memcached 模块，大家可能觉得并无特别之处。如果稍微细看，甚至觉得有点像 handler 模块，当大家看到这段代码以后，必定疑惑为什么会跟 handler 模块一模一样。

        clcf = ngx_http_conf_get_module_loc_conf(cf, ngx_http_core_module);
        clcf->handler = ngx_http_memcached_handler;
因为 upstream 模块使用的就是 handler 模块的接入方式。同时，upstream 模块的指令系统的设计也是遵循 handler 模块的基本规则：配置该模块才会执行该模块。


        { ngx_string("memcached_pass"),
          NGX_HTTP_LOC_CONF|NGX_HTTP_LIF_CONF|NGX_CONF_TAKE1,
          ngx_http_memcached_pass,
          NGX_HTTP_LOC_CONF_OFFSET,
          0,
          NULL }
所以大家觉得眼熟是好事，说明大家对 Handler 的写法已经很熟悉了。

Upstream 模块
那么，upstream 模块的特别之处究竟在哪里呢？答案是就在模块处理函数的实现中。upstream 模块的处理函数进行的操作都包含一个固定的流程。在 memcached 的例子中，可以观察 ngx_http_memcached_handler 的代码，可以发现，这个固定的操作流程是：

创建 upstream 数据结构。
        if (ngx_http_upstream_create(r) != NGX_OK) {
            return NGX_HTTP_INTERNAL_SERVER_ERROR;
        }
设置模块的 tag 和 schema。schema 现在只会用于日志，tag 会用于 buf_chain 管理。
        u = r->upstream;

        ngx_str_set(&u->schema, "memcached://");
        u->output.tag = (ngx_buf_tag_t) &ngx_http_memcached_module;
设置 upstream 的后端服务器列表数据结构。
        mlcf = ngx_http_get_module_loc_conf(r, ngx_http_memcached_module);
        u->conf = &mlcf->upstream;
设置 upstream 回调函数。在这里列出的代码稍稍调整了代码顺序。
        u->create_request = ngx_http_memcached_create_request;
        u->reinit_request = ngx_http_memcached_reinit_request;
        u->process_header = ngx_http_memcached_process_header;
        u->abort_request = ngx_http_memcached_abort_request;
        u->finalize_request = ngx_http_memcached_finalize_request;
        u->input_filter_init = ngx_http_memcached_filter_init;
        u->input_filter = ngx_http_memcached_filter;
创建并设置 upstream 环境数据结构。
        ctx = ngx_palloc(r->pool, sizeof(ngx_http_memcached_ctx_t));
        if (ctx == NULL) {
            return NGX_HTTP_INTERNAL_SERVER_ERROR;
        }

        ctx->rest = NGX_HTTP_MEMCACHED_END;
        ctx->request = r;

        ngx_http_set_ctx(r, ctx, ngx_http_memcached_module);

        u->input_filter_ctx = ctx;
完成 upstream 初始化并进行收尾工作。
        r->main->count++;
        ngx_http_upstream_init(r);
        return NGX_DONE;
任何 upstream 模块，简单如 memcached，复杂如 proxy、fastcgi 都是如此。不同的 upstream 模块在这 6 步中的最大差别会出现在第 2、3、4、5 上。其中第 2、4 两步很容易理解，不同的模块设置的标志和使用的回调函数肯定不同。第 5 步也不难理解，只有第3步是最为晦涩的，不同的模块在取得后端服务器列表时，策略的差异非常大，有如 memcached 这样简单明了的，也有如 proxy 那样逻辑复杂的。这个问题先记下来，等把memcached剖析清楚了，再单独讨论。

第 6 步是一个常态。将 count 加 1，然后返回 NGX_DONE。Nginx 遇到这种情况，虽然会认为当前请求的处理已经结束，但是不会释放请求使用的内存资源，也不会关闭与客户端的连接。之所以需要这样，是因为 Nginx 建立了 upstream 请求和客户端请求之间一对一的关系，在后续使用 ngx_event_pipe 将 upstream 响应发送回客户端时，还要使用到这些保存着客户端信息的数据结构。这部分会在后面的原理篇做具体介绍，这里不再展开。

将 upstream 请求和客户端请求进行一对一绑定，这个设计有优势也有缺陷。优势就是简化模块开发，可以将精力集中在模块逻辑上，而缺陷同样明显，一对一的设计很多时候都不能满足复杂逻辑的需要。对于这一点，将会在后面的原理篇来阐述。

回调函数
前面剖析了 memcached 模块的骨架，现在开始逐个解决每个回调函数。

ngx_http_memcached_create_request：很简单的按照设置的内容生成一个 key，接着生成一个“get $key”的请求，放在 r->upstream->request_bufs 里面。

ngx_http_memcached_reinit_request：无需初始化。

ngx_http_memcached_abort_request：无需额外操作。

ngx_http_memcached_finalize_request：无需额外操作。

ngx_http_memcached_process_header：模块的业务重点函数。memcache 协议的头部信息被定义为第一行文本，可以找到这段代码证明：
        for (p = u->buffer.pos; p < u->buffer.last; p++) {
            if ( * p == LF) {
            goto found;
        }
如果在已读入缓冲的数据中没有发现 LF('\n')字符，函数返回 NGX_AGAIN，表示头部未完全读入，需要继续读取数据。Nginx 在收到新的数据以后会再次调用该函数。

Nginx 处理后端服务器的响应头时只会使用一块缓存，所有数据都在这块缓存中，所以解析头部信息时不需要考虑头部信息跨越多块缓存的情况。而如果头部过大，不能保存在这块缓存中，Nginx 会返回错误信息给客户端，并记录 error log，提示缓存不够大。

process_header 的重要职责是将后端服务器返回的状态翻译成返回给客户端的状态。例如，在 ngx_http_memcached_process_header 中，有这样几段代码：

        r->headers_out.content_length_n = ngx_atoof(len, p - len - 1);

        u->headers_in.status_n = 200;
        u->state->status = 200;

        u->headers_in.status_n = 404;
        u->state->status = 404;
u->state 用于计算 upstream 相关的变量。比如 u->state->status 将被用于计算变量“upstream_status”的值。u->headers_in 将被作为返回给客户端的响应返回状态码。而第一行则是设置返回给客户端的响应的长度。

在这个函数中不能忘记的一件事情是处理完头部信息以后需要将读指针 pos 后移，否则这段数据也将被复制到返回给客户端的响应的正文中，进而导致正文内容不正确。


        u->buffer.pos = p + 1;
process_header 函数完成响应头的正确处理，应该返回 NGX_OK。如果返回 NGX_AGAIN，表示未读取完整数据，需要从后端服务器继续读取数据。返回 NGX_DECLINED 无意义，其他任何返回值都被认为是出错状态，Nginx 将结束 upstream 请求并返回错误信息。

ngx_http_memcached_filter_init：修正从后端服务器收到的内容长度。因为在处理 header 时没有加上这部分长度。

ngx_http_memcached_filter：memcached 模块是少有的带有处理正文的回调函数的模块。因为 memcached 模块需要过滤正文末尾 CRLF "END" CRLF，所以实现了自己的 filter 回调函数。处理正文的实际意义是将从后端服务器收到的正文有效内容封装成 ngx_chain_t，并加在 u->out_bufs 末尾。Nginx 并不进行数据拷贝，而是建立 ngx_buf_t 数据结构指向这些数据内存区，然后由 ngx_chain_t 组织这些 buf。这种实现避免了内存大量搬迁，也是 Nginx 高效的奥秘之一。
本节回顾
这一节介绍了 upstream 模块的基本组成。upstream 模块是从 handler 模块发展而来，指令系统和模块生效方式与 handler 模块无异。不同之处在于，upstream 模块在 handler 函数中设置众多回调函数。实际工作都是由这些回调函数完成的。每个回调函数都是在 upstream 的某个固定阶段执行，各司其职，大部分回调函数一般不会真正用到。upstream 最重要的回调函数是 create_request、process_header 和 input_filter，他们共同实现了与后端服务器的协议的解析部分。




负载均衡模块

负载均衡模块用于从upstream指令定义的后端主机列表中选取一台主机。Nginx 先使用负载均衡模块找到一台主机，再使用 upstream 模块实现与这台主机的交互。为了方便介绍负载均衡模块，做到言之有物，以下选取 Nginx 内置的 ip hash 模块作为实际例子进行分析。

配置
要了解负载均衡模块的开发方法，首先需要了解负载均衡模块的使用方法。因为负载均衡模块与之前书中提到的模块差别比较大，所以我们从配置入手比较容易理解。

在配置文件中，我们如果需要使用 ip hash 的负载均衡算法。我们需要写一个类似下面的配置：

        upstream test {
            ip_hash;

            server 192.168.0.1;
            server 192.168.0.2;
        }
从配置我们可以看出负载均衡模块的使用场景：

核心指令ip_hash只能在 upstream {}中使用。这条指令用于通知 Nginx 使用 ip hash 负载均衡算法。如果没加这条指令，Nginx 会使用默认的 round robin 负载均衡模块。请各位读者对比 handler 模块的配置，是不是有共同点？
upstream {}中的指令可能出现在server指令前，可能出现在server指令后，也可能出现在两条server指令之间。各位读者可能会有疑问，有什么差别么？那么请各位读者尝试下面这个配置：
        upstream test {
            server 192.168.0.1 weight=5;
            ip_hash;
            server 192.168.0.2 weight=7;
        }
神奇的事情出现了：

        nginx: [emerg] invalid parameter "weight=7" in nginx.conf:103
        configuration file nginx.conf test failed
可见 ip_hash 指令的确能影响到配置的解析。

指令
配置决定指令系统，现在就来看 ip_hash 的指令定义：

    static ngx_command_t  ngx_http_upstream_ip_hash_commands[] = {

        { ngx_string("ip_hash"),
          NGX_HTTP_UPS_CONF|NGX_CONF_NOARGS,
          ngx_http_upstream_ip_hash,
          0,
          0,
          NULL },

        ngx_null_command
    };
没有特别的东西，除了指令属性是 NGX_HTTP_UPS_CONF。这个属性表示该指令的适用范围是 upstream{}。

钩子
以从前面的章节得到的经验，大家应该知道这里就是模块的切入点了。负载均衡模块的钩子代码都是有规律的，这里通过 ip_hash 模块来分析这个规律。

    static char *
    ngx_http_upstream_ip_hash(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)
    {
        ngx_http_upstream_srv_conf_t  *uscf;

        uscf = ngx_http_conf_get_module_srv_conf(cf, ngx_http_upstream_module);

        uscf->peer.init_upstream = ngx_http_upstream_init_ip_hash;

        uscf->flags = NGX_HTTP_UPSTREAM_CREATE
                    |NGX_HTTP_UPSTREAM_MAX_FAILS
                    |NGX_HTTP_UPSTREAM_FAIL_TIMEOUT
                    |NGX_HTTP_UPSTREAM_DOWN;

        return NGX_CONF_OK;
    }
这段代码中有两点值得我们注意。一个是 uscf->flags 的设置，另一个是设置 init_upstream 回调。

设置 uscf->flags
NGX_HTTP_UPSTREAM_CREATE：创建标志，如果含有创建标志的话，Nginx 会检查重复创建，以及必要参数是否填写；

NGX_HTTP_UPSTREAM_MAX_FAILS：可以在 server 中使用 max_fails 属性；

NGX_HTTP_UPSTREAM_FAIL_TIMEOUT：可以在 server 中使用 fail_timeout 属性；

NGX_HTTP_UPSTREAM_DOWN：可以在 server 中使用 down 属性；

NGX_HTTP_UPSTREAM_WEIGHT：可以在 server 中使用 weight 属性；

NGX_HTTP_UPSTREAM_BACKUP：可以在 server 中使用 backup 属性。
聪明的读者如果联想到刚刚遇到的那个神奇的配置错误，可以得出一个结论：在负载均衡模块的指令处理函数中可以设置并修改 upstream{} 中server指令支持的属性。这是一个很重要的性质，因为不同的负载均衡模块对各种属性的支持情况都是不一样的，那么就需要在解析配置文件的时候检测出是否使用了不支持的负载均衡属性并给出错误提示，这对于提升系统维护性是很有意义的。但是，这种机制也存在缺陷，正如前面的例子所示，没有机制能够追加检查在更新支持属性之前已经配置了不支持属性的server指令。

设置 init_upstream 回调
Nginx 初始化 upstream 时，会在 ngx_http_upstream_init_main_conf 函数中调用设置的回调函数初始化负载均衡模块。这里不太好理解的是 uscf 的具体位置。通过下面的示意图，说明 upstream 负载均衡模块的配置的内存布局。



从图上可以看出，MAIN_CONF 中 ngx_upstream_module 模块的配置项中有一个指针数组 upstreams，数组中的每个元素对应就是配置文件中每一个 upstream{}的信息。更具体的将会在后面的原理篇讨论。

初始化配置

init_upstream 回调函数执行时需要初始化负载均衡模块的配置，还要设置一个新钩子，这个钩子函数会在 Nginx 处理每个请求时作为初始化函数调用，关于这个新钩子函数的功能，后面会有详细的描述。这里，我们先分析 IP hash 模块初始化配置的代码：

    ngx_http_upstream_init_round_robin(cf, us);
    us->peer.init = ngx_http_upstream_init_ip_hash_peer;
这段代码非常简单：IP hash 模块首先调用另一个负载均衡模块 Round Robin 的初始化函数，然后再设置自己的处理请求阶段初始化钩子。实际上几个负载均衡模块可以组成一条链表，每次都是从链首的模块开始进行处理。如果模块决定不处理，可以将处理权交给链表中的下一个模块。这里，IP hash 模块指定 Round Robin 模块作为自己的后继负载均衡模块，所以在自己的初始化配置函数中也对 Round Robin 模块进行初始化。

初始化请求
Nginx 收到一个请求以后，如果发现需要访问 upstream，就会执行对应的 peer.init 函数。这是在初始化配置时设置的回调函数。这个函数最重要的作用是构造一张表，当前请求可以使用的 upstream 服务器被依次添加到这张表中。之所以需要这张表，最重要的原因是如果 upstream 服务器出现异常，不能提供服务时，可以从这张表中取得其他服务器进行重试操作。此外，这张表也可以用于负载均衡的计算。之所以构造这张表的行为放在这里而不是在前面初始化配置的阶段，是因为upstream需要为每一个请求提供独立隔离的环境。

为了讨论 peer.init 的核心，我们还是看 IP hash 模块的实现：

    r->upstream->peer.data = &iphp->rrp;

    ngx_http_upstream_init_round_robin_peer(r, us);

    r->upstream->peer.get = ngx_http_upstream_get_ip_hash_peer;
第一行是设置数据指针，这个指针就是指向前面提到的那张表；

第二行是调用 Round Robin 模块的回调函数对该模块进行请求初始化。面前已经提到，一个负载均衡模块可以调用其他负载均衡模块以提供功能的补充。

第三行是设置一个新的回调函数get。该函数负责从表中取出某个服务器。除了 get 回调函数，还有另一个r->upstream->peer.free的回调函数。该函数在 upstream 请求完成后调用，负责做一些善后工作。比如我们需要维护一个 upstream 服务器访问计数器，那么可以在 get 函数中对其加 1，在 free 中对其减 1。如果是 SSL 的话，Nginx 还提供两个回调函数 peer.set_session 和 peer.save_session。一般来说，有两个切入点实现负载均衡算法，其一是在这里，其二是在 get 回调函数中。

peer.get 和 peer.free 回调函数
这两个函数是负载均衡模块最底层的函数，负责实际获取一个连接和回收一个连接的预备操作。之所以说是预备操作，是因为在这两个函数中，并不实际进行建立连接或者释放连接的动作，而只是执行获取连接的地址或维护连接状态的操作。需要理解的清楚一点，在 peer.get 函数中获取连接的地址信息，并不代表这时连接一定没有被建立，相反的，通过 get 函数的返回值，Nginx 可以了解是否存在可用连接，连接是否已经建立。这些返回值总结如下：

返回值	说明	Nginx 后续动作
NGX_DONE	得到了连接地址信息，并且连接已经建立。	直接使用连接，发送数据。
NGX_OK	得到了连接地址信息，但连接并未建立。	建立连接，如连接不能立即建立，设置事件，
暂停执行本请求，执行别的请求。
NGX_BUSY	所有连接均不可用。	返回502错误至客户端。
各位读者看到上面这张表，可能会有几个问题浮现出来：

Q: 什么时候连接是已经建立的？

A: 使用后端 keepalive 连接的时候，连接在使用完以后并不关闭，而是存放在一个队列中，新的请求只需要从队列中取出连接，这些连接都是已经准备好的。

Q: 什么叫所有连接均不可用？

A: 初始化请求的过程中，建立了一张表，get 函数负责每次从这张表中不重复的取出一个连接，当无法从表中取得一个新的连接时，即所有连接均不可用。

Q: 对于一个请求，peer.get 函数可能被调用多次么？

A: 正式如此。当某次 peer.get 函数得到的连接地址连接不上，或者请求对应的服务器得到异常响应，Nginx 会执行 ngx_http_upstream_next，然后可能再次调用 peer.get 函数尝试别的连接。upstream 整体流程如下：



本节回顾
这一节介绍了负载均衡模块的基本组成。负载均衡模块的配置区集中在 upstream{}块中。负载均衡模块的回调函数体系是以 init_upstream 为起点，经历 init_peer，最终到达 peer.get 和 peer.free。其中 init_peer 负责建立每个请求使用的 server 列表，peer.get 负责从 server 列表中选择某个 server（一般是不重复选择），而 peer.free 负责 server 释放前的资源释放工作。最后，这一节通过一张图将 upstream 模块和负载均衡模块在请求处理过程中的相互关系展现出来。




core 模块

Nginx 的启动模块
启动模块从启动 Nginx 进程开始，做了一系列的初始化工作，源代码位于src/core/nginx.c，从 main 函数开始:

时间、正则、错误日志、ssl 等初始化
读入命令行参数
OS 相关初始化
读入并解析配置
核心模块初始化
创建各种暂时文件和目录
创建共享内存
打开 listen 的端口
所有模块初始化
启动 worker 进程




event 模块

event 的类型和功能
Nginx 是以 event（事件）处理模型为基础的模块。它为了支持跨平台，抽象出了 event 模块。它支持的 event 处理类型有：AIO（异步IO），/dev/poll（Solaris 和 Unix 特有），epoll（Linux 特有），eventport（Solaris 10 特有），kqueue（BSD 特有），poll，rtsig（实时信号），select 等。

event 模块的主要功能就是，监听 accept 后建立的连接，对读写事件进行添加删除。事件处理模型和 Nginx 的非阻塞 IO 模型结合在一起使用。当 IO 可读可写的时候，相应的读写事件就会被唤醒，此时就会去处理事件的回调函数。

特别对于 Linux，Nginx 大部分 event 采用 epoll EPOLLET（边沿触发）的方法来触发事件，只有 listen 端口的读事件是 EPOLLLT（水平触发）。对于边沿触发，如果出现了可读事件，必须及时处理，否则可能会出现读事件不再触发，连接饿死的情况。

typedef struct {
        /* 添加删除事件 */
        ngx_int_t  (*add)(ngx_event_t *ev, ngx_int_t event, ngx_uint_t flags);
        ngx_int_t  (*del)(ngx_event_t *ev, ngx_int_t event, ngx_uint_t flags);

        ngx_int_t  (*enable)(ngx_event_t *ev, ngx_int_t event, ngx_uint_t flags);
        ngx_int_t  (*disable)(ngx_event_t *ev, ngx_int_t event, ngx_uint_t flags);

        /* 添加删除连接，会同时监听读写事件 */
        ngx_int_t  (*add_conn)(ngx_connection_t *c);
        ngx_int_t  (*del_conn)(ngx_connection_t *c, ngx_uint_t flags);

        ngx_int_t  (*process_changes)(ngx_cycle_t *cycle, ngx_uint_t nowait);
        /* 处理事件的函数 */
        ngx_int_t  (*process_events)(ngx_cycle_t *cycle, ngx_msec_t timer,
                                   ngx_uint_t flags);

        ngx_int_t  (*init)(ngx_cycle_t *cycle, ngx_msec_t timer);
        void       (*done)(ngx_cycle_t *cycle);
} ngx_event_actions_t;
上述是 event 处理抽象出来的关键结构体，可以看到，每个 event 处理模型，都需要实现部分功能。最关键的是 add 和 del 功能，就是最基本的添加和删除事件的函数。

accept 锁
Nginx 是多进程程序，80 端口是各进程所共享的，多进程同时 listen 80 端口，势必会产生竞争，也产生了所谓的“惊群”效应。当内核 accept 一个连接时，会唤醒所有等待中的进程，但实际上只有一个进程能获取连接，其他的进程都是被无效唤醒的。所以 Nginx 采用了自有的一套 accept 加锁机制，避免多个进程同时调用 accept。Nginx 多进程的锁在底层默认是通过 CPU 自旋锁来实现。如果操作系统不支持自旋锁，就采用文件锁。

Nginx 事件处理的入口函数是 ngx_process_events_and_timers()，下面是部分代码，可以看到其加锁的过程：

if (ngx_use_accept_mutex) {
        if (ngx_accept_disabled > 0) {
                ngx_accept_disabled--;

        } else {
                if (ngx_trylock_accept_mutex(cycle) == NGX_ERROR) {
                        return;
                }

                if (ngx_accept_mutex_held) {
                        flags |= NGX_POST_EVENTS;

                } else {
                        if (timer == NGX_TIMER_INFINITE
                                || timer > ngx_accept_mutex_delay)
                        {
                                timer = ngx_accept_mutex_delay;
                        }
                }
        }
}
在 ngx_trylock_accept_mutex()函数里面，如果拿到了锁，Nginx 会把 listen 的端口读事件加入 event 处理，该进程在有新连接进来时就可以进行 accept 了。注意 accept 操作是一个普通的读事件。下面的代码说明了这点：

(void) ngx_process_events(cycle, timer, flags);

if (ngx_posted_accept_events) {
        ngx_event_process_posted(cycle, &ngx_posted_accept_events);
}

if (ngx_accept_mutex_held) {
        ngx_shmtx_unlock(&ngx_accept_mutex);
}
ngx_process_events()函数是所有事件处理的入口，它会遍历所有的事件。抢到了 accept 锁的进程跟一般进程稍微不同的是，它被加上了 NGX_POST_EVENTS 标志，也就是说在 ngx_process_events() 函数里面只接受而不处理事件，并加入 post_events 的队列里面。直到 ngx_accept_mutex 锁去掉以后才去处理具体的事件。为什么这样？因为 ngx_accept_mutex 是全局锁，这样做可以尽量减少该进程抢到锁以后，从 accept 开始到结束的时间，以便其他进程继续接收新的连接，提高吞吐量。

ngx_posted_accept_events 和 ngx_posted_events 就分别是 accept 延迟事件队列和普通延迟事件队列。可以看到 ngx_posted_accept_events 还是放到 ngx_accept_mutex 锁里面处理的。该队列里面处理的都是 accept 事件，它会一口气把内核 backlog 里等待的连接都 accept 进来，注册到读写事件里。

而 ngx_posted_events 是普通的延迟事件队列。一般情况下，什么样的事件会放到这个普通延迟队列里面呢？我的理解是，那些 CPU 耗时比较多的都可以放进去。因为 Nginx 事件处理都是根据触发顺序在一个大循环里依次处理的，因为 Nginx 一个进程同时只能处理一个事件，所以有些耗时多的事件会把后面所有事件的处理都耽搁了。

除了加锁，Nginx 也对各进程的请求处理的均衡性作了优化，也就是说，如果在负载高的时候，进程抢到的锁过多，会导致这个进程被禁止接受请求一段时间。

比如，在 ngx_event_accept 函数中，有类似代码：

ngx_accept_disabled = ngx_cycle->connection_n / 8
              - ngx_cycle->free_connection_n;
ngx_cycle->connection_n 是进程可以分配的连接总数，ngx_cycle->free_connection_n 是空闲的进程数。上述等式说明了，当前进程的空闲进程数小于 1/8 的话，就会被禁止 accept 一段时间。

定时器
Nginx 在需要用到超时的时候，都会用到定时器机制。比如，建立连接以后的那些读写超时。Nginx 使用红黑树来构造定期器，红黑树是一种有序的二叉平衡树，其查找插入和删除的复杂度都为 O(logn)，所以是一种比较理想的二叉树。

定时器的机制就是，二叉树的值是其超时时间，每次查找二叉树的最小值，如果最小值已经过期，就删除该节点，然后继续查找，直到所有超时节点都被删除。





安装 Nginx+Lua 开发环境

首先我们选择使用 OpenResty，其是由 Nginx 核心加很多第三方模块组成，其最大的亮点是默认集成了 Lua 开发环境，使得 Nginx 可以作为一个 Web Server 使用。借助于 Nginx 的事件驱动模型和非阻塞 IO，可以实现高性能的 Web 应用程序。而且 OpenResty 提供了大量组件如 Mysql、Redis、Memcached 等等，使在 Nginx 上开发Web 应用更方便更简单。目前在京东如实时价格、秒杀、动态服务、单品页、列表页等都在使用Nginx+Lua 架构，其他公司如淘宝、去哪儿网等。

安装环境
安装步骤可以参考 http://openresty.org/#Installation。

创建目录 /usr/servers，以后我们把所有软件安装在此目录

Java 代码 收藏代码

mkdir -p /usr/servers  
cd /usr/servers/  
安装依赖（我的环境是 ubuntu，可以使用如下命令安装，其他的可以参考 openresty 安装步骤）

Java 代码 收藏代码

apt-get install libreadline-dev libncurses5-dev libpcre3-dev libssl-dev perl  
下载 ngx_openresty-1.7.7.2.tar.gz 并解压

Java 代码 收藏代码

wget http://openresty.org/download/ngx_openresty-1.7.7.2.tar.gz  
tar -xzvf ngx_openresty-1.7.7.2.tar.gz  
ngx_openresty-1.7.7.2/bundle 目录里存放着 nginx 核心和很多第三方模块，比如有我们需要的 Lua 和 LuaJIT。

安装 LuaJIT

Java 代码 收藏代码

cd bundle/LuaJIT-2.1-20150120/  
make clean && make && make install  
ln -sf luajit-2.1.0-alpha /usr/local/bin/luajit  
下载 ngx_cache_purge 模块，该模块用于清理 nginx 缓存

Java 代码 收藏代码

cd /usr/servers/ngx_openresty-1.7.7.2/bundle  
wget https://github.com/FRiCKLE/ngx_cache_purge/archive/2.3.tar.gz  
tar -xvf 2.3.tar.gz  
下载 nginx_upstream_check_module 模块，该模块用于 ustream 健康检查

Java 代码 收藏代码

cd /usr/servers/ngx_openresty-1.7.7.2/bundle  
wget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v0.3.0.tar.gz  
tar -xvf v0.3.0.tar.gz   
安装 ngx_openresty

Java 代码 收藏代码

cd /usr/servers/ngx_openresty-1.7.7.2  
./configure --prefix=/usr/servers --with-http_realip_module  --with-pcre  --with-luajit --add-module=./bundle/ngx_cache_purge-2.3/ --add-module=./bundle/nginx_upstream_check_module-0.3.0/ -j2  
make && make install  
--with*** 安装一些内置/集成的模块
--with-http_realip_module 取用户真实 ip 模块
-with-pcre Perl 兼容的达式模块
--with-luajit 集成 luajit 模块
--add-module 添加自定义的第三方模块，如此次的 ngx_che_purge

到 /usr/servers 目录下

Java 代码 收藏代码

cd /usr/servers/    
ll   
会发现多出来了如下目录，说明安装成功
/usr/servers/luajit ：luajit 环境，luajit 类似于 java 的 jit，即即时编译，lua 是一种解释语言，通过 luajit 可以即时编译 lua 代码到机器代码，得到很好的性能；
/usr/servers/lualib：要使用的 lua 库，里边提供了一些默认的 lua 库，如 redis，json 库等，也可以把一些自己开发的或第三方的放在这；
/usr/servers/nginx ：安装的 nginx；

通过 /usr/servers/nginx/sbin/nginx -V 查看 nginx 版本和安装的模块

启动 nginx

/usr/servers/nginx/sbin/nginx

接下来该配置 nginx+lua 开发环境了

配置环境
配置及 Nginx HttpLuaModule 文档在可以查看 http://openresty.org/#Installation。

编辑 nginx.conf 配置文件

Java 代码 收藏代码

vim /usr/servers/nginx/conf/nginx.conf  
在 http 部分添加如下配置

Java 代码 收藏代码

\#lua模块路径，多个之间”;”分隔，其中”;;”表示默认搜索路径，默认到/usr/servers/nginx下找  
lua_package_path "/usr/servers/lualib/?.lua;;";  #lua 模块  
lua_package_cpath "/usr/servers/lualib/?.so;;";  #c模块   
为了方便开发我们在 /usr/servers/nginx/conf 目录下创建一个 lua.conf

Java 代码 收藏代码

\#lua.conf  
server {  
    listen       80;  
    server_name  _;  
}  
在 nginx.conf 中的 http 部分添加 include lua.conf 包含此文件片段

Java 代码 收藏代码

include lua.conf;  
测试是否正常

Java 代码 收藏代码

/usr/servers/nginx/sbin/nginx -t

如果显示如下内容说明配置成功

nginx: the configuration file /usr/servers/nginx/conf/nginx.conf syntax is ok
nginx: configuration file /usr/servers/nginx/conf/nginx.conf test is successful
HelloWorld
在 lua.conf 中 server 部分添加如下配置

Java 代码 收藏代码

location /lua {  
    default_type 'text/html';  
        content_by_lua 'ngx.say("hello world")';  
}  
测试配置是否正确

Java 代码 收藏代码

/usr/servers/nginx/sbin/nginx -t

重启 nginx

Java 代码 收藏代码

/usr/servers/nginx/sbin/nginx -s reload

访问如 http://192.168.1.6/lua（自己的机器根据实际情况换 ip），可以看到如下内容

hello world

lua 代码文件

我们把 lua 代码放在 nginx 配置中会随着 lua 的代码的增加导致配置文件太长不好维护，因此我们应该把 lua 代码移到外部文件中存储。

Java 代码 收藏代码

vim /usr/servers/nginx/conf/lua/test.lua    
Java 代码 收藏代码

\#添加如下内容  
ngx.say("hello world");   
然后 lua.conf 修改为

Java 代码 收藏代码

location /lua {  
    default_type 'text/html';  
    content_by_lua_file conf/lua/test.lua; #相对于nginx安装目录  
}    
此处 conf/lua/test.lua 也可以使用绝对路径 /usr/servers/nginx/conf/lua/test.lua。

lua_code_cache

默认情况下 lua_code_cache 是开启的，即缓存 lua 代码，即每次 lua 代码变更必须reload nginx 才生效，如果在开发阶段可以通过 lua_code_cache off;关闭缓存，这样调试时每次修改 lua 代码不需要 reload nginx；但是正式环境一定记得开启缓存。

Java 代码 收藏代码

    location /lua {  
        default_type 'text/html';  
        lua_code_cache off;  
        content_by_lua_file conf/lua/test.lua;  
}  
开启后 reload nginx 会看到如下报警

nginx: [alert] lua_code_cache is off; this will hurt performance in /usr/servers/nginx/conf/lua.conf:8

错误日志

如果运行过程中出现错误，请不要忘记查看错误日志。

Java 代码 收藏代码

tail -f /usr/servers/nginx/logs/error.log  
到此我们的基本环境搭建完毕。

nginx+lua 项目构建
以后我们的 nginx lua 开发文件会越来越多，我们应该把其项目化，已方便开发。项目目录结构如下所示：

example

example.conf     ---该项目的nginx 配置文件
lua              ---我们自己的lua代码
  test.lua
lualib            ---lua依赖库/第三方依赖
  *.lua
  *.so
其中我们把 lualib 也放到项目中的好处就是以后部署的时候可以一起部署，防止有的服务器忘记复制依赖而造成缺少依赖的情况。

我们将项目放到到 /usr/example 目录下。

/usr/servers/nginx/conf/nginx.conf 配置文件如下(此处我们最小化了配置文件)

Java 代码 收藏代码

\#user  nobody;  
worker_processes  2;  
error_log  logs/error.log;  
events {  
    worker_connections  1024;  
}  
http {  
    include       mime.types;  
    default_type  text/html;  

    #lua模块路径，其中”;;”表示默认搜索路径，默认到/usr/servers/nginx下找  
    lua_package_path "/usr/example/lualib/?.lua;;";  #lua 模块  
    lua_package_cpath "/usr/example/lualib/?.so;;";  #c模块  
    include /usr/example/example.conf;  
}    
通过绝对路径包含我们的 lua 依赖库和 nginx 项目配置文件。

/usr/example/example.conf 配置文件如下

Java 代码 收藏代码

server {  
    listen       80;  
    server_name  _;  

    location /lua {  
        default_type 'text/html';  
        lua_code_cache off;  
        content_by_lua_file /usr/example/lua/test.lua;  
    }  
}   
lua 文件我们使用绝对路径 /usr/example/lua/test.lua。

到此我们就可以把 example 扔 svn 上了。




Nginx+Lua 开发入门

Nginx 入门
本文目的是学习 Nginx+Lua 开发，对于 Nginx 基本知识可以参考如下文章：

nginx 启动、关闭、重启

http://www.cnblogs.com/derekchen/archive/2011/02/17/1957209.html

agentzh 的 Nginx 教程

http://openresty.org/download/agentzh-nginx-tutorials-zhcn.html

Nginx+Lua 入门

http://17173ops.com/2013/11/01/17173-ngx-lua-manual.shtml

nginx 配置指令的执行顺序

http://zhongfox.github.io/blog/server/2013/05/15/nginx-exec-order/

nginx 与 lua 的执行顺序和步骤说明

http://www.mrhaoting.com/?p=157

Nginx 配置文件 nginx.conf 中文详解

http://www.ha97.com/5194.html

Tengine 的 Nginx 开发从入门到精通

http://tengine.taobao.org/book/

官方文档

http://wiki.nginx.org/Configuration

Lua 入门

本文目的是学习 Nginx+Lua 开发，对于 Lua 基本知识可以参考如下文章：

Lua 简明教程

http://coolshell.cn/articles/10739.html

lua 在线 lua 学习教程

http://book.luaer.cn/

Lua 5.1 参考手册

http://www.codingnow.com/2000/download/lua_manual.html

Lua 5.3 参考手册

http://cloudwu.github.io/lua53doc/

Nginx Lua API
和一般的 Web Server 类似，我们需要接收请求、处理并输出响应。而对于请求我们需要获取如请求参数、请求头、Body 体等信息；而对于处理就是调用相应的 Lua 代码即可；输出响应需要进行响应状态码、响应头和响应内容体的输出。因此我们从如上几个点出发即可。

接收请求

example.conf 配置文件

Java 代码 收藏代码

location ~ /lua_request/(\d+)/(\d+) {  
    #设置nginx变量  
    set $a $1;   
    set $b $host;  
    default_type "text/html";  
    #nginx内容处理  
    content_by_lua_file /usr/example/lua/test_request.lua;  
    #内容体处理完成后调用  
    echo_after_body "ngx.var.b $b";  
}    
test_request.lua

Java 代码 收藏代码

--nginx变量  
local var = ngx.var  
ngx.say("ngx.var.a : ", var.a, "<br/>")  
ngx.say("ngx.var.b : ", var.b, "<br/>")  
ngx.say("ngx.var[2] : ", var[2], "<br/>")  
ngx.var.b = 2;  

ngx.say("<br/>")  

--请求头  
local headers = ngx.req.get_headers()  
ngx.say("headers begin", "<br/>")  
ngx.say("Host : ", headers["Host"], "<br/>")  
ngx.say("user-agent : ", headers["user-agent"], "<br/>")  
ngx.say("user-agent : ", headers.user_agent, "<br/>")  
for k,v in pairs(headers) do  
    if type(v) == "table" then  
        ngx.say(k, " : ", table.concat(v, ","), "<br/>")  
    else  
        ngx.say(k, " : ", v, "<br/>")  
    end  
end  
ngx.say("headers end", "<br/>")  
ngx.say("<br/>")  

--get请求uri参数  
ngx.say("uri args begin", "<br/>")  
local uri_args = ngx.req.get_uri_args()  
for k, v in pairs(uri_args) do  
    if type(v) == "table" then  
        ngx.say(k, " : ", table.concat(v, ", "), "<br/>")  
    else  
        ngx.say(k, ": ", v, "<br/>")  
    end  
end  
ngx.say("uri args end", "<br/>")  
ngx.say("<br/>")  

--post请求参数  
ngx.req.read_body()  
ngx.say("post args begin", "<br/>")  
local post_args = ngx.req.get_post_args()  
for k, v in pairs(post_args) do  
    if type(v) == "table" then  
        ngx.say(k, " : ", table.concat(v, ", "), "<br/>")  
    else  
        ngx.say(k, ": ", v, "<br/>")  
    end  
end  
ngx.say("post args end", "<br/>")  
ngx.say("<br/>")  
--请求的http协议版本  
ngx.say("ngx.req.http_version : ", ngx.req.http_version(), "<br/>")  
--请求方法  
ngx.say("ngx.req.get_method : ", ngx.req.get_method(), "<br/>")  
--原始的请求头内容  
ngx.say("ngx.req.raw_header : ",  ngx.req.raw_header(), "<br/>")  
--请求的body内容体  
ngx.say("ngx.req.get_body_data() : ", ngx.req.get_body_data(), "<br/>")  
ngx.say("<br/>")   
ngx.var ： nginx 变量，如果要赋值如 ngx.var.b = 2，此变量必须提前声明；另外对于 nginx location 中使用正则捕获的捕获组可以使用 ngx.var [捕获组数字]获取；

ngx.req.get_headers：获取请求头，默认只获取前100，如果想要获取所以可以调用ngx.req.get_headers(0)；获取带中划线的请求头时请使用如 headers.user_agent 这种方式；如果一个请求头有多个值，则返回的是 table；

ngx.req.get_uri_args：获取 url 请求参数，其用法和 get_headers 类似；

ngx.req.get_post_args：获取 post 请求内容体，其用法和 get_headers 类似，但是必须提前调用 ngx.req.read_body() 来读取 body 体（也可以选择在 nginx 配置文件使用lua_need_request_body on;开启读取 body 体，但是官方不推荐）；

ngx.req.raw_header：未解析的请求头字符串；

ngx.req.get_body_data：为解析的请求 body 体内容字符串。

如上方法处理一般的请求基本够用了。另外在读取 post 内容体时根据实际情况设置 client_body_buffer_size 和 client_max_body_size 来保证内容在内存而不是在文件中。

使用如下脚本测试

Java 代码 收藏代码

wget --post-data 'a=1&b=2' 'http://127.0.0.1/lua_request/1/2?a=3&b=4' -O -   
输出响应

example.conf 配置文件

Java 代码 收藏代码

location /lua_response_1 {  
    default_type "text/html";  
    content_by_lua_file /usr/example/lua/test_response_1.lua;  
}    
test_response_1.lua

Java 代码 收藏代码

--写响应头  
ngx.header.a = "1"  
--多个响应头可以使用table  
ngx.header.b = {"2", "3"}  
--输出响应  
ngx.say("a", "b", "<br/>")  
ngx.print("c", "d", "<br/>")  
--200状态码退出  
return ngx.exit(200)  
ngx.header：输出响应头；
ngx.print：输出响应内容体；
ngx.say：通ngx.print，但是会最后输出一个换行符；
ngx.exit：指定状态码退出。
example.conf 配置文件

Java 代码 收藏代码

location /lua_response_2 {  
    default_type "text/html";  
    content_by_lua_file /usr/example/lua/test_response_2.lua;  
}  
test_response_2.lua

Java 代码 收藏代码

ngx.redirect("http://jd.com", 302)    
ngx.redirect：重定向；

ngx.status= 状态码，设置响应的状态码；ngx.resp.get_headers() 获取设置的响应状态码；ngx.send_headers() 发送响应状态码，当调用 ngx.say/ngx.print 时自动发送响应状态码；可以通过 ngx.headers_sent=true 判断是否发送了响应状态码。

其他 API

example.conf 配置文件

Java 代码 收藏代码

location /lua_other {  
    default_type "text/html";  
    content_by_lua_file /usr/example/lua/test_other.lua;  
}  
test_other.lua

Java 代码 收藏代码

--未经解码的请求uri  
local request_uri = ngx.var.request_uri;  
ngx.say("request_uri : ", request_uri, "<br/>");  
--解码  
ngx.say("decode request_uri : ", ngx.unescape_uri(request_uri), "<br/>");  
--MD5  
ngx.say("ngx.md5 : ", ngx.md5("123"), "<br/>")  
--http time  
ngx.say("ngx.http_time : ", ngx.http_time(ngx.time()), "<br/>")  
ngx.escape_uri/ngx.unescape_uri ： uri 编码解码；
ngx.encode_args/ngx.decode_args：参数编码解码；
ngx.encode_base64/ngx.decode_base64：BASE64 编码解码；
ngx.re.match：nginx 正则表达式匹配；
更多 Nginx Lua API 请参考 http://wiki.nginx.org/HttpLuaModule#Nginx_API_for_Lua。

Nginx 全局内存

使用过如 Java 的朋友可能知道如 Ehcache 等这种进程内本地缓存，Nginx 是一个 Master 进程多个 Worker 进程的工作方式，因此我们可能需要在多个 Worker 进程中共享数据，那么此时就可以使用 ngx.shared.DICT 来实现全局内存共享。

首先在 nginx.conf 的 http 部分分配内存大小

Java 代码 收藏代码

\#共享全局变量，在所有worker间共享  
lua_shared_dict shared_data 1m;  
example.conf 配置文件

Java 代码 收藏代码

location /lua_shared_dict {  
    default_type "text/html";  
    content_by_lua_file /usr/example/lua/test_lua_shared_dict.lua;  
}    
test_lua_shared_dict.lua

Java 代码 收藏代码

--1、获取全局共享内存变量  
local shared_data = ngx.shared.shared_data  

--2、获取字典值  
local i = shared_data:get("i")  
if not i then  
    i = 1  
    --3、惰性赋值  
    shared_data:set("i", i)  
    ngx.say("lazy set i ", i, "<br/>")  
end  
--递增  
i = shared_data:incr("i", 1)  
ngx.say("i=", i, "<br/>")   
更多 API 请参考 http://wiki.nginx.org/HttpLuaModule#ngx.shared.DICT。

到此基本的 Nginx Lua API 就学完了，对于请求处理和输出响应如上介绍的 API 完全够用了，更多 API 请参考官方文档。

Nginx Lua 模块指令
Nginx 共11个处理阶段，而相应的处理阶段是可以做插入式处理，即可插拔式架构；另外指令可以在 http、server、server if、location、location if 几个范围进行配置：

指令

所处处理阶段

使用范围

解释

init_by_lua

init_by_lua_file

loading-config

http

nginx Master进程加载配置时执行；

通常用于初始化全局配置/预加载Lua模块

init_worker_by_lua

init_worker_by_lua_file

starting-worker

http

每个Nginx Worker进程启动时调用的计时器，如果Master进程不允许则只会在init_by_lua之后调用；

通常用于定时拉取配置/数据，或者后端服务的健康检查

set_by_lua

set_by_lua_file

rewrite

server,server if,location,location if

设置nginx变量，可以实现复杂的赋值逻辑；此处是阻塞的，Lua代码要做到非常快；

rewrite_by_lua

rewrite_by_lua_file

rewrite tail

http,server,location,location if

rrewrite阶段处理，可以实现复杂的转发/重定向逻辑；

access_by_lua

access_by_lua_file

access tail

http,server,location,location if

请求访问阶段处理，用于访问控制

content_by_lua

content_by_lua_file

content

location，location if

内容处理器，接收请求处理并输出响应

header_filter_by_lua

header_filter_by_lua_file

output-header-filter

http，server，location，location if

设置header和cookie

body_filter_by_lua

body_filter_by_lua_file

output-body-filter

http，server，location，location if

对响应数据进行过滤，比如截断、替换。

log_by_lua

log_by_lua_file

log

http，server，location，location if

log阶段处理，比如记录访问量/统计平均响应时间

更详细的解释请参考 http://wiki.nginx.org/HttpLuaModule#Directives。如上指令很多并不常用，因此我们只拿其中的一部分做演示。

init_by_lua

每次 Nginx 重新加载配置时执行，可以用它来完成一些耗时模块的加载，或者初始化一些全局配置；在 Master 进程创建 Worker 进程时，此指令中加载的全局变量会进行 Copy-OnWrite，即会复制到所有全局变量到 Worker 进程。

nginx.conf 配置文件中的 http 部分添加如下代码

Java 代码 收藏代码

\#共享全局变量，在所有worker间共享  
lua_shared_dict shared_data 1m;  

init_by_lua_file /usr/example/lua/init.lua;  
init.lua

Java 代码 收藏代码

--初始化耗时的模块  
local redis = require 'resty.redis'  
local cjson = require 'cjson'  

--全局变量，不推荐  
count = 1  

--共享全局内存  
local shared_data = ngx.shared.shared_data  
shared_data:set("count", 1)  
test.lua

Java 代码 收藏代码

count = count + 1  
ngx.say("global variable : ", count)  
local shared_data = ngx.shared.shared_data  
ngx.say(", shared memory : ", shared_data:get("count"))  
shared_data:incr("count", 1)  
ngx.say("hello world")  
访问如 http://192.168.1.2/lua 会发现全局变量一直不变，而共享内存一直递增

global variable : 2 , shared memory : 8 hello world

另外注意一定在生产环境开启 lua_code_cache，否则每个请求都会创建 Lua VM 实例。

init_worker_by_lua

用于启动一些定时任务，比如心跳检查，定时拉取服务器配置等等；此处的任务是跟 Worker 进程数量有关系的，比如有2个 Worker 进程那么就会启动两个完全一样的定时任务。

nginx.conf 配置文件中的 http 部分添加如下代码

Java 代码 收藏代码

init_worker_by_lua_file /usr/example/lua/init_worker.lua;  
init_worker.lua

Java 代码 收藏代码

local count = 0  
local delayInSeconds = 3  
local heartbeatCheck = nil  

heartbeatCheck = function(args)  
   count = count + 1  
   ngx.log(ngx.ERR, "do check ", count)  

   local ok, err = ngx.timer.at(delayInSeconds, heartbeatCheck)  

   if not ok then  
      ngx.log(ngx.ERR, "failed to startup heartbeart worker...", err)  
   end  
end  

heartbeatCheck()  
ngx.timer.at：延时调用相应的回调方法；ngx.timer.at(秒单位延时，回调函数，回调函数的参数列表)；可以将延时设置为0即得到一个立即执行的任务，任务不会在当前请求中执行不会阻塞当前请求，而是在一个轻量级线程中执行。

另外根据实际情况设置如下指令

lua_max_pending_timers 1024; #最大等待任务数
lua_max_running_timers 256; #最大同时运行任务数
set_by_lua

设置 nginx 变量，我们用的 set 指令即使配合 if 指令也很难实现负责的赋值逻辑；

example.conf 配置文件

Java 代码 收藏代码

location /lua_set_1 {  
    default_type "text/html";  
    set_by_lua_file $num /usr/example/lua/test_set_1.lua;  
    echo $num;  
}    
set_by_lua_file：语法 set_by_lua_file $var lua_file arg1 arg2...; 在 lua代码中可以实现所有复杂的逻辑，但是要执行速度很快，不要阻塞；

test_set_1.lua

Java 代码 收藏代码

local uri_args = ngx.req.get_uri_args()  
local i = uri_args["i"] or 0  
local j = uri_args["j"] or 0  

return i + j   
得到请求参数进行相加然后返回。

访问如 http://192.168.1.2/lua_set_1?i=1&j=10 进行测试。 如果我们用纯 set 指令是无法实现的。

再举个实际例子，我们实际工作时经常涉及到网站改版，有时候需要新老并存，或者切一部分流量到新版

首先在 example.conf 中使用 map 指令来映射 host 到指定 nginx 变量，方便我们测试

Java 代码 收藏代码

############ 测试时使用的动态请求  
map $host $item_dynamic {  
    default                     "0";  
    item2014.jd.com            "1";  
}   
如绑定 hosts

192.168.1.2 item.jd.com;
192.168.1.2 item2014.jd.com;
此时我们想访问 item2014.jd.com 时访问新版，那么我们可以简单的使用如

Java 代码 收藏代码

if ($item_dynamic = "1") {  
   proxy_pass http://new;  
}  
proxy_pass http://old;  
但是我们想把商品编号为 8 位(比如品类为图书的)没有改版完成，需要按照相应规则跳转到老版，但是其他的到新版；虽然使用 if 指令能实现，但是比较麻烦，基本需要这样

Java 代码 收藏代码

set jump "0";  
if($item_dynamic = "1") {  
    set $jump "1";  
}  
if(uri ~ "^/6[0-9]{7}.html") {  
   set $jump "${jump}2";  
}   
\#非强制访问新版，且访问指定范围的商品  
if (jump == "02") {  
   proxy_pass http://old;  
}  
proxy_pass http://new;   
以上规则还是比较简单的，如果涉及到更复杂的多重 if/else 或嵌套 if/else 实现起来就更痛苦了，可能需要到后端去做了；此时我们就可以借助 lua 了：

Java 代码 收藏代码

set_by_lua $to_book '  
     local ngx_match = ngx.re.match  
     local var = ngx.var  
     local skuId = var.skuId  
     local r = var.item_dynamic ~= "1" and ngx.re.match(skuId, "^[0-9]{8}$")  
     if r then return "1" else return "0" end;  
';  
set_by_lua $to_mvd '  
     local ngx_match = ngx.re.match  
     local var = ngx.var  
     local skuId = var.skuId  
     local r = var.item_dynamic ~= "1" and ngx.re.match(skuId, "^[0-9]{9}$")  
     if r then return "1" else return "0" end;  
';  
\#自营图书  
if ($to_book) {  
    proxy_pass http://127.0.0.1/old_book/$skuId.html;  
}  
\#自营音像  
if ($to_mvd) {  
    proxy_pass http://127.0.0.1/old_mvd/$skuId.html;  
}  
\#默认  
proxy_pass http://127.0.0.1/proxy/$skuId.html;  
rewrite_by_lua

执行内部 URL 重写或者外部重定向，典型的如伪静态化的 URL 重写。其默认执行在 rewrite 处理阶段的最后。

example.conf 配置文件

Java 代码 收藏代码

location /lua_rewrite_1 {  
    default_type "text/html";  
    rewrite_by_lua_file /usr/example/lua/test_rewrite_1.lua;  
    echo "no rewrite";  
}  
test_rewrite_1.lua

Java 代码 收藏代码

if ngx.req.get_uri_args()["jump"] == "1" then  
   return ngx.redirect("http://www.jd.com?jump=1", 302)  
end    
当我们请求 http://192.168.1.2/lua_rewrite_1 时发现没有跳转，而请求 http://192.168.1.2/lua_rewrite_1?jump=1 时发现跳转到京东首页了。 此处需要301/302跳转根据自己需求定义。

example.conf 配置文件

Java 代码 收藏代码

location /lua_rewrite_2 {  
    default_type "text/html";  
    rewrite_by_lua_file /usr/example/lua/test_rewrite_2.lua;  
    echo "rewrite2 uri : $uri, a : $arg_a";  
}  
test_rewrite_2.lua

Java 代码 收藏代码

if ngx.req.get_uri_args()["jump"] == "1" then  
   ngx.req.set_uri("/lua_rewrite_3", false);  
   ngx.req.set_uri("/lua_rewrite_4", false);  
   ngx.req.set_uri_args({a = 1, b = 2});  
end     
ngx.req.set_uri(uri, false)：可以内部重写 uri（可以带参数），等价于 rewrite ^ /lua_rewrite_3；通过配合 if/else 可以实现 rewrite ^ /lua_rewrite_3 break；这种功能；此处两者都是 location 内部 url 重写，不会重新发起新的 location 匹配；

ngx.req.set_uri_args：重写请求参数，可以是字符串(a=1&b=2)也可以是 table；

访问如 http://192.168.1.2/lua_rewrite_2?jump=0 时得到响应 rewrite2 uri : /lua_rewrite_2, a :

访问如 http://192.168.1.2/lua_rewrite_2?jump=1 时得到响应 rewrite2 uri : /lua_rewrite_4, a : 1

example.conf 配置文件

Java 代码 收藏代码

location /lua_rewrite_3 {  
    default_type "text/html";  
    rewrite_by_lua_file /usr/example/lua/test_rewrite_3.lua;  
    echo "rewrite3 uri : $uri";  
}  
c test_rewrite_3.lua

Java 代码 收藏代码

if ngx.req.get_uri_args()["jump"] == "1" then  
   ngx.req.set_uri("/lua_rewrite_4", true);  
   ngx.log(ngx.ERR, "=========")  
   ngx.req.set_uri_args({a = 1, b = 2});  
end    
ngx.req.set_uri(uri, true)：可以内部重写 uri，即会发起新的匹配 location 请求，等价于 rewrite ^ /lua_rewrite_4 last；此处看 error log 是看不到我们记录的log。

所以请求如 http://192.168.1.2/lua_rewrite_3?jump=1 会到新的 location 中得到响应，此处没有 /lua_rewrite_4，所以匹配到 /lua 请求，得到类似如下的响应 global variable : 2 , shared memory : 1 hello world

即

rewrite ^ /lua_rewrite_3;                 等价于  ngx.req.set_uri("/lua_rewrite_3", false);
rewrite ^ /lua_rewrite_3 break;       等价于  ngx.req.set_uri("/lua_rewrite_3", false); 加 if/else判断/break/return
rewrite ^ /lua_rewrite_4 last;           等价于  ngx.req.set_uri("/lua_rewrite_4", true);
注意，在使用 rewrite_by_lua 时，开启 rewrite_log on;后也看不到相应的 rewrite log。

access_by_lua

用于访问控制，比如我们只允许内网 ip 访问，可以使用如下形式

Java 代码 收藏代码

allow     127.0.0.1;  
allow     10.0.0.0/8;  
allow     192.168.0.0/16;  
allow     172.16.0.0/12;  
deny      all;  
example.conf 配置文件

Java 代码 收藏代码

location /lua_access {  
    default_type "text/html";  
    access_by_lua_file /usr/example/lua/test_access.lua;  
    echo "access";  
}  
test_access.lua

Java 代码 收藏代码

if ngx.req.get_uri_args()["token"] ~= "123" then  
   return ngx.exit(403)  
end    
即如果访问如 http://192.168.1.2/lua_access?token=234 将得到 403 Forbidden 的响应。这样我们可以根据如 cookie/ 用户 token 来决定是否有访问权限。

content_by_lua

此指令之前已经用过了，此处就不讲解了。

另外在使用 PCRE 进行正则匹配时需要注意正则的写法，具体规则请参考 http://wiki.nginx.org/HttpLuaModule中的Special PCRE Sequences部 分。还有其他的注意事项也请阅读官方文档。




Redis/SSDB+Twemproxy 安装与使用

目前对于互联网公司不使用 Redis 的很少，Redis 不仅仅可以作为 key-value 缓存，而且提供了丰富的数据结果如 set、list、map 等，可以实现很多复杂的功能；但是 Redis 本身主要用作内存缓存，不适合做持久化存储，因此目前有如 SSDB、ARDB 等，还有如京东的 JIMDB，它们都支持 Redis 协议，可以支持 Redis 客户端直接访问；而这些持久化存储大多数使用了如LevelDB、RocksDB、LMDB 持久化引擎来实现数据的持久化存储；京东的 JIMDB 主要分为两个版本：LevelDB 和 LMDB，而我们看到的京东商品详情页就是使用 LMDB 引擎作为存储的，可以实现海量KV存储；当然 SSDB 在京东内部也有些部门在使用；另外调研过得如豆瓣的 beansDB 也是很不错的。具体这些持久化引擎之间的区别可以自行查找资料学习。

Twemproxy 是一个 Redis/Memcached 代理中间件，可以实现诸如分片逻辑、HashTag、减少连接数等功能；尤其在有大量应用服务器的场景下 Twemproxy 的角色就凸显了，能有效减少连接数。

Redis 安装与使用
下载 redis 并安装

Java 代码

cd /usr/servers/  
wget https://github.com/antirez/redis/archive/2.8.19.tar.gz  
tar -xvf 2.8.19.tar.gz  
cd redis-2.8.19/  
make     
通过如上步骤构建完毕。

后台启动 Redis 服务器

Java 代码

nohup /usr/servers/redis-2.8.19/src/redis-server  /usr/servers/redis-2.8.19/redis.conf &  
查看是否启动成功

Java 代码

ps -aux | grep redis  
进入客户端

Java 代码

/usr/servers/redis-2.8.19/src/redis-cli  -p 6379  
执行如下命令

Java 代码

127.0.0.1:6379> set i 1  
OK  
127.0.0.1:6379> get i  
"1"     
通过如上命令可以看到我们的 Redis 安装成功。更多细节请参考 http://redis.io/。

SSDB 安装与使用
下载 SSDB 并安装

Java 代码

\#首先确保安装了g++，如果没有安装，如ubuntu可以使用如下命令安装  
apt-get install g++  
cd /usr/servers  
wget https://github.com/ideawu/ssdb/archive/1.8.0.tar.gz  
tar -xvf 1.8.0.tar.gz  
make   
后台启动 SSDB 服务器

Java 代码

nohup /usr/servers/ssdb-1.8.0/ssdb-server  /usr/servers/ssdb-1.8.0/ssdb.conf &  
查看是否启动成功

Java 代码

ps -aux | grep ssdb  
进入客户端

Java 代码

/usr/servers/ssdb-1.8.0/tools/ssdb-cli -p 8888  
/usr/servers/redis-2.8.19/src/redis-cli  -p 888  
因为 SSDB 支持 Redis 协议，所以用 Redis 客户端也可以访问

执行如下命令

Java 代码

127.0.0.1:8888> set i 1  
OK  
127.0.0.1:8888> get i  
"1"    
安装过程中遇到错误请参考 http://ssdb.io/docs/zh_cn/install.html；对于 SSDB 的配置请参考官方文档 https://github.com/ideawu/ssdb[http://ssdb.io/docs/zh_cn/install.html](http://ssdb.io/docs/zh_cn/install.html)。

Twemproxy 安装与使用
首先需要安装 autoconf、automake、libtool 工具，比如 ubuntu 可以使用如下命令安装

Java 代码

apt-get install autoconf automake  
apt-get install libtool  
下载 Twemproxy 并安装

Java 代码

cd /usr/servers  
wget https://github.com/twitter/twemproxy/archive/v0.4.0.tar.gz  
tar -xvf v0.4.0.tar.gz    
cd twemproxy-0.4.0/  
autoreconf -fvi  
./configure && make    
此处根据要注意，如上安装方式在有些服务器上可能在大量如mset时可能导致 Twemproxy 崩溃，需要使用如 CFLAGS="-O1" ./configure && make 或 CFLAGS="-O3 -fno-strict-aliasing" ./configure && make 安装。

配置

Java 代码

vim /usr/servers/twemproxy-0.4.0/conf/nutcracker.yml  
Java 代码

server1:  
  listen: 127.0.0.1:1111  
  hash: fnv1a_64  
  distribution: ketama  
  redis: true  
  servers:  
   - 127.0.0.1:6379:1  
启动 Twemproxy 代理

Java 代码

/usr/servers/twemproxy-0.4.0/src/nutcracker  -d -c /usr/servers/twemproxy-0.4.0/conf/nutcracker.yml    
-d 指定后台启动 -c 指定配置文件；此处我们指定了代理端口为 1111，其他配置的含义后续介绍。

查看是否启动成功

Java 代码

ps -aux | grep nutcracker  
进入客户端

Java 代码

/usr/servers/redis-2.8.19/src/redis-cli  -p 1111  
执行如下命令

Java 代码

127.0.0.1:1111> set i 1  
OK  
127.0.0.1:1111> get i  
"1"     
Twemproxy 文档请参考 https://github.com/twitter/twemproxy。

到此基本的安装就完成了。接下来做一些介绍。

Redis 设置
基本设置

Java 代码

\#端口设置，默认6379    
port 6379    
\#日志文件，默认/dev/null    
logfile ""     
Redis 内存

Java 代码

内存大小对应关系   
\# 1k => 1000 bytes  
\# 1kb => 1024 bytes  
\# 1m => 1000000 bytes  
\# 1mb => 1024*1024 bytes  
\# 1g => 1000000000 bytes  
\# 1gb => 1024*1024*1024 bytes  

\#设置Redis占用100mb的大小  
maxmemory  100mb  

\#如果内存满了就需要按照如相应算法进行删除过期的/最老的  
\#volatile-lru 根据LRU算法移除设置了过期的key  
\#allkeys-lru  根据LRU算法移除任何key(包含那些未设置过期时间的key)  
\#volatile-random/allkeys->random 使用随机算法而不是LRU进行删除  
\#volatile-ttl 根据Time-To-Live移除即将过期的key   
\#noeviction   永不过期，而是报错  
maxmemory-policy volatile-lru  

\#Redis并不是真正的LRU/TTL，而是基于采样进行移除的，即如采样10个数据移除其中最老的/即将过期的  
maxmemory-samples 10   
而如 Memcached 是真正的 LRU，此处要根据实际情况设置缓存策略，如缓存用户数据时可能带上了过期时间，此时采用 volatile-lru 即可；而假设我们的数据未设置过期时间，此时可以考虑使用 allkeys-lru/allkeys->random；假设我们的数据不允许从内存删除那就使用noeviction。

内存大小尽量在系统内存的 60%~80% 之间，因为如客户端、主从时复制时都需要缓存区的，这些也是耗费系统内存的。

Redis 本身是单线程的，因此我们可以设置每个实例在 6-8GB 之间，通过启动更多的实例提高吞吐量。如 128GB 的我们可以开启 8GB * 10 个实例，充分利用多核 CPU。

Redis 主从

实际项目时，为了提高吞吐量，我们使用主从策略，即数据写到主 Redis，读的时候从从 Redis上读，这样可以通过挂载更多的从来提高吞吐量。而且可以通过主从机制，在叶子节点开启持久化方式防止数据丢失。

Java 代码

\#在配置文件中挂载主从，不推荐这种方式，我们实际应用时Redis可能是会宕机的  
slaveof masterIP masterPort  
\#从是否只读，默认yes  
slave-read-only yes  
\#当从失去与主的连接或者复制正在进行时，从是响应客户端（可能返回过期的数据）还是返回“SYNC with master in progress”错误，默认yes响应客户端  
slave-serve-stale-data yes  
\#从库按照默认10s的周期向主库发送PING测试连通性  
repl-ping-slave-period 10  
\#设置复制超时时间（SYNC期间批量I/O传输、PING的超时时间），确保此值大于repl-ping-slave-period  
\#repl-timeout 60  
\#当从断开与主的连接时的复制缓存区，仅当第一个从断开时创建一个，缓存区越大从断开的时间可以持续越长  
\# repl-backlog-size 1mb  
\#当从与主断开持续多久时清空复制缓存区，此时从就需要全量复制了，如果设置为0将永不清空    
\# repl-backlog-ttl 3600  
\#slave客户端缓存区，如果缓存区超过256mb将直接断开与从的连接，如果持续60秒超过64mb也会断开与从的连接  
client-output-buffer-limit slave 256mb 64mb 60   
此处需要根据实际情况设置client-output-buffer-limit slave和 repl-backlog-size；比如如果网络环境不好，从与主经常断开，而每次设置的数据都特别大而且速度特别快（大量设置html片段）那么就需要加大repl-backlog-size。 
主从示例

Java 代码

cd /usr/servers/redis-2.8.19  
cp redis.conf redis_6660.conf  
cp redis.conf redis_6661.conf  
vim redis_6660.conf  
vim redis_6661.conf   
将端口分别改为 port 6660 和 port 6661，然后启动

Java 代码

nohup /usr/servers/redis-2.8.19/src/redis-server  /usr/servers/redis-2.8.19/redis_6660.conf &  
nohup /usr/servers/redis-2.8.19/src/redis-server  /usr/servers/redis-2.8.19/redis_6661.conf &    
查看是否启动

Java 代码

ps -aux | grep redis  
进入从客户端，挂主

Java 代码

/usr/servers/redis-2.8.19/src/redis-cli  -p 6661    
Java 代码

127.0.0.1:6661> slaveof 127.0.0.1 6660  
OK  
127.0.0.1:6661> info replication  
\# Replication  
role:slave  
master_host:127.0.0.1  
master_port:6660  
master_link_status:up  
master_last_io_seconds_ago:3  
master_sync_in_progress:0  
slave_repl_offset:57  
slave_priority:100  
slave_read_only:1  
connected_slaves:0  
master_repl_offset:0  
repl_backlog_active:0  
repl_backlog_size:1048576  
repl_backlog_first_byte_offset:0  
repl_backlog_histlen:0   
进入主

Java 代码

/usr/servers/redis-2.8.19# /usr/servers/redis-2.8.19/src/redis-cli  -p 6660  
Java 代码

127.0.0.1:6660> info replication  
\# Replication  
role:master  
connected_slaves:1  
slave0:ip=127.0.0.1,port=6661,state=online,offset=85,lag=1  
master_repl_offset:85  
repl_backlog_active:1  
repl_backlog_size:1048576  
repl_backlog_first_byte_offset:2  
repl_backlog_histlen:84  
127.0.0.1:6660> set i 1  
OK   
进入从

Java 代码

/usr/servers/redis-2.8.19/src/redis-cli  -p 6661    
Java 代码

127.0.0.1:6661> get i  
"1"     
此时可以看到主从挂载成功，可以进行主从复制了。使用 slaveof no one 断开主从。

Redis 持久化

Redis 虽然不适合做持久化存储，但是为了防止数据丢失有时需要进行持久化存储，此时可以挂载一个从（叶子节点）只进行持久化存储工作，这样假设其他服务器挂了，我们可以通过这个节点进行数据恢复。

Redis 持久化有 RDB 快照模式和 AOF 追加模式，根据自己需求进行选择。

RDB 持久化

Java 代码

\#格式save seconds changes 即N秒变更N次则保存，从如下默认配置可以看到丢失数据的周期很长，通过save “” 配置可以完全禁用此持久化  
save 900 1    
save 300 10    
save 60 10000   
\#RDB是否进行压缩，压缩耗CPU但是可以减少存储大小  
rdbcompression yes  
\#RDB保存的位置，默认当前位置    
dir ./  
\#RDB保存的数据库名称  
dbfilename dump.rdb    
\#不使用AOF模式，即RDB模式  
appendonly no     
可以通过 set 一个数据，然后很快的 kill 掉 redis 进程然后再启动会发现数据丢失了。

AOF 持久化

AOF（append only file）即文件追加模式，即把每一个用户操作的命令保存下来，这样就会存在好多重复的命令导致恢复时间过长，那么可以通过相应的配置定期进行 AOF 重写来减少重复。

Java 代码

\#开启AOF  
appendonly yes  
\#AOF保存的位置，默认当前位置    
dir ./  
\#AOF保存的数据库名称  
appendfilename appendonly.aof  
\#持久化策略，默认每秒fsync一次，也可以选择always即每次操作都进行持久化，或者no表示不进行持久化而是借助操作系统的同步将缓存区数据写到磁盘  
appendfsync everysec  

\#AOF重写策略（同时满足如下两个策略进行重写）  
\#当AOF文件大小占到初始文件大小的多少百分比时进行重写  
auto-aof-rewrite-percentage 100  
\#触发重写的最小文件大小  
auto-aof-rewrite-min-size 64mb  

\#为减少磁盘操作，暂缓重写阶段的磁盘同步  
no-appendfsync-on-rewrite no     
此处的 appendfsync everysec 可以认为是 RDB 和 AOF 的一个折中方案。

#当 bgsave 出错时停止写（MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk.），遇到该错误可以暂时改为 no，当写成功后再改回 yes stop-writes-on-bgsave-error yes

更多 Redis 持久化请参考 http://redis.readthedocs.org/en/latest/topic/persistence.html。

Redis 动态调整配置

获取 maxmemory(10mb)

Java 代码

127.0.0.1:6660> config get maxmemory  
1) "maxmemory"  
2) "10485760"   
设置新的 maxmemory(20mb)

Java 代码

127.0.0.1:6660> config set maxmemory 20971520  
OK  
但是此时重启 redis 后该配置会丢失，可以执行如下命令重写配置文件

Java 代码

127.0.0.1:6660> config rewrite  
OK   
注意：此时所以配置包括主从配置都会重写。

Redis 执行 Lua 脚本

Redis 客户端支持解析和处理 lua 脚本，因为 Redis 的单线程机制，我们可以借助 Lua 脚本实现一些原子操作，如扣减库存/红包之类的。此处不建议使用 EVAL 直接发送 lua 脚本到客户端，因为其每次都会进行 Lua 脚本的解析，而是使用 SCRIPT LOAD+ EVALSHA 进行操作。未来不知道是否会用 luajit 来代替 lua，让redis lua 脚本性能更强。

到此基本的 Redis 知识就讲完了。

Twemproxy 设置
一旦涉及到一台物理机无法存储的情况就需要考虑使用分片机制将数据存储到多台服务器，可以说是 Redis 集群；如果客户端都是如 Java 没什么问题，但是如果有多种类型客户端（如 PHP、C）等也要使用那么需要保证它们的分片逻辑是一样的；另外随着客户端的增加，连接数也会随之增多，发展到一定地步肯定会出现连接数不够用的；此时 Twemproxy 就可以上场了。主要作用：分片、减少连接数。另外还提供了 Hash Tag 机制来帮助我们将相似的数据存储到同一个分片。另外也可以参考豌豆荚的 https://github.com/wandoulabs/codis。

基本配置

其使用 YML 语法，如

Java 代码

server1:  
  listen: 127.0.0.1:1111  
  hash: fnv1a_64  
  distribution: ketama  
  timeout:1000  
  redis: true  
  servers:  
   - 127.0.0.1:6660:1  
   - 127.0.0.1:6661:1    
server1：是给当前分片配置起的名字，一个配置文件可以有多个分片配置；
listen ： 监听的 ip 和端口；
hash：散列算法；
distribution：分片算法，比如一致性 Hash/ 取模；
timeout：连接后端 Redis 或接收响应的超时时间；
redis：是否是 redis 代理，如果是 false 则是 memcached 代理；
servers：代理的服务器列表，该列表会使用 distribution 配置的分片算法进行分片；
分片算法

hash 算法：

one_at_a_time
md5
crc16
crc32 (crc32 implementation compatible with libmemcached)
crc32a (correct crc32 implementation as per the spec)
fnv1_64
fnv1a_64
fnv1_32
fnv1a_32
hsieh
murmur
jenkins  
分片算法：

ketama(一致性 Hash 算法)
modula(取模)
random(随机算法)
服务器列表

servers:

ip:port:weight alias
如
servers:
127.0.0.1:6660:1
127.0.0.1:6661:1
或者
servers:
127.0.0.1:6660:1 server1
127.0.0.1:6661:1 server2
推荐使用后一种方式，默认情况下使用 ip:port:weight 进行散列并分片，这样假设服务器宕机换上新的服务器，那么此时得到的散列值就不一样了，因此建议给每个配置起一个别名来保证映射到自己想要的服务器。即如果不使用一致性 Hash 算法来作缓存服务器，而是作持久化存储服务器时就更有必要了（即不存在服务器下线的情况，即使服务器 ip:port 不一样但仍然要得到一样的分片结果）。

HashTag

比如一个商品有：商品基本信息(p:id:)、商品介绍(d:id:)、颜色尺码(c:id:)等，假设我们存储时不采用 HashTag 将会导致这些数据不会存储到一个分片，而是分散到多个分片，这样获取时将需要从多个分片获取数据进行合并，无法进行 mget；那么如果有了 HashTag，那么可以使用“::”中间的数据做分片逻辑，这样 id 一样的将会分到一个分片。

nutcracker.yml配置如下

Java 代码

server1:  
  listen: 127.0.0.1:1111  
  hash: fnv1a_64  
  distribution: ketama  
  redis: true  
  hash_tag: "::"  
  servers:  
   - 127.0.0.1:6660:1 server1  
   - 127.0.0.1:6661:1 server2  
连接 Twemproxy

Java 代码

/usr/servers/redis-2.8.19/src/redis-cli  -p 1111  
Java 代码

127.0.0.1:1111> set p:12: 1  
OK  
127.0.0.1:1111> set d:12: 1  
OK  
127.0.0.1:1111> set c:12: 1  
OK  
在我的服务器上可以连接 6660 端口

Java 代码

/usr/servers/redis-2.8.19/src/redis-cli  -p 6660  
127.0.0.1:6660> get p:12:   
"1"  
127.0.0.1:6660> get d:12:   
"1"  
127.0.0.1:6660> get c:12:   
"1"  
一致性 Hash 与服务器宕机

如果我们把 Redis 服务器作为缓存服务器并使用一致性 Hash 进行分片，当有服务器宕机时需要自动从一致性 Hash 环上摘掉，或者其上线后自动加上，此时就需要如下配置：

#是否在节点故障无法响应时自动摘除该节点，如果作为存储需要设置为为false auto_eject_hosts: true #重试时间（毫秒），重新连接一个临时摘掉的故障节点的间隔，如果判断节点正常会自动加到一致性Hash环上 server_retry_timeout: 30000 #节点故障无法响应多少次从一致性Hash环临时摘掉它，默认是2 server_failure_limit: 2

支持的 Redis 命令

不是所有 Redis 命令都支持，请参考 https://github.com/twitter/twemproxy/blob/master/notes/redis.md。

因为我们所有的 Twemproxy 配置文件规则都是一样的，因此我们应该将其移到我们项目中。

Java 代码

cp /usr/servers/twemproxy-0.4.0/conf/nutcracker.yml  /usr/example/   
另外 Twemproxy 提供了启动/重启/停止脚本方便操作，但是需要修改配置文件位置为 /usr/example/nutcracker.yml。

Java 代码

chmod +x /usr/servers/twemproxy-0.4.0/scripts/nutcracker.init   
vim /usr/servers/twemproxy-0.4.0/scripts/nutcracker.init    
将 OPTIONS 改为

OPTIONS="-d -c /usr/example/nutcracker.yml"

另外注释掉. /etc/rc.d/init.d/functions；将 daemon --user ${USER} ${prog} $OPTIONS 改为 ${prog} $OPTIONS；将 killproc 改为 killall。

这样就可以使用如下脚本进行启动、重启、停止了。
/usr/servers/twemproxy-0.4.0/scripts/nutcracker.init {start|stop|status|restart|reload|condrestart}

对于扩容最简单的办法是：

创建新的集群；
双写两个集群；
把数据从老集群迁移到新集群（不存在才设置值，防止覆盖新的值）；
复制速度要根据实际情况调整，不能影响老集群的性能；
切换到新集群即可，如果使用Twemproxy代理层的话，可以做到迁移对读的应用透明。




Lua 模块开发

在实际开发中，不可能把所有代码写到一个大而全的 lua 文件中，需要进行分模块开发；而且模块化是高性能 Lua 应用的关键。使用 require 第一次导入模块后，所有 Nginx 进程全局共享模块的数据和代码，每个 Worker 进程需要时会得到此模块的一个副本（Copy-On-Write），即模块可以认为是每 Worker 进程共享而不是每 Nginx Server 共享；另外注意之前我们使用 init_by_lua 中初始化的全局变量是每请求复制一个；如果想在多个 Worker 进程间共享数据可以使用 ngx.shared.DICT 或如 Redis 之类的存储。

在 /usr/example/lualib 中已经提供了大量第三方开发库如 cjson、redis 客户端、mysql客户端：

cjson.so
resty/
   aes.lua
   core.lua
   dns/
   lock.lua
   lrucache/
   lrucache.lua
   md5.lua
   memcached.lua
   mysql.lua
   random.lua
   redis.lua
   ……
需要注意在使用前需要将库在 nginx.conf 中导入：

Java 代码

\#lua模块路径，其中”;;”表示默认搜索路径，默认到/usr/servers/nginx下找  
lua_package_path "/usr/example/lualib/?.lua;;";  #lua 模块  
lua_package_cpath "/usr/example/lualib/?.so;;";  #c模块    
使用方式是在lua中通过如下方式引入

Java 代码

local cjson = require(“cjson”)  
local redis = require(“resty.redis”)    
接下来我们来开发一个简单的 lua 模块。

Java 代码

vim /usr/example/lualib/module1.lua   
Java 代码

local count = 0  
local function hello()  
   count = count + 1  
   ngx.say("count : ", count)  
end  

local _M = {  
   hello = hello  
}  

return _M    
开发时将所有数据做成局部变量/局部函数；通过 _M 导出要暴露的函数，实现模块化封装。

接下来创建 test_module_1.lua

Java 代码

vim /usr/example/lua/test_module_1.lua  
Java 代码

local module1 = require("module1")  

module1.hello()   
使用 local var = require ("模块名")，该模块会到 lua_package_path 和lua_package_cpath 声明的的位置查找我们的模块，对于多级目录的使用 require ("目录1.目录2.模块名")加载。

example.conf 配置

Java 代码

location /lua_module_1 {  
    default_type 'text/html';  
    lua_code_cache on;  
    content_by_lua_file /usr/example/lua/test_module_1.lua;  
}  
访问如 http://192.168.1.2/lua_module_1 进行测试，会得到类似如下的数据，count 会递增

count : 1
count ：2
……
count ：N
此时可能发现 count 一直递增，假设我们的 worker_processes 2，我们可以通过 kill -9 nginx worker process 杀死其中一个 Worker 进程得到 count 数据变化。

假设我们创建了 vim/usr/example/lualib/test/module2.lua 模块，可以通过 local module2 = require("test.module2") 加载模块

基本的模块开发就完成了，如果是只读数据可以通过模块中声明 local 变量存储；如果想在每 Worker 进程共享，请考虑竞争；如果要在多个 Worker 进程间共享请考虑使用 ngx.shared.DICT 或如 Redis 存储。





常用 Lua 开发库 1-redis、mysql、http 客户端

对于开发来说需要有好的生态开发库来辅助我们快速开发，而 Lua 中也有大多数我们需要的第三方开发库如 Redis、Memcached、Mysql、Http 客户端、JSON、模板引擎等。 一些常见的 Lua 库可以在 github 上搜索，https://github.com/search?utf8=%E2%9C%93&q=lua+resty。

Redis 客户端
lua-resty-redis 是为基于 cosocket API 的 ngx_lua 提供的 Lua redis 客户端，通过它可以完成 Redis 的操作。默认安装 OpenResty 时已经自带了该模块，使用文档可参考https://github.com/openresty/lua-resty-redis。

在测试之前请启动 Redis 实例：

nohup /usr/servers/redis-2.8.19/src/redis-server/usr/servers/redis-2.8.19/redis_6660.conf &

基本操作

编辑 test_redis_baisc.lua

Java 代码

local function close_redis(red)  
    if not red then  
        return  
    end  
    local ok, err = red:close()  
    if not ok then  
        ngx.say("close redis error : ", err)  
    end  
end  

local redis = require("resty.redis")  

--创建实例  
local red = redis:new()  
--设置超时（毫秒）  
red:set_timeout(1000)  
--建立连接  
local ip = "127.0.0.1"  
local port = 6660  
local ok, err = red:connect(ip, port)  
if not ok then  
    ngx.say("connect to redis error : ", err)  
    return close_redis(red)  
end  
--调用API进行处理  
ok, err = red:set("msg", "hello world")  
if not ok then  
    ngx.say("set msg error : ", err)  
    return close_redis(red)  
end  

--调用API获取数据  
local resp, err = red:get("msg")  
if not resp then  
    ngx.say("get msg error : ", err)  
    return close_reedis(red)  
end  
--得到的数据为空处理  
if resp == ngx.null then  
    resp = ''  --比如默认值  
end  
ngx.say("msg : ", resp)  

close_redis(red)    
基本逻辑很简单，要注意此处判断是否为 nil，需要跟 ngx.null 比较。

example.conf 配置文件

Java 代码

 location /lua_redis_basic {  
    default_type 'text/html';  
    lua_code_cache on;  
    content_by_lua_file /usr/example/lua/test_redis_basic.lua;  
}  
访问如 http://192.168.1.2/lua_redis_basic 进行测试，正常情况得到如下信息 msg : hello world

连接池

建立 TCP 连接需要三次握手而释放 TCP 连接需要四次握手，而这些往返时延仅需要一次，以后应该复用 TCP 连接，此时就可以考虑使用连接池，即连接池可以复用连接。

我们只需要将之前的 close_redis 函数改造为如下即可：

Java 代码

local function close_redis(red)  
    if not red then  
        return  
    end  
    --释放连接(连接池实现)  
    local pool_max_idle_time = 10000 --毫秒  
    local pool_size = 100 --连接池大小  
    local ok, err = red:set_keepalive(pool_max_idle_time, pool_size)  
    if not ok then  
        ngx.say("set keepalive error : ", err)  
    end  
end    
即设置空闲连接超时时间防止连接一直占用不释放；设置连接池大小来复用连接。

此处假设调用 red:set_keepalive()，连接池大小通过 nginx.conf 中 http 部分的如下指令定义：

#默认连接池大小，默认 30 lua_socket_pool_size 30; #默认超时时间,默认 60s lua_socket_keepalive_timeout 60s;

注意：

连接池是每 Worker 进程的，而不是每 Server 的；
当连接超过最大连接池大小时，会按照 LRU 算法回收空闲连接为新连接使用；
连接池中的空闲连接出现异常时会自动被移除；
连接池是通过 ip 和 port 标识的，即相同的 ip 和 port 会使用同一个连接池（即使是不同类型的客户端如 Redis、Memcached）；
连接池第一次 set_keepalive 时连接池大小就确定下了，不会再变更；
cosocket 的连接池 http://wiki.nginx.org/HttpLuaModule#tcpsock:setkeepalive。
pipeline

pipeline 即管道，可以理解为把多个命令打包然后一起发送；MTU（Maxitum Transmission Unit 最大传输单元）为二层包大小，一般为 1500 字节；而 MSS（Maximum Segment Size 最大报文分段大小）为四层包大小，其一般是 1500-20（IP 报头）-20（TCP 报头）=1460 字节；因此假设我们执行的多个 Redis 命令能在一个报文中传输的话，可以减少网络往返来提高速度。因此可以根据实际情况来选择走 pipeline 模式将多个命令打包到一个报文发送然后接受响应，而 Redis 协议也能很简单的识别和解决粘包。

修改之前的代码片段

Java 代码

red:init_pipeline()  
red:set("msg1", "hello1")  
red:set("msg2", "hello2")  
red:get("msg1")  
red:get("msg2")  
local respTable, err = red:commit_pipeline()  

--得到的数据为空处理  
if respTable == ngx.null then  
    respTable = {}  --比如默认值  
end  

--结果是按照执行顺序返回的一个table  
for i, v in ipairs(respTable) do  
   ngx.say("msg : ", v, "<br/>")  
end    
通过 init_pipeline() 初始化，然后通过 commit_pipieline() 打包提交 init_pipeline() 之后的Redis命令；返回结果是一个 lua table，可以通过 ipairs 循环获取结果；

配置相应 location，测试得到的结果

msg : OK
msg : OK
msg : hello1
msg : hello2
Redis Lua 脚本

利用 Redis 单线程特性，可以通过在 Redis 中执行 Lua 脚本实现一些原子操作。如之前的 red:get("msg") 可以通过如下两种方式实现：

1、直接 eval：

Java 代码

local resp, err = red:eval("return redis.call('get', KEYS[1])", 1, "msg");  
2、script load 然后 evalsha SHA1 校验和，这样可以节省脚本本身的服务器带宽： Java 代码

local sha1, err = red:script("load",  "return redis.call('get', KEYS[1])");  
if not sha1 then  
   ngx.say("load script error : ", err)  
   return close_redis(red)  
end  
ngx.say("sha1 : ", sha1, "<br/>")  
local resp, err = red:evalsha(sha1, 1, "msg");  
首先通过 script load 导入脚本并得到一个 sha1 校验和（仅需第一次导入即可），然后通过evalsha 执行 sha1 校验和即可，这样如果脚本很长通过这种方式可以减少带宽的消耗。

此处仅介绍了最简单的 redis lua 脚本，更复杂的请参考官方文档学习使用。

另外 Redis 集群分片算法该客户端没有提供需要自己实现，当然可以考虑直接使用类似于Twemproxy 这种中间件实现。

Memcached 客户端使用方式和本文类似，本文就不介绍了。

Mysql 客户端
lua-resty-mysql 是为基于 cosocket API 的 ngx_lua 提供的 Lua Mysql 客户端，通过它可以完成 Mysql 的操作。默认安装 OpenResty 时已经自带了该模块，使用文档可参考https://github.com/openresty/lua-resty-mysql。

编辑 test_mysql.lua

Java 代码

local function close_db(db)  
    if not db then  
        return  
    end  
    db:close()  
end  

local mysql = require("resty.mysql")  
--创建实例  
local db, err = mysql:new()  
if not db then  
    ngx.say("new mysql error : ", err)  
    return  
end  
--设置超时时间(毫秒)  
db:set_timeout(1000)  

local props = {  
    host = "127.0.0.1",  
    port = 3306,  
    database = "mysql",  
    user = "root",  
    password = "123456"  
}  

local res, err, errno, sqlstate = db:connect(props)  

if not res then  
   ngx.say("connect to mysql error : ", err, " , errno : ", errno, " , sqlstate : ", sqlstate)  
   return close_db(db)  
end  

--删除表  
local drop_table_sql = "drop table if exists test"  
res, err, errno, sqlstate = db:query(drop_table_sql)  
if not res then  
   ngx.say("drop table error : ", err, " , errno : ", errno, " , sqlstate : ", sqlstate)  
   return close_db(db)  
end  

--创建表  
local create_table_sql = "create table test(id int primary key auto_increment, ch varchar(100))"  
res, err, errno, sqlstate = db:query(create_table_sql)  
if not res then  
   ngx.say("create table error : ", err, " , errno : ", errno, " , sqlstate : ", sqlstate)  
   return close_db(db)  
end  

--插入  
local insert_sql = "insert into test (ch) values('hello')"  
res, err, errno, sqlstate = db:query(insert_sql)  
if not res then  
   ngx.say("insert error : ", err, " , errno : ", errno, " , sqlstate : ", sqlstate)  
   return close_db(db)  
end  

res, err, errno, sqlstate = db:query(insert_sql)  

ngx.say("insert rows : ", res.affected_rows, " , id : ", res.insert_id, "<br/>")  

--更新  
local update_sql = "update test set ch = 'hello2' where id =" .. res.insert_id  
res, err, errno, sqlstate = db:query(update_sql)  
if not res then  
   ngx.say("update error : ", err, " , errno : ", errno, " , sqlstate : ", sqlstate)  
   return close_db(db)  
end  

ngx.say("update rows : ", res.affected_rows, "<br/>")  
--查询  
local select_sql = "select id, ch from test"  
res, err, errno, sqlstate = db:query(select_sql)  
if not res then  
   ngx.say("select error : ", err, " , errno : ", errno, " , sqlstate : ", sqlstate)  
   return close_db(db)  
end  

for i, row in ipairs(res) do  
   for name, value in pairs(row) do  
     ngx.say("select row ", i, " : ", name, " = ", value, "<br/>")  
   end  
end  

ngx.say("<br/>")  
--防止sql注入  
local ch_param = ngx.req.get_uri_args()["ch"] or ''  
--使用ngx.quote_sql_str防止sql注入  
local query_sql = "select id, ch from test where ch = " .. ngx.quote_sql_str(ch_param)  
res, err, errno, sqlstate = db:query(query_sql)  
if not res then  
   ngx.say("select error : ", err, " , errno : ", errno, " , sqlstate : ", sqlstate)  
   return close_db(db)  
end  

for i, row in ipairs(res) do  
   for name, value in pairs(row) do  
     ngx.say("select row ", i, " : ", name, " = ", value, "<br/>")  
   end  
end  

--删除  
local delete_sql = "delete from test"  
res, err, errno, sqlstate = db:query(delete_sql)  
if not res then  
   ngx.say("delete error : ", err, " , errno : ", errno, " , sqlstate : ", sqlstate)  
   return close_db(db)  
end  

ngx.say("delete rows : ", res.affected_rows, "<br/>")  

close_db(db)  
对于新增/修改/删除会返回如下格式的响应：

Java 代码

{  
    insert_id = 0,  
    server_status = 2,  
    warning_count = 1,  
    affected_rows = 32,  
    message = nil  
}  
affected_rows 表示操作影响的行数，insert_id 是在使用自增序列时产生的 id。

对于查询会返回如下格式的响应：

Java 代码

{  
    { id= 1, ch= "hello"},  
    { id= 2, ch= "hello2"}  
}   
null 将返回 ngx.null。

example.conf 配置文件

Java 代码

location /lua_mysql {  
   default_type 'text/html';  
   lua_code_cache on;  
   content_by_lua_file /usr/example/lua/test_mysql.lua;  
}  
访问如 http://192.168.1.2/lua_mysql?ch=hello 进行测试，得到如下结果

Java 代码

insert rows : 1 , id : 2  
update rows : 1  
select row 1 : ch = hello  
select row 1 : id = 1  
select row 2 : ch = hello2  
select row 2 : id = 2  
select row 1 : ch = hello  
select row 1 : id = 1  
delete rows : 2    
客户端目前还没有提供预编译 SQL 支持（即占位符替换位置变量），这样在入参时记得使用 ngx.quote_sql_str 进行字符串转义，防止 sql 注入；连接池和之前 Redis 客户端完全一样就不介绍了。

对于 Mysql 客户端的介绍基本够用了，更多请参考 https://github.com/openresty/lua-resty-mysql。

其他如 MongoDB 等数据库的客户端可以从 github 上查找使用。

Http 客户端
OpenResty 默认没有提供 Http 客户端，需要使用第三方提供；当然我们可以通过 ngx.location.capture 去方式实现，但是有一些限制，后边我们再做介绍。

我们可以从 github 上搜索相应的客户端，比如 https://github.com/pintsized/lua-resty-http。

lua-resty-http

下载 lua-resty-http 客户端到 lualib

Java 代码

cd /usr/example/lualib/resty/  
wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http_headers.lua  
wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http.lua  
test_http_1.lua

Java 代码

local http = require("resty.http")  
--创建http客户端实例  
local httpc = http.new()  

local resp, err = httpc:request_uri("http://s.taobao.com", {  
    method = "GET",  
    path = "/search?q=hello",  
    headers = {  
        ["User-Agent"] = "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.111 Safari/537.36"  
    }  
})  

if not resp then  
    ngx.say("request error :", err)  
    return  
end  

--获取状态码  
ngx.status = resp.status  

--获取响应头  
for k, v in pairs(resp.headers) do  
    if k ~= "Transfer-Encoding" and k ~= "Connection" then  
        ngx.header[k] = v  
    end  
end  
--响应体  
ngx.say(resp.body)  

httpc:close()  
响应头中的 Transfer-Encoding 和 Connection 可以忽略，因为这个数据是当前 server 输出的。

example.conf 配置文件

Java 代码

location /lua_http_1 {  
   default_type 'text/html';  
   lua_code_cache on;  
   content_by_lua_file /usr/example/lua/test_http_1.lua;  
}   
在 nginx.conf 中的 http 部分添加如下指令来做 DNS 解析

Java 代码

resolver 8.8.8.8;    
记得要配置 DNS 解析器 resolver 8.8.8.8，否则域名是无法解析的。

访问如 http://192.168.1.2/lua_http_1 会看到淘宝的搜索界面。

使用方式比较简单，如超时和连接池设置和之前 Redis 客户端一样，不再阐述。更多客户端使用规则请参考 https://github.com/pintsized/lua-resty-http。

ngx.location.capture

ngx.location.capture 也可以用来完成 http 请求，但是它只能请求到相对于当前 nginx 服务器的路径，不能使用之前的绝对路径进行访问，但是我们可以配合 nginx upstream 实现我们想要的功能。

在 nginx.cong中 的 http 部分添加如下 upstream 配置

Java 代码

upstream backend {  
    server s.taobao.com;  
    keepalive 100;  
}    
即我们将请求 upstream 到 backend；另外记得一定要添加之前的 DNS 解析器。

在 example.conf 配置如下 location

Java 代码

location ~ /proxy/(.*) {  
   internal;  
   proxy_pass http://backend/$1$is_args$args;  
}    
internal 表示只能内部访问，即外部无法通过 url 访问进来； 并通过 proxy_pass 将请求转发到 upstream。

test_http_2.lua

Java 代码

local resp = ngx.location.capture("/proxy/search", {  
    method = ngx.HTTP_GET,  
    args = {q = "hello"}  

})  
if not resp then  
    ngx.say("request error :", err)  
    return  
end  
ngx.log(ngx.ERR, tostring(resp.status))  

--获取状态码  
ngx.status = resp.status  

--获取响应头  
for k, v in pairs(resp.header) do  
    if k ~= "Transfer-Encoding" and k ~= "Connection" then  
        ngx.header[k] = v  
    end  
end  
--响应体  
if resp.body then  
    ngx.say(resp.body)  
end   
通过 ngx.location.capture 发送一个子请求，此处因为是子请求，所有请求头继承自当前请求，还有如 ngx.ctx和ngx.var 是否继承可以参考官方文档 http://wiki.nginx.org/HttpLuaModule#ngx.location.capture。 另外还提供了 ngx.location.capture_multi用于并发发出多个请求，这样总的响应时间是最慢的一个，批量调用时有用。

example.conf 配置文件

Java 代码

location /lua_http_2 {  
   default_type 'text/html';  
   lua_code_cache on;  
   content_by_lua_file /usr/example/lua/test_http_2.lua;  
}  
访问如 http://192.168.1.2/lua_http_2 进行测试可以看到淘宝搜索界面。

我们通过 upstream+ngx.location.capture 方式虽然麻烦点，但是得到更好的性能和upstream 的连接池、负载均衡、故障转移、proxy cache 等特性。

不过因为继承在当前请求的请求头，所以可能会存在一些问题，比较常见的就是 gzip 压缩问题，ngx.location.capture 不会解压缩后端服务器的 GZIP 内容，解决办法可以参考https://github.com/openresty/lua-nginx-module/issues/12；因为我们大部分这种http 调用的都是内部服务，因此完全可以在 proxy location 中添加proxy_pass_request_headers off; 来不传递请求头。





JSON 库、编码转换、字符串处理

JSON 库
在进行数据传输时 JSON 格式目前应用广泛，因此从 Lua 对象与 JSON 字符串之间相互转换是一个非常常见的功能；目前 Lua 也有几个 JSON 库，本人用过 cjson、dkjson。其中 cjson的语法严格（比如 unicode \u0020\u7eaf ），要求符合规范否则会解析失败（如 \u002），而 dkjson 相对宽松，当然也可以通过修改 cjson 的源码来完成一些特殊要求。而在使用dkjson 时也没有遇到性能问题，目前使用的就是 dkjson。使用时要特别注意的是大部分 JSON库都仅支持 UTF-8 编码；因此如果你的字符编码是如 GBK 则需要先转换为 UTF-8 然后进行处理。

test_cjson.lua

Java 代码

local cjson = require("cjson")  

--lua对象到字符串  
local obj = {  
    id = 1,  
    name = "zhangsan",  
    age = nil,  
    is_male = false,  
    hobby = {"film", "music", "read"}  
}  

local str = cjson.encode(obj)  
ngx.say(str, "<br/>")  

--字符串到lua对象  
str = '{"hobby":["film","music","read"],"is_male":false,"name":"zhangsan","id":1,"age":null}'  
local obj = cjson.decode(str)  

ngx.say(obj.age, "<br/>")  
ngx.say(obj.age == nil, "<br/>")  
ngx.say(obj.age == cjson.null, "<br/>")  
ngx.say(obj.hobby[1], "<br/>")  

--循环引用  
obj = {  
   id = 1  
}  
obj.obj = obj  
-- Cannot serialise, excessive nesting  
--ngx.say(cjson.encode(obj), "<br/>")  
local cjson_safe = require("cjson.safe")  
--nil  
ngx.say(cjson_safe.encode(obj), "<br/>")    
null 将会转换为 cjson.null；循环引用会抛出异常 Cannot serialise, excessive nesting，默认解析嵌套深度是 1000，可以通过 cjson.encode_max_depth() 设置深度提高性能；使用 cjson.safe 不会抛出异常而是返回 nil。

example.conf 配置文件

Java 代码

location ~ /lua_cjson {  
   default_type 'text/html';  
   lua_code_cache on;  
   content_by_lua_file /usr/example/lua/test_cjson.lua;  
}  
访问如 http://192.168.1.2/lua_cjson 将得到如下结果

Java 代码

{"hobby":["film","music","read"],"is_male":false,"name":"zhangsan","id":1}  
null  
false  
true  
film  
nil  
lua-cjson 文档 http://www.kyne.com.au/~mark/software/lua-cjson-manual.html。

接下来学习下 dkjson。

下载 dkjson 库

Java 代码

cd /usr/example/lualib/  
wget http://dkolf.de/src/dkjson-lua.fsl/raw/dkjson.lua?name=16cbc26080996d9da827df42cb0844a25518eeb3 -O dkjson.lua  
test_dkjson.lua

Java 代码

local dkjson = require("dkjson")  

--lua对象到字符串  
local obj = {  
    id = 1,  
    name = "zhangsan",  
    age = nil,  
    is_male = false,  
    hobby = {"film", "music", "read"}  
}  

local str = dkjson.encode(obj, {indent = true})  
ngx.say(str, "<br/>")  

--字符串到lua对象  
str = '{"hobby":["film","music","read"],"is_male":false,"name":"zhangsan","id":1,"age":null}'  
local obj, pos, err = dkjson.decode(str, 1, nil)  

ngx.say(obj.age, "<br/>")  
ngx.say(obj.age == nil, "<br/>")  
ngx.say(obj.hobby[1], "<br/>")  

--循环引用  
obj = {  
   id = 1  
}  
obj.obj = obj  
--reference cycle  
--ngx.say(dkjson.encode(obj), "<br/>")  
默认情况下解析的 json 的字符会有缩排和换行，使用 {indent = true} 配置将把所有内容放在一行。和 cjson 不同的是解析 json 字符串中的 null 时会得到 nil。

example.conf 配置文件

Java 代码

location ~ /lua_dkjson {  
   default_type 'text/html';  
   lua_code_cache on;  
   content_by_lua_file /usr/example/lua/test_dkjson.lua;  
}  
访问如 http://192.168.1.2/lua_dkjson 将得到如下结果

Java 代码

{ "hobby":["film","music","read"], "is_male":false, "name":"zhangsan", "id":1 }  
nil  
true  
film   
dkjson 文档 http://dkolf.de/src/dkjson-lua.fsl/home和http://dkolf.de/src/dkjson-lua.fsl/wiki?name=Documentation。

编码转换
我们在使用一些类库时会发现大部分库仅支持 UTF-8 编码，因此如果使用其他编码的话就需要进行编码转换的处理；而 Linux 上最常见的就是 iconv，而 lua-iconv 就是它的一个 Lua API 的封装。

安装 lua-iconv 可以通过如下两种方式：

ubuntu 下可以使用如下方式

Java 代码

apt-get install luarocks  
luarocks install lua-iconv   
cp /usr/local/lib/lua/5.1/iconv.so  /usr/example/lualib/    
源码安装方式，需要有 gcc 环境

Java 代码

wget https://github.com/do^Cloads/ittner/lua-iconv/lua-iconv-7.tar.gz  
tar -xvf lua-iconv-7.tar.gz  
cd lua-iconv-7  
gcc -O2 -fPIC -I/usr/include/lua5.1 -c luaiconv.c -o luaiconv.o -I/usr/include  
gcc -shared -o iconv.so -L/usr/local/lib luaiconv.o -L/usr/lib  
cp iconv.so  /usr/example/lualib/  
test_iconv.lua

Java 代码

ngx.say("中文")    
此时文件编码必须为 UTF-8，即 Lua 文件编码为什么里边的字符编码就是什么。

example.conf 配置文件

Java 代码

location ~ /lua_iconv {  
   default_type 'text/html';  
   charset gbk;  
   lua_code_cache on;  
   content_by_lua_file /usr/example/lua/test_iconv.lua;  
}  
通过 charset 告诉浏览器我们的字符编码为 gbk。

访问 http://192.168.1.2/lua_iconv 会发现输出乱码；

此时需要我们将 test_iconv.lua 中的字符进行转码处理：

Java 代码

local iconv = require("iconv")  
local togbk = iconv.new("gbk", "utf-8")  
local str, err = togbk:iconv("中文")  
ngx.say(str)    
通过转码我们得到最终输出的内容编码为 gbk， 使用方式 iconv.new (目标编码, 源编码)。

有如下可能出现的错误：

Java 代码

nil     
    没有错误成功。  
iconv.ERROR_NO_MEMORY  
    内存不足。  
iconv.ERROR_INVALID  
    有非法字符。  
iconv.ERROR_INCOMPLETE  
    有不完整字符。  
iconv.ERROR_FINALIZED  
    使用已经销毁的转换器，比如垃圾回收了。  
iconv.ERROR_UNKNOWN   
    未知错误  
iconv 在转换时遇到非法字符或不能转换的字符就会失败，此时可以使用如下方式忽略转换失败的字符

Java 代码

local togbk_ignore = iconv.new("GBK//IGNORE", "UTF-8")  
另外在实际使用中进行 UTF-8 到 GBK 转换过程时，会发现有些字符在 GBK 编码表但是转换不了，此时可以使用更高的编码 GB18030 来完成转换。

更多介绍请参考 http://ittner.github.io/lua-iconv/。

位运算
Lua 5.3 之前是没有提供位运算支持的，需要使用第三方库，比如 LuaJIT 提供了 bit 库。

test_bit.lua

Java 代码

local bit = require("bit")  
ngx.say(bit.lshift(1, 2))    
lshift 进行左移位运算，即得到4。

其他位操作 API 请参考 http://bitop.luajit.org/api.html。Lua 5.3 的位运算操作符 http://cloudwu.github.io/lua53doc/manual.html#3.4.2.

cache
ngx_lua 模块本身提供了全局共享内存 ngx.shared.DICT 可以实现全局共享，另外可以使用如 Redis 来实现缓存。另外还一个 lua-resty-lrucache 实现，其和 ngx.shared.DICT 不一样的是它是每 Worker 进程共享，即每个 Worker 进行会有一份缓存，而且经过实际使用发现其性能不如 ngx.shared.DICT。但是其好处就是不需要进行全局配置。

创建缓存模块来实现只初始化一次：

Java 代码

vim /usr/example/lualib/mycache.lua    
Java 代码

local lrucache = require("resty.lrucache")  
--创建缓存实例，并指定最多缓存多少条目  
local cache, err = lrucache.new(200)  
if not cache then  
   ngx.log(ngx.ERR, "create cache error : ", err)  
end  

local function set(key, value, ttlInSeconds)  
    cache:set(key, value, ttlInSeconds)  
end  

local function get(key)  
    return cache:get(key)  
end  

local _M = {  
  set = set,  
  get = get  
}  

return _M   
此处利用了模块的特性实现了每个 Worker 进行只初始化一次 cache 实例。

test_lrucache.lua

Java 代码

local mycache = require("mycache")  
local count = mycache.get("count") or 0  
count = count + 1  
mycache.set("count", count, 10 * 60 * 60) --10分钟  
ngx.say(mycache.get("count"))     
可以实现诸如访问量统计，但仅是每 Worker 进程的。

example.conf 配置文件

Java 代码

location ~ /lua_lrucache {  
   default_type 'text/html';  
   lua_code_cache on;  
   content_by_lua_file /usr/example/lua/test_lrucache.lua;  
}    
访问如 http://192.168.1.2/lua_lrucache 测试。

更多介绍请参考 https://github.com/openresty/lua-resty-lrucache。

字符串处理
Lua 5.3 之前没有提供字符操作相关的函数，如字符串截取、替换等都是字节为单位操作；在实际使用时尤其包含中文的场景下显然不能满足需求；即使 Lua 5.3 也仅提供了基本的 UTF-8 操作。

Lua UTF-8 库 https://github.com/starwing/luautf8

LuaRocks 安装

Java 代码

\#首先确保git安装了  
apt-get install git  
luarocks install utf8  
cp /usr/local/lib/lua/5.1/utf8.so  /usr/example/lualib/  
源码安装

Java 代码

wget https://github.com/starwing/luautf8/archive/master.zip  
unzip master.zip  
cd luautf8-master/  
gcc -O2 -fPIC -I/usr/include/lua5.1 -c utf8.c -o utf8.o -I/usr/include  
gcc -shared -o utf8.so -L/usr/local/lib utf8.o -L/usr/lib  
test_utf8.lua

Java 代码

local utf8 = require("utf8")  
local str = "abc中文"  
ngx.say("len : ", utf8.len(str), "<br/>")  
ngx.say("sub : ", utf8.sub(str, 1, 4))   
文件编码必须为 UTF8，此处我们实现了最常用的字符串长度计算和字符串截取。

example.conf 配置文件

Java 代码

location ~ /lua_utf8 {  
   default_type 'text/html';  
   lua_code_cache on;  
   content_by_lua_file /usr/example/lua/test_utf8.lua;  
}  
访问如 http://192.168.1.2/lua_utf8 测试得到如下结果

len : 5 sub : abc中

字符串转换为 unicode 编码：

Java 代码

local bit = require("bit")  
local bit_band = bit.band  
local bit_bor = bit.bor  
local bit_lshift = bit.lshift  
local string_format = string.format  
local string_byte = string.byte  
local table_concat = table.concat  

local function utf8_to_unicode(str)  
    if not str or str == "" or str == ngx.null then  
        return nil  
    end  
    local res, seq, val = {}, 0, nil  
    for i = 1, #str do  
        local c = string_byte(str, i)  
        if seq == 0 then  
            if val then  
                res[#res + 1] = string_format("%04x", val)  
            end  

           seq = c < 0x80 and 1 or c < 0xE0 and 2 or c < 0xF0 and 3 or  
                              c < 0xF8 and 4 or --c < 0xFC and 5 or c < 0xFE and 6 or  
                              0  
            if seq == 0 then  
                ngx.log(ngx.ERR, 'invalid UTF-8 character sequence' .. ",,," .. tostring(str))  
                return str  
            end  

            val = bit_band(c, 2 ^ (8 - seq) - 1)  
        else  
            val = bit_bor(bit_lshift(val, 6), bit_band(c, 0x3F))  
        end  
        seq = seq - 1  
    end  
    if val then  
        res[#res + 1] = string_format("%04x", val)  
    end  
    if #res == 0 then  
        return str  
    end  
    return "\\u" .. table_concat(res, "\\u")  
end  

ngx.say("utf8 to unicode : ", utf8_to_unicode("abc中文"), "<br/>")    
如上方法将输出 utf8 to unicode : \u0061\u0062\u0063\u4e2d\u6587。

删除空格：

Java 代码

local function ltrim(s)  
    if not s then  
        return s  
    end  
    local res = s  
    local tmp = string_find(res, '%S')  
    if not tmp then  
        res = ''  
    elseif tmp ~= 1 then  
        res = string_sub(res, tmp)  
    end  
    return res  
end  

local function rtrim(s)  
    if not s then  
        return s  
    end  
    local res = s  
    local tmp = string_find(res, '%S%s*$')  
    if not tmp then  
        res = ''  
    elseif tmp ~= #res then  
        res = string_sub(res, 1, tmp)  
    end  

    return res  
end  

local function trim(s)  
    if not s then  
        return s  
    end  
    local res1 = ltrim(s)  
    local res2 = rtrim(res1)  
    return res2  
end  
字符串分割：

Java 代码

function split(szFullString, szSeparator)  
    local nFindStartIndex = 1  
    local nSplitIndex = 1  
    local nSplitArray = {}  
    while true do  
       local nFindLastIndex = string.find(szFullString, szSeparator, nFindStartIndex)  
       if not nFindLastIndex then  
        nSplitArray[nSplitIndex] = string.sub(szFullString, nFindStartIndex, string.len(szFullString))  
        break  
       end  
       nSplitArray[nSplitIndex] = string.sub(szFullString, nFindStartIndex, nFindLastIndex - 1)  
       nFindStartIndex = nFindLastIndex + string.len(szSeparator)  
       nSplitIndex = nSplitIndex + 1  
    end  
    return nSplitArray  
end    
如 split("a,b,c", ",") 将得到一个分割后的 table。

到此基本的字符串操作就完成了，其他 luautf8 模块的 API 和 LuaAPI 类似可以参考 http://cloudwu.github.io/lua53doc/manual.html#6.4 http://cloudwu.github.io/lua53doc/manual.html#6.5

另外对于 GBK 的操作，可以先转换为 UTF-8，最后再转换为 GBK 即可。




常用 Lua 开发库 3-模板渲染

动态 web 网页开发是 Web 开发中一个常见的场景，比如像京东商品详情页，其页面逻辑是非常复杂的，需要使用模板技术来实现。而 Lua 中也有许多模板引擎，如目前我在使用的 lua-resty-template，可以渲染很复杂的页面，借助 LuaJIT 其性能也是可以接受的。

如果学习过 JavaEE 中的 servlet 和 JSP 的话，应该知道 JSP 模板最终会被翻译成Servlet 来执行；而 lua-resty-template 模板引擎可以认为是 JSP，其最终会被翻译成 Lua 代码，然后通过 ngx.print 输出。

而 lua-resty-template 和大多数模板引擎是类似的，大体内容有：

模板位置：从哪里查找模板；
变量输出/转义：变量值输出；
代码片段：执行代码片段，完成如 if/else、for 等复杂逻辑，调用对象函数/方法；
注释：解释代码片段含义；
include：包含另一个模板片段；
其他：lua-resty-template 还提供了不需要解析片段、简单布局、可复用的代码块、宏指令等支持。
首先需要下载 lua-resty-template

Java 代码

cd /usr/example/lualib/resty/  
wget https://raw.githubusercontent.com/bungle/lua-resty-template/master/lib/resty/template.lua  
mkdir /usr/example/lualib/resty/html  
cd /usr/example/lualib/resty/html   
wget https://raw.githubusercontent.com/bungle/lua-resty-template/master/lib/resty/template/html.lua    
接下来就可以通过如下代码片段引用了

Java 代码

local template = require("resty.template")  
模板位置
我们需要告诉 lua-resty-template 去哪儿加载我们的模块，此处可以通过 set 指令定义template_location、template_root 或者从 root 指令定义的位置加载。

如我们可以在 example.conf 配置文件的 server 部分定义

Java 代码

\#first match ngx location  
set $template_location "/templates";  
\#then match root read file  
set $template_root "/usr/example/templates";  
也可以通过在server部分定义root指令  
Java 代码

root /usr/example/templates;  
其顺序是

Java 代码

local function load_ngx(path)  
    local file, location = path, ngx_var.template_location  
    if file:sub(1)  == "/" then file = file:sub(2) end  
    if location and location ~= "" then  
        if location:sub(-1) == "/" then location = location:sub(1, -2) end  
        local res = ngx_capture(location .. '/' .. file)  
        if res.status == 200 then return res.body end  
    end  
    local root = ngx_var.template_root or ngx_var.document_root  
    if root:sub(-1) == "/" then root = root:sub(1, -2) end  
    return read_file(root .. "/" .. file) or path  
end   
通过 ngx.location.capture从template_location 查找，如果找到（状态为为 200 ）则使用该内容作为模板；此种方式是一种动态获取模板方式；
如果定义了 template_root，则从该位置通过读取文件的方式加载模板；
如果没有定义 template_root，则默认从 root 指令定义的 document_root 处加载模板。
此处建议首先 template_root，如果实在有问题再使用 template_location，尽量不要通过 root 指令定义的 document_root 加载，因为其本身的含义不是给本模板引擎使用的。

接下来定义模板位置

Java 代码

mkdir /usr/example/templates  
mkdir /usr/example/templates2  
example.conf 配置 server 部分

Java 代码

\#first match ngx location  
set $template_location "/templates";  
\#then match root read file  
set $template_root "/usr/example/templates";  

location /templates {  
     internal;  
     alias /usr/example/templates2;  
}    
首先查找 /usr/example/template2，找不到会查找 /usr/example/templates。

然后创建两个模板文件

Java 代码

vim /usr/example/templates2/t1.html  
内容为

Java 代码

template2  
Java 代码

vim /usr/example/templates/t1.html   
内容为

Java 代码

template1  
test_temlate_1.lua

Java 代码

local template = require("resty.template")  
template.render("t1.html")  
example.conf 配置文件

Java 代码

location /lua_template_1 {  
    default_type 'text/html';  
    lua_code_cache on;  
    content_by_lua_file /usr/example/lua/test_template_1.lua;  
}    
访问如 http://192.168.1.2/lua_template_1 将看到 template2 输出。然后 rm/usr/example/templates2/t1.html，reload nginx 将看到 template1 输出。

接下来的测试我们会把模板文件都放到 /usr/example/templates 下。

API

使用模板引擎目的就是输出响应内容；主要用法两种：直接通过 ngx.print 输出或者得到模板渲染之后的内容按照想要的规则输出。

test_template_2.lua

Java 代码

local template = require("resty.template")  
--是否缓存解析后的模板，默认true  
template.caching(true)  
--渲染模板需要的上下文(数据)  
local context = {title = "title"}  
--渲染模板  
template.render("t1.html", context)  

ngx.say("<br/>")  
--编译得到一个lua函数  
local func = template.compile("t1.html")  
--执行函数，得到渲染之后的内容  
local content = func(context)  
--通过ngx API输出  
ngx.say(content)    
常见用法即如下两种方式：要么直接将模板内容直接作为响应输出，要么得到渲染后的内容然后按照想要的规则输出。

examle.conf 配置文件

Java 代码

location /lua_template_2 {  
    default_type 'text/html';  
    lua_code_cache on;  
    content_by_lua_file /usr/example/lua/test_template_2.lua;  
}  
使用示例
test_template_3.lua

Java 代码

local template = require("resty.template")  

local context = {  
    title = "测试",  
    name = "张三",  
    description = "<script>alert(1);</script>",  
    age = 20,  
    hobby = {"电影", "音乐", "阅读"},  
    score = {语文 = 90, 数学 = 80, 英语 = 70},  
    score2 = {  
        {name = "语文", score = 90},  
        {name = "数学", score = 80},  
        {name = "英语", score = 70},  
    }  
}  

template.render("t3.html", context)    
请确认文件编码为 UTF-8；context 即我们渲染模板使用的数据。

模板文件 /usr/example/templates/t3.html

Java 代码

{(header.html)}  
   <body>  
      {# 不转义变量输出 #}  
      姓名：{* string.upper(name) *}<br/>  
      {# 转义变量输出 #}  
      简介：{{description}}<br/>  
      {# 可以做一些运算 #}  
      年龄: {* age + 1 *}<br/>  
      {# 循环输出 #}  
      爱好：  
      {% for i, v in ipairs(hobby) do %}  
         {% if i > 1 then %}，{% end %}  
         {* v *}  
      {% end %}<br/>  

      成绩：  
      {% local i = 1; %}  
      {% for k, v in pairs(score) do %}  
         {% if i > 1 then %}，{% end %}  
         {* k *} = {* v *}  
         {% i = i + 1 %}  
      {% end %}<br/>  
      成绩2：  
      {% for i = 1, #score2 do local t = score2[i] %}  
         {% if i > 1 then %}，{% end %}  
          {* t.name *} = {* t.score *}  
      {% end %}<br/>  
      {# 中间内容不解析 #}  
      {-raw-}{(file)}{-raw-}  
{(footer.html)}    
{(include_file)}：包含另一个模板文件；
{ var }：变量输出；
{{ var }}：变量转义输出；
{% code %}：代码片段；
{# comment #}：注释；
{-raw-}：中间的内容不会解析，作为纯文本输出；
模板最终被转换为 Lua 代码进行执行，所以模板中可以执行任意 Lua 代码。

example.conf 配置文件

Java 代码

location /lua_template_3 {  
    default_type 'text/html';  
    lua_code_cache on;  
    content_by_lua_file /usr/example/lua/test_template_3.lua;  
}    
访问如 http://192.168.1.2/lua_template_3 进行测试。

基本的模板引擎使用到此就介绍完了。




HTTP 服务

此处我说的 HTTP 服务主要指如访问京东网站时我们看到的热门搜索、用户登录、实时价格、实时库存、服务支持、广告语等这种非 Web 页面，而是在 Web 页面中异步加载的相关数据。这些服务有个特点即访问量巨大、逻辑比较单一；但是如实时库存逻辑其实是非常复杂的。在京东这些服务每天有几亿十几亿的访问量，比如实时库存服务曾经在没有任何 IP 限流、DDos 防御的情况被刷到600多万/分钟的访问量，而且能轻松应对。支撑如此大的访问量就需要考虑设计良好的架构，并很容易实现水平扩展。

架构
此处介绍下我曾使用过 Nginx+JavaEE 的架构。

单 DB 架构



早期架构可能就是 Nginx 直接 upstream 请求到后端 Tomcat，扩容时基本是增加新的 Tomcat 实例，然后通过 Nginx 负载均衡 upstream 过去。此时数据库还不是瓶颈。当访问量到一定级别，数据库的压力就上来了，此处单纯的靠单个数据库可能扛不住了，此时可以通过数据库的读写分离或加缓存来实现。

DB+Cache/ 数据库读写分离架构



此时就通过使用如数据库读写分离或者 Redis 这种缓存来支撑更大的访问量。使用缓存这种架构会遇到的问题诸如缓存与数据库数据不同步造成数据不一致（一般设置过期时间），或者如 Redis 挂了，此时会直接命中数据库导致数据库压力过大；可以考虑 Redis 的主从或者一致性Hash 算法做分片的 Redis 集群；使用缓存这种架­构要求应用对数据的一致性要求不是很高；比如像下订单这种要落地的数据不适合用 Redis 存储，但是订单的读取可以使用缓存。

Nginx+Lua+Local Redis+Mysql 集群架构



首先 Nginx 通过 Lua 读取本机 Redis 缓存，如果不命中才回源到后端 Tomcat 集群；后端Tomcat 集群再读取 Mysql 数据库。Redis 都是安装到和 Nginx 同一台服务器，Nginx 直接读本机可以减少网络延时。Redis 通过主从方式同步数据，Redis 主从一般采用树的方式实现：



在叶子节点可以做 AOF 持久化，保证在主 Redis 挂时能进行恢复；此处假设对 Redis 很依赖的话，可以考虑多主 Redis 架构，而不是单主，来防止单主挂了时数据的不一致和击穿到后端Tomcat 集群。这种架构的缺点就是要求 Redis 实例数据量较小，如果单机内存不足以存储这么多数据，当然也可以通过如尾号为 1 的在 A 服务器，尾号为 2 的在 B 服务器这种方式实现；缺点也很明显，运维复杂、扩展性差。

Nginx+Lua+Redis 集群 +Mysql 集群架构



和之前架构不同的点是此时我们使用一致性 Hash 算法实现 Redis 集群而不是读本机 Redis，保证其中一台挂了，只有很少的数据会丢失，防止击穿到数据库。Redis 集群分片可以使用Twemproxy；如果 Tomcat 实例很多的话，此时就要考虑 Redis 和 Mysql 链接数问题，因为大部分 Redis/Mysql 客户端都是通过连接池实现，此时的链接数会成为瓶颈。一般方法是通过中间件来减少链接数。



Twemproxy 与 Redis 之间通过单链接交互，并 Twemproxy 实现分片逻辑；这样我们可以水平扩展更多的 Twemproxy 来增加链接数。

此时的问题就是 Twemproxy 实例众多，应用维护配置困难；此时就需要在之上做负载均衡，比如通过 LVS/HAProxy 实现 VIP（虚拟 IP ），可以做到切换对应用透明、故障自动转移；还可以通过实现内网 DNS 来做其负载均衡。



本文没有涉及 Nginx 之上是如何架构的，对于 Nginx、Redis、Mysql 等的负载均衡、资源的CDN 化不是本文关注的点，有兴趣可以参考

很早的 Taobao CDN 架构

Nginx/LVS/HAProxy 负载均衡软件的优缺点详解

实现
接下来我们来搭建一下第四种架构。



以获取如京东商品页广告词为例，如下图



假设京东有10亿商品，那么广告词极限情况是10亿；所以在设计时就要考虑：

数据量，数据更新是否频繁且更新量是否很大；
是K-V还是关系，是否需要批量获取，是否需要按照规则查询。
而对于本例，广告词更新量不会很大，每分钟可能在几万左右；而且是 K-V 的，其实适合使用关系存储；因为广告词是商家维护，因此后台查询需要知道这些商品是哪个商家的；而对于前台是不关心商家的，是 KV 存储，所以前台显示的可以放进如 Redis 中。 即存在两种设计：

所有数据存储到 Mysql，然后热点数据加载到 Redis；
关系存储到 Mysql，而数据存储到如SSDB这种持久化KV存储中。
基本数据结构：商品 ID、广告词、所属商家、开始时间、结束时间、是否有效。

后台逻辑
商家登录后台；
按照商家分页查询商家数据，此处要按照商品关键词或商品类目查询的话，需要走商品系统的搜索子系统，如通过 Solr或elasticsearch 实现搜索子系统；
进行广告词的增删改查；
增删改时可以直接更新 Redis 缓存或者只删除 Redis 缓存（第一次前台查询时写入缓存）；
前台逻辑
首先 Nginx 通过 Lua 查询 Redis 缓存；
查询不到的话回源到 Tomcat，Tomcat 读取数据库查询到数据，然后把最新的数据异步写入 Redis（一般设置过期时间，如5分钟）；此处设计时要考虑假设 Tomcat 读取 Mysql 的极限值是多少，然后设计降级开关，如假设每秒回源达到 100，则直接不查询 Mysql 而返回空的广告词来防止 Tomcat 应用雪崩。
为了简单，我们不进行后台的设计实现，只做前端的设计实现，此时数据结构我们简化为[商品ID、广告词]。另外有朋友可能看到了，可以直接把 Tomcat 部分干掉，通过 Lua 直接读取Mysql 进行回源实现。为了完整性此处我们还是做回源到 Tomcat 的设计，因为如果逻辑比较复杂的话或一些限制（比如使用 Java 特有协议的 RPC）还是通过 Java 去实现更方便一些。

项目搭建
项目部署目录结构。

Java 代码

/usr/chapter6  
  redis_6660.conf  
  redis_6661.conf  
  nginx_chapter6.conf  
  nutcracker.yml  
  nutcracker.init  
  webapp  
WEB-INF  
   lib  
   classes  
   web.xml   
Redis+Twemproxy 配置
此处根据实际情况来决定 Redis 大小，此处我们已两个 Redis 实例（6660、6661），在Twemproxy 上通过一致性 Hash 做分片逻辑。

安装

之前已经介绍过 Redis 和 Twemproxy 的安装了。

Redis配置redis_6660.conf和redis_6661.conf

Java 代码

\#分别为6660 6661  
port 6660  
\#进程ID 分别改为redis_6660.pid redis_6661.pid  
pidfile "/var/run/redis_6660.pid"  
\#设置内存大小，根据实际情况设置，此处测试仅设置20mb  
maxmemory 20mb  
\#内存不足时，按照过期时间进行LRU删除  
maxmemory-policy volatile-lru  
\#Redis的过期算法不是精确的而是通过采样来算的，默认采样为3个，此处我们改成10  
maxmemory-samples 10  
\#不进行RDB持久化  
save “”  
\#不进行AOF持久化  
appendonly no   
将如上配置放到 redis_6660.conf 和 redis_6661.conf 配置文件最后即可，后边的配置会覆盖前边的。

Twemproxy配置nutcracker.yml

Java 代码

server1:  
  listen: 127.0.0.1:1111  
  hash: fnv1a_64  
  distribution: ketama  
  redis: true  
  timeout: 1000  
  servers:  
   - 127.0.0.1:6660:1 server1  
   - 127.0.0.1:6661:1 server2   
复制 nutcracker.init 到 /usr/chapter6 下，并修改配置文件为 /usr/chapter6/nutcracker.yml。

启动

Java 代码

nohup /usr/servers/redis-2.8.19/src/redis-server  /usr/chapter6/redis_6660.conf &  
nohup /usr/servers/redis-2.8.19/src/redis-server  /usr/chapter6/redis_6661.conf &  
/usr/chapter6/nutcracker.init start  
ps -aux | grep -e redis  -e nutcracker  
Mysql+Atlas 配置
Atlas 类似于 Twemproxy，是 Qihoo 360 基于 Mysql Proxy 开发的一个 Mysql 中间件，据称每天承载读写请求数达几十亿，可以实现分表、读写分离、数据库连接池等功能，缺点是没有实现跨库分表（分库）功能，需要在客户端使用分库逻辑。另一个选择是使用如阿里的 TDDL，它是在客户端完成之前说的功能。到底选择是在客户端还是在中间件根据实际情况选择。

此处我们不做 Mysql 的主从复制（读写分离），只做分库分表实现。

Mysql 初始化

为了测试我们此处分两个表。

Java 代码

CREATE DATABASE chapter6 DEFAULT CHARACTER SET utf8;  
use chapter6;  
CREATE TABLE  chapter6.ad_0(  
      sku_id BIGINT,  
      content VARCHAR(4000)  
) ENGINE=InnoDB  DEFAULT CHARSET=utf8;  
CREATE TABLE  chapter6.ad_1  
      sku_id BIGINT,  
      content VARCHAR(4000)  
) ENGINE=InnoDB  DEFAULT CHARSET=utf8;  
Atlas 安装

Java 代码

cd /usr/servers/  
wget https://github.com/Qihoo360/Atlas/archive/2.2.1.tar.gz -O Atlas-2.2.1.tar.gz  
tar -xvf Atlas-2.2.1.tar.gz  
cd Atlas-2.2.1/  
\#Atlas依赖mysql_config，如果没有可以通过如下方式安装  
apt-get install libmysqlclient-dev  
\#安装Lua依赖  
wget http://www.lua.org/ftp/lua-5.1.5.tar.gz  
tar -xvf lua-5.1.5.tar.gz  
cd lua-5.1.5/  
make linux && make install  
\#安装glib依赖  
apt-get install libglib2.0-dev  
\#安装libevent依赖  
apt-get install libevent    
\#安装flex依赖  
apt-get install flex  
\#安装jemalloc依赖  
apt-get install libjemalloc-dev  
\#安装OpenSSL依赖  
apt-get install openssl  
apt-get install libssl-dev  
 apt-get install libssl0.9.8  

./configure --with-mysql=/usr/bin/mysql_config  
./bootstrap.sh  
make && make install  
Atlas 配置

Java 代码

vim /usr/local/mysql-proxy/conf/chapter6.cnf  
Java代码  收藏代码
[mysql-proxy]  
\#Atlas代理的主库，多个之间逗号分隔  
proxy-backend-addresses = 127.0.0.1:3306  
\#Atlas代理的从库，多个之间逗号分隔，格式ip:port@weight，权重默认1  
\#proxy-read-only-backend-addresses = 127.0.0.1:3306,127.0.0.1:3306  
\#用户名/密码，密码使用/usr/servers/Atlas-2.2.1/script/encrypt 123456加密  
pwds = root:/iZxz+0GRoA=  
\#后端进程运行  
daemon = true  
\#开启monitor进程，当worker进程挂了自动重启  
keepalive = true  
\#工作线程数，对Atlas的性能有很大影响，可根据情况适当设置  
event-threads = 64  
\#日志级别  
log-level = message  
\#日志存放的路径  
log-path = /usr/chapter6/  
\#实例名称，用于同一台机器上多个Atlas实例间的区分  
instance = test  
\#监听的ip和port  
proxy-address = 0.0.0.0:1112  
\#监听的管理接口的ip和port  
admin-address = 0.0.0.0:1113  
\#管理接口的用户名  
admin-username = admin  
\#管理接口的密码  
admin-password = 123456  
\#分表逻辑  
tables = chapter6.ad.sku_id.2  
\#默认字符集  
charset = utf8     
因为本例没有做读写分离，所以读库 proxy-read-only-backend-addresses 没有配置。分表逻辑即：数据库名.表名.分表键.表的个数，分表的表名格式是 table_N，N 从 0 开始。

Atlas 启动/重启/停止

Java 代码

/usr/local/mysql-proxy/bin/mysql-proxyd chapter6 start  
/usr/local/mysql-proxy/bin/mysql-proxyd chapter6 restart  
/usr/local/mysql-proxy/bin/mysql-proxyd chapter6 stop     
如上命令会自动到 /usr/local/mysql-proxy/conf 目录下查找 chapter6.cnf 配置文件。

Atlas 管理

通过如下命令进入管理接口

Java 代码

mysql -h127.0.0.1 -P1113  -uadmin -p123456  
通过执行 SELECT * FROM help 查看帮助。还可以通过一些 SQL 进行服务器的动态添加/移除。

Atlas 客户端

通过如下命令进入客户端接口

Java 代码

mysql -h127.0.0.1 -P1112  -uroot -p123456    
Java 代码

use chapter6;  
insert into ad values(1 '测试1);      
insert into ad values(2, '测试2');      
insert into ad values(3 '测试3);      
select * from ad where sku_id=1;  
select * from ad where sku_id=2;  
#通过如下sql可以看到实际的分表结果  
select * from ad_0;  
select * from ad_1;   
此时无法执行 select from ad，需要使用如 “select from ad where sku_id=1” 这种 SQL 进行查询；即需要带上 sku_id 且必须是相等比较；如果是范围或模糊是不可以的；如果想全部查询，只能挨着遍历所有表进行查询。即在客户端做查询-聚合。

此处实际的分表逻辑是按照商家进行分表，而不是按照商品编号，因为我们后台查询时是按照商家维度的，此处是为了测试才使用商品编号的。

到此基本的 Atlas 就介绍完了，更多内容请参考如下资料： Mysql 主从复制 http://369369.blog.51cto.com/319630/790921/ Mysql中间件介绍 http://www.guokr.com/blog/475765/ Atlas使用 http://www.0550go.com/database/mysql/mysql-atlas.html Atlas文档 https://github.com/Qihoo360/Atlas/blob/master/README_ZH.md

Java+Tomcat 安装
Java 安装

Java 代码

cd /usr/servers/  
\#首先到如下网站下载JDK  
\#http://www.oracle.com/technetwork/cn/java/javase/downloads/jdk7-downloads-1880260.html  
\#本文下载的是 jdk-7u75-linux-x64.tar.gz。  
tar -xvf jdk-7u75-linux-x64.tar.gz  
vim ~/.bashrc  
在文件最后添加如下环境变量  
export JAVA_HOME=/usr/servers/jdk1.7.0_75/  
export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH  
export CLASSPATH=$CLASSPATH:.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib  

\#使环境变量生效  
source ~/.bashrc  
Tomcat 安装

Java 代码

cd /usr/servers/  
wget http://ftp.cuhk.edu.hk/pub/packages/apache.org/tomcat/tomcat-7/v7.0.59/bin/apache-tomcat-7.0.59.tar.gz  
tar -xvf apache-tomcat-7.0.59.tar.gz  
cd apache-tomcat-7.0.59/  
\#启动   
/usr/servers/apache-tomcat-7.0.59/bin/startup.sh   
\#停止  
/usr/servers/apache-tomcat-7.0.59/bin/shutdown.sh  
\#删除tomcat默认的webapp  
rm -r apache-tomcat-7.0.59/webapps/*  
\#通过Catalina目录发布web应用  
cd apache-tomcat-7.0.59/conf/Catalina/localhost/  
vim ROOT.xml   
ROOT.xml

Java 代码

<!-- 访问路径是根，web应用所属目录为/usr/chapter6/webapp -->  
<Context path="" docBase="/usr/chapter6/webapp"></Context>    
Java 代码

\#创建一个静态文件随便添加点内容  
vim /usr/chapter6/webapp/index.html  
\#启动  
/usr/servers/apache-tomcat-7.0.59/bin/startup.sh    
访问如 http://192.168.1.2:8080/index.html 能处理内容说明配置成功。

Java 代码

\#变更目录结构  
cd /usr/servers/  
mv apache-tomcat-7.0.59 tomcat-server1  
\#此处我们创建两个tomcat实例  
cp –r tomcat-server1 tomcat-server2  
vim tomcat-server2/conf/server.xml     
Java 代码

\#如下端口进行变更  
8080--->8090  
8005--->8006  
启动两个 Tomcat

Java 代码

/usr/servers/tomcat-server1/bin/startup.sh   
/usr/servers/tomcat-server2/bin/startup.sh  
分别访问，如果能正常访问说明配置正常。

http://192.168.1.2:8080/index.html
http://192.168.1.2:8090/index.html
如上步骤使我们在一个服务器上能启动两个 tomcat 实例，这样的好处是我们可以做本机的 Tomcat 负载均衡，假设一个 tomcat 重启时另一个是可以工作的，从而不至于不给用户返回响应。

Java+Tomcat 逻辑开发
搭建项目

我们使用 Maven 搭建 Web 项目，Maven 知识请自行学习。

项目依赖

本文将最小化依赖，即仅依赖我们需要的 servlet、mysql、druid、jedis。

Java 代码

<dependencies>  
  <dependency>  
    <groupId>javax.servlet</groupId>  
    <artifactId>javax.servlet-api</artifactId>  
    <version>3.0.1</version>  
    <scope>provided</scope>  
  </dependency>  
  <dependency>  
    <groupId>mysql</groupId>  
    <artifactId>mysql-connector-java</artifactId>  
    <version>5.1.27</version>  
  </dependency>  
  <dependency>  
    <groupId>com.alibaba</groupId>  
    <artifactId>druid</artifactId>  
    <version>1.0.5</version>  
  </dependency>  
  <dependency>  
    <groupId>redis.clients</groupId>  
    <artifactId>jedis</artifactId>  
    <version>2.5.2</version>  
  </dependency>  
</dependencies>  
核心代码

com.github.zhangkaitao.chapter6.servlet.AdServlet

Java 代码

public class AdServlet extends HttpServlet {  
    @Override  
    protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {  
        String idStr = req.getParameter("id");  
        Long id = Long.valueOf(idStr);  
        //1、读取Mysql获取数据  
        String content = null;  
        try {  
            content = queryDB(id);  
        } catch (Exception e) {  
            e.printStackTrace();  
            resp.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);  
            return;  
        }  
        if(content != null) {  
            //2.1、如果获取到，异步写Redis  
            asyncSetToRedis(idStr, content);  
            //2.2、如果获取到，把响应内容返回  
            resp.setCharacterEncoding("UTF-8");  
            resp.getWriter().write(content);  
        } else {  
            //2.3、如果获取不到，返回404状态码  
            resp.setStatus(HttpServletResponse.SC_NOT_FOUND);  
        }  
    }  

    private DruidDataSource datasource = null;  
    private JedisPool jedisPool = null;  

    {  
        datasource = new DruidDataSource();  
        datasource.setUrl("jdbc:mysql://127.0.0.1:1112/chapter6?useUnicode=true&characterEncoding=utf-8&autoReconnect=true");  
        datasource.setUsername("root");  
        datasource.setPassword("123456");  
        datasource.setMaxActive(100);  

        GenericObjectPoolConfig poolConfig = new GenericObjectPoolConfig();  
        poolConfig.setMaxTotal(100);  
        jedisPool = new JedisPool(poolConfig, "127.0.0.1", 1111);  
    }  

    private String queryDB(Long id) throws Exception {  
        Connection conn = null;  
        try {  
            conn = datasource.getConnection();  
            String sql = "select content from ad where sku_id = ?";  
            PreparedStatement psst = conn.prepareStatement(sql);  
            psst.setLong(1, id);  
            ResultSet rs = psst.executeQuery();  
            String content = null;  
            if(rs.next()) {  
                content = rs.getString("content");  
            }  
            rs.close();  
            psst.close();  
            return content;  
        } catch (Exception e) {  
            throw e;  
        } finally {  
            if(conn != null) {  
                conn.close();  
            }  
        }  
    }  

    private ExecutorService executorService = Executors.newFixedThreadPool(10);  
    private void asyncSetToRedis(final String id, final String content) {  
        executorService.submit(new Runnable() {  
            @Override  
            public void run() {  
                Jedis jedis = null;  
                try {  
                    jedis = jedisPool.getResource();  
                    jedis.setex(id, 5 * 60, content);//5分钟  
                } catch (Exception e) {  
                    e.printStackTrace();  
                    jedisPool.returnBrokenResource(jedis);  
                } finally {  
                    jedisPool.returnResource(jedis);  
                }  

            }  
        });  
    }  
}    
整个逻辑比较简单，此处更新缓存一般使用异步方式去更新，这样不会阻塞主线程；另外此处可以考虑走 Servlet 异步化来提示吞吐量。

web.xml 配置

Java 代码

<servlet>  
    <servlet-name>adServlet</servlet-name>  
    <servlet-class>com.github.zhangkaitao.chapter6.servlet.AdServlet</servlet-class>  
</servlet>  
<servlet-mapping>  
    <servlet-name>adServlet</servlet-name>  
    <url-pattern>/ad</url-pattern>  
</servlet-mapping>  
打 WAR 包

Java 代码

cd D:\workspace\chapter6  
mvn clean package  
此处使用maven命令打包，比如本例将得到chapter6.war，然后将其上传到服务器的/usr/chapter6/webapp，然后通过unzip chapter6.war解压。
测试

启动 Tomcat 实例，分别访问如下地址将看到广告内容：

Java 代码

http://192.168.1.2:8080/ad?id=1  
http://192.168.1.2:8090/ad?id=1  
nginx 配置

vim /usr/chapter6/nginx_chapter6.conf

Java 代码

upstream backend {  
    server 127.0.0.1:8080 max_fails=5 fail_timeout=10s weight=1 backup=false;  
    server 127.0.0.1:8090 max_fails=5 fail_timeout=10s weight=1 backup=false;  
    check interval=3000 rise=1 fall=2 timeout=5000 type=tcp default_down=false;  
    keepalive 100;  
}  
server {  
    listen       80;  
    server_name  _;  

    location ~ /backend/(.*) {  
        keepalive_timeout   30s;  
        keepalive_requests  100;  

        rewrite /backend(/.*) $1 break;  
        #之后该服务将只有内部使用，ngx.location.capture  
        proxy_pass_request_headers off;  
        #more_clear_input_headers Accept-Encoding;  
        proxy_next_upstream error timeout;  
        proxy_pass http://backend;  
    }  
}   
upstream 配置：http://nginx.org/cn/docs/http/ngx_http_upstream_module.html。
server：指定上游到的服务器， weight：权重，权重可以认为负载均衡的比例； fail_timeout+max_fails：在指定时间内失败多少次认为服务器不可用，通过proxy_next_upstream来判断是否失败。
check：ngx_http_upstream_check_module模块，上游服务器的健康检查，interval：发送心跳包的时间间隔，rise：连续成功rise次数则认为服务器up，fall：连续失败fall次则认为服务器down，timeout：上游服务器请求超时时间，type：心跳检测类型（比如此处使用 tcp ）更多配置请参考 [https://github.com/yaoweibin/ nginx_upstream_check_module](https://github.com/yaoweibin/ nginx_upstream_check_module) 和 http://tengine.taobao.org/document_cn/http_upstream_check_cn.html。
keepalive：用来支持 upstream server http keepalive 特性(需要上游服务器支持，比如 tomcat )。默认的负载均衡算法是 round-robin，还可以根据 ip、url 等做 hash 来做负载均衡。更多资料请参考官方文档。

tomcat keepalive 配置： http://tomcat.apache.org/tomcat-7.0-doc/config/http.html。 maxKeepAliveRequests：默认100； keepAliveTimeout：默认等于 connectionTimeout，默认 60 秒；

location proxy 配置：http://nginx.org/cn/docs/http/ngx_http_proxy_module.html。 rewrite：将当前请求的 url 重写，如我们请求时是 /backend/ad，则重写后是 /ad。 proxy_pass：将整个请求转发到上游服务器。 proxy_next_upstream：什么情况认为当前 upstream server 失败，需要 next upstream，默认是连接失败/超时，负载均衡参数。 proxy_pass_request_headers：之前已经介绍过了，两个原因：1、假设上游服务器不需要请求头则没必要传输请求头；2、ngx.location.capture时防止 gzip 乱码（也可以使用more_clear_input_headers 配置）。 keepalive：keepalive_timeout：keepalive 超时设置，keepalive_requests：长连接数量。此处的 keepalive（别人访问该 location 时的长连接）和 upstream keepalive（nginx 与上游服务器的长连接）是不一样的；此处注意，如果您的服务是面向客户的，而且是单个动态内容就没必要使用长连接了。

vim /usr/servers/nginx/conf/nginx.conf

Java 代码

include /usr/chapter6/nginx_chapter6.conf;  
\#为了方便测试，注释掉example.conf  
\#include /usr/example/example.conf;  
重启 nginx

/usr/servers/nginx/sbin/nginx -s reload

访问如 192.168.1.2/backend/ad?id=1 即看到结果。可以 kill 掉一个 tomcat，可以看到服务还是正常的。

vim /usr/chapter6/nginx_chapter6.conf

Java 代码

location ~ /backend/(.*) {  
    internal;  
    keepalive_timeout   30s;  
    keepalive_requests  1000;  
    #支持keep-alive  
    proxy_http_version 1.1;  
    proxy_set_header Connection "";  

    rewrite /backend(/.*) $1 break;  
    proxy_pass_request_headers off;  
    #more_clear_input_headers Accept-Encoding;  
    proxy_next_upstream error timeout;  
    proxy_pass http://backend;  
}   
加上 internal，表示只有内部使用该服务。

Nginx+Lua 逻辑开发
核心代码

/usr/chapter6/ad.lua

Java 代码

local redis = require("resty.redis")  
local cjson = require("cjson")  
local cjson_encode = cjson.encode  
local ngx_log = ngx.log  
local ngx_ERR = ngx.ERR  
local ngx_exit = ngx.exit  
local ngx_print = ngx.print  
local ngx_re_match = ngx.re.match  
local ngx_var = ngx.var  

local function close_redis(red)  
    if not red then  
        return  
    end  
    --释放连接(连接池实现)  
    local pool_max_idle_time = 10000 --毫秒  
    local pool_size = 100 --连接池大小  
    local ok, err = red:set_keepalive(pool_max_idle_time, pool_size)  

    if not ok then  
        ngx_log(ngx_ERR, "set redis keepalive error : ", err)  
    end  
end  
local function read_redis(id)  
    local red = redis:new()  
    red:set_timeout(1000)  
    local ip = "127.0.0.1"  
    local port = 1111  
    local ok, err = red:connect(ip, port)  
    if not ok then  
        ngx_log(ngx_ERR, "connect to redis error : ", err)  
        return close_redis(red)  
    end  

    local resp, err = red:get(id)  
    if not resp then  
        ngx_log(ngx_ERR, "get redis content error : ", err)  
        return close_redis(red)  
    end  
        --得到的数据为空处理  
    if resp == ngx.null then  
        resp = nil  
    end  
    close_redis(red)  

    return resp  
end  

local function read_http(id)  
    local resp = ngx.location.capture("/backend/ad", {  
        method = ngx.HTTP_GET,  
        args = {id = id}  
    })  

    if not resp then  
        ngx_log(ngx_ERR, "request error :", err)  
        return  
    end  

    if resp.status ~= 200 then  
        ngx_log(ngx_ERR, "request error, status :", resp.status)  
        return  
    end  

    return resp.body  
end  

--获取id  
local id = ngx_var.id  

--从redis获取  
local content = read_redis(id)  

--如果redis没有，回源到tomcat  
if not content then  
   ngx_log(ngx_ERR, "redis not found content, back to http, id : ", id)  
    content = read_http(id)  
end  

--如果还没有返回404  
if not content then  
   ngx_log(ngx_ERR, "http not found content, id : ", id)  
   return ngx_exit(404)  
end  

--输出内容  
ngx.print("show_ad(")  
ngx_print(cjson_encode({content = content}))  
ngx.print(")")   
将可能经常用的变量做成局部变量，如 local ngx_print = ngx.print；使用 jsonp 方式输出，此处我们可以将请求 url 限定为 /ad/id 方式，这样的好处是1、可以尽可能早的识别无效请求；2、可以走 nginx 缓存 /CDN 缓存，缓存的 key 就是 URL，而不带任何参数，防止那些通过加随机数穿透缓存；3、jsonp 使用固定的回调函数 show_ad()，或者限定几个固定的回调来减少缓存的版本。

vim /usr/chapter6/nginx_chapter6.conf

Java 代码

location ~ ^/ad/(\d+)$ {  
    default_type 'text/html';  
    charset utf-8;  
    lua_code_cache on;  
    set $id $1;  
    content_by_lua_file /usr/chapter6/ad.lua;  
}  
重启 nginx

Java 代码

/usr/servers/nginx/sbin/nginx -s reload  
访问如 http://192.168.1.2/ad/1 即可得到结果。而且注意观察日志，第一次访问时不命中Redis，回源到 Tomcat；第二次请求时就会命中 Redis 了。

第一次访问时将看到 /usr/servers/nginx/logs/error.log 输出类似如下的内容，而第二次请求相同的 url 不再有如下内容：

Java 代码

redis not found content, back to http, id : 2  
到此整个架构就介绍完了，此处可以直接不使用 Tomcat，而是 Lua 直连 Mysql 做回源处理；另外本文只是介绍了大体架构，还有更多业务及运维上的细节需要在实际应用中根据自己的场景自己摸索。后续如使用 LVS/HAProxy 做负载均衡、使用 CDN 等可以查找资料学习。





Web 开发实战2——商品详情页

本章以京东商品详情页为例，京东商品详情页虽然仅是单个页面，但是其数据聚合源是非常多的，除了一些实时性要求比较高的如价格、库存、服务支持等通过 AJAX 异步加载加载之外，其他的数据都是在后端做数据聚合然后拼装网页模板的。 http://item.jd.com/1217499.html




如图所示，商品页主要包括商品基本信息（基本信息、图片列表、颜色/尺码关系、扩展属性、规格参数、包装清单、售后保障等）、商品介绍、其他信息（分类、品牌、店铺【第三方卖家】、店内分类【第三方卖家】、同类相关品牌）。更多细节此处就不阐述了。

整个京东有数亿商品，如果每次动态获取如上内容进行模板拼装，数据来源之多足以造成性能无法满足要求；最初的解决方案是生成静态页，但是静态页的最大的问题：1、无法迅速响应页面需求变更；2、很难做多版本线上对比测试。如上两个因素足以制约商品页的多样化发展，因此静态化技术不是很好的方案。

通过分析，数据主要分为四种：商品页基本信息、商品介绍（异步加载）、其他信息（分类、品牌、店铺等）、其他需要实时展示的数据（价格、库存等）。而其他信息如分类、品牌、店铺是非常少的，完全可以放到一个占用内存很小的 Redis 中存储；而商品基本信息我们可以借鉴静态化技术将数据做聚合存储，这样的好处是数据是原子的，而模板是随时可变的，吸收了静态页聚合的优点，弥补了静态页的多版本缺点；另外一个非常严重的问题就是严重依赖这些相关系统，如果它们挂了或响应慢则商品页就挂了或响应慢；商品介绍我们也通过 AJAX 技术惰性加载（因为是第二屏，只有当用户滚动鼠标到该屏时才显示）；而实时展示数据通过 AJAX 技术做异步加载；因此我们可以做如下设计：

接收商品变更消息，做商品基本信息的聚合，即从多个数据源获取商品相关信息如图片列表、颜色尺码、规格参数、扩展属性等等，聚合为一个大的 JSON 数据做成数据闭环，以 key-value存储；因为是闭环，即使依赖的系统挂了我们商品页还是能继续服务的，对商品页不会造成任何影响；
接收商品介绍变更消息，存储商品介绍信息；
介绍其他信息变更消息，存储其他信息。
整个架构如下图所示：



技术选型
MQ 可以使用如 Apache ActiveMQ； Worker/ 动态服务可以通过如 Java 技术实现； RPC 可以选择如 alibaba Dubbo； KV 持久化存储可以选择 SSDB（如果使用 SSD 盘则可以选择 SSDB+RocksDB 引擎）或者 ARDB（ LMDB 引擎版）； 缓存使用 Redis； SSDB/Redis 分片使用如 Twemproxy，这样不管使用 Java 还是 Nginx+Lua，它们都不关心分片逻辑； 前端模板拼装使用 Nginx+Lua； 数据集群数据存储的机器可以采用 RAID 技术或者主从模式防止单点故障； 因为数据变更不频繁，可以考虑 SSD 替代机械硬盘。

核心流程
首先我们监听商品数据变更消息；
接收到消息后，数据聚合 Worker 通过 RPC 调用相关系统获取所有要展示的数据，此处获取数据的来源可能非常多而且响应速度完全受制于这些系统，可能耗时几百毫秒甚至上秒的时间；
将数据聚合为 JSON 串存储到相关数据集群；
前端 Nginx 通过 Lua 获取相关集群的数据进行展示；商品页需要获取基本信息+其他信息进行模板拼装，即拼装模板仅需要两次调用（另外因为其他信息数据量少且对一致性要求不高，因此我们完全可以缓存到 Nginx 本地全局内存，这样可以减少远程调用提高性能）；当页面滚动到商品介绍页面时异步调用商品介绍服务获取数据；
如果从聚合的 SSDB 集群 /Redis 中获取不到相关数据；则回源到动态服务通过 RPC 调用相关系统获取所有要展示的数据返回（此处可以做限流处理，因为如果大量请求过来的话可能导致服务雪崩，需要采取保护措施），此处的逻辑和数据聚合 Worker 完全一样；然后发送 MQ 通知数据变更，这样下次访问时就可以从聚合的 SSDB 集群 /Redis 中获取数据了。
基本流程如上所述，主要分为 Worker、动态服务、数据存储和前端展示；因为系统非常复杂，只介绍动态服务和前端展示、数据存储架构；Worker 部分不做实现。

项目搭建
项目部署目录结构。

/usr/chapter7
  ssdb_basic_7770.conf
  ssdb_basic_7771.conf
  ssdb_basic_7772.conf
  ssdb_basic_7773.conf
  ssdb_desc_8880.conf
  ssdb_desc_8881.conf
  ssdb_desc_8882.conf
  ssdb_desc_8883.conf
  redis_other_6660.conf
  redis_other_6661.conf
  nginx_chapter7.conf
  nutcracker.yml
  nutcracker.init
  item.html
  header.html
  footer.html
  item.lua
  desc.lua
  lualib
    item.lua
    item
      common.lua
  webapp
WEB-INF
   lib
   classes
   web.xml
数据存储实现


整体架构为主从模式，写数据到主集群，读数据从从集群读取数据，这样当一个集群不足以支撑流量时可以使用更多的集群来支撑更多的访问量；集群分片使用 Twemproxy 实现。

商品基本信息 SSDB 集群配置

vim /usr/chapter7/ssdb_basic_7770.conf \

Java 代码

work_dir = /usr/data/ssdb_7770  
pidfile = /usr/data/ssdb_7770.pid  

server:  
        ip: 0.0.0.0  
        port: 7770  
        allow: 127.0.0.1  
        allow: 192.168  

replication:  
        binlog: yes  
        sync_speed: -1  
        slaveof:  
logger:  
        level: error  
        output: /usr/data/ssdb_7770.log  
        rotate:  
                size: 1000000000  

leveldb:  
        cache_size: 500  
        block_size: 32  
        write_buffer_size: 64  
        compaction_speed: 1000  
        compression: yes  
vim /usr/chapter7/ssdb_basic_7771.conf

Java 代码

work_dir = /usr/data/ssdb_7771  
pidfile = /usr/data/ssdb_7771.pid  

server:  
        ip: 0.0.0.0  
        port: 7771  
        allow: 127.0.0.1  
        allow: 192.168  

replication:  
        binlog: yes  
        sync_speed: -1  
        slaveof:  
logger:  
        level: error  
        output: /usr/data/ssdb_7771.log  
        rotate:  
                size: 1000000000  

leveldb:  
        cache_size: 500  
        block_size: 32  
        write_buffer_size: 64  
        compaction_speed: 1000  
        compression: yes   
vim /usr/chapter7/ssdb_basic_7772.conf

Java 代码

work_dir = /usr/data/ssdb_7772  
pidfile = /usr/data/ssdb_7772.pid  

server:  
        ip: 0.0.0.0  
        port: 7772  
        allow: 127.0.0.1  
        allow: 192.168  

replication:  
        binlog: yes  
        sync_speed: -1  
        slaveof:  
                type: sync  
                ip: 127.0.0.1  
                port: 7770  

logger:  
        level: error  
        output: /usr/data/ssdb_7772.log  
        rotate:  
                size: 1000000000  

leveldb:  
        cache_size: 500  
        block_size: 32  
        write_buffer_size: 64  
        compaction_speed: 1000  
        compression: yes  
vim /usr/chapter7/ssdb_basic_7773.conf

Java 代码

work_dir = /usr/data/ssdb_7773  
pidfile = /usr/data/ssdb_7773.pid  

server:  
        ip: 0.0.0.0  
        port: 7773  
        allow: 127.0.0.1  
        allow: 192.168  

replication:  
        binlog: yes  
        sync_speed: -1  
        slaveof:  
                type: sync  
                ip: 127.0.0.1  
                port: 7771  

logger:  
        level: error  
        output: /usr/data/ssdb_7773.log  
        rotate:  
                size: 1000000000  

leveldb:  
        cache_size: 500  
        block_size: 32  
        write_buffer_size: 64  
        compaction_speed: 1000  
        compression: yes  
配置文件使用 Tab 而不是空格做缩排，（复制到配置文件后请把空格替换为 Tab ）。主从关系：7770(主)-->7772(从)，7771(主)--->7773(从)；配置文件如何配置请参考 https://github.com/ideawu/ssdb-docs/blob/master/src/zh_cn/config.md。

创建工作目录

Java 代码

mkdir -p /usr/data/ssdb_7770  
mkdir -p /usr/data/ssdb_7771  
mkdir -p /usr/data/ssdb_7772  
mkdir -p /usr/data/ssdb_7773  
启动

Java 代码

nohup /usr/servers/ssdb-1.8.0/ssdb-server  /usr/chapter7/ssdb_basic_7770.conf &  
nohup /usr/servers/ssdb-1.8.0/ssdb-server  /usr/chapter7/ssdb_basic_7771.conf &  
nohup /usr/servers/ssdb-1.8.0/ssdb-server  /usr/chapter7/ssdb_basic_7772.conf &  
nohup /usr/servers/ssdb-1.8.0/ssdb-server  /usr/chapter7/ssdb_basic_7773.conf &   
通过 ps -aux | grep ssdb 命令看是否启动了，tail -f nohup.out 查看错误信息。

商品介绍 SSDB 集群配置

vim /usr/chapter7/ssdb_desc_8880.conf

Java 代码

work_dir = /usr/data/ssdb_8880  
pidfile = /usr/data/ssdb8880.pid  

server:  
        ip: 0.0.0.0  
        port: 8880  
        allow: 127.0.0.1  
        allow: 192.168  

replication:  
        binlog: yes  
        sync_speed: -1  
        slaveof:  
logger:  
        level: error  
        output: /usr/data/ssdb_8880.log  
        rotate:  
                size: 1000000000  

leveldb:  
        cache_size: 500  
        block_size: 32  
        write_buffer_size: 64  
        compaction_speed: 1000  
        compression: yes  
vim /usr/chapter7/ssdb_desc_8881.conf

Java 代码

work_dir = /usr/data/ssdb_8881  
pidfile = /usr/data/ssdb8881.pid  

server:  
        ip: 0.0.0.0  
        port: 8881  
        allow: 127.0.0.1  
        allow: 192.168  

logger:  
        level: error  
        output: /usr/data/ssdb_8881.log  
        rotate:  
                size: 1000000000  

leveldb:  
        cache_size: 500  
        block_size: 32  
        write_buffer_size: 64  
        compaction_speed: 1000  
        compression: yes  
vim /usr/chapter7/ssdb_desc_8882.conf

Java 代码

work_dir = /usr/data/ssdb_8882  
pidfile = /usr/data/ssdb_8882.pid  

server:  
        ip: 0.0.0.0  
        port: 8882  
        allow: 127.0.0.1  
        allow: 192.168  

replication:  
        binlog: yes  
        sync_speed: -1  
        slaveof:  
replication:  
        binlog: yes  
        sync_speed: -1  
        slaveof:  
                type: sync  
                ip: 127.0.0.1  
                port: 8880  

logger:  
        level: error  
        output: /usr/data/ssdb_8882.log  
        rotate:  
                size: 1000000000  

leveldb:  
        cache_size: 500  
        block_size: 32  
        write_buffer_size: 64  
        compaction_speed: 1000  
        compression: yes  
vim /usr/chapter7/ssdb_desc_8883.conf

Java 代码

work_dir = /usr/data/ssdb_8883  
pidfile = /usr/data/ssdb_8883.pid  

server:  
        ip: 0.0.0.0  
        port: 8883  
        allow: 127.0.0.1  
        allow: 192.168  

replication:  
        binlog: yes  
        sync_speed: -1  
        slaveof:  
                type: sync  
                ip: 127.0.0.1  
                port: 8881  

logger:  
        level: error  
        output: /usr/data/ssdb_8883.log  
        rotate:  
                size: 1000000000  

leveldb:  
        cache_size: 500  
        block_size: 32  
        write_buffer_size: 64  
        compaction_speed: 1000  
        compression: yes  
配置文件使用 Tab 而不是空格做缩排（复制到配置文件后请把空格替换为 Tab ）。主从关系：7770(主)-->7772(从)，7771(主)--->7773(从)；配置文件如何配置请参考 https://github.com/ideawu/ssdb-docs/blob/master/src/zh_cn/config.md。

创建工作目录

Java 代码

mkdir -p /usr/data/ssdb_888{0,1,2,3}  
启动

Java 代码

nohup /usr/servers/ssdb-1.8.0/ssdb-server  /usr/chapter7/ssdb_desc_8880.conf &  
nohup /usr/servers/ssdb-1.8.0/ssdb-server  /usr/chapter7/ssdb_desc_8881.conf &  
nohup /usr/servers/ssdb-1.8.0/ssdb-server  /usr/chapter7/ssdb_desc_8882.conf &  
nohup /usr/servers/ssdb-1.8.0/ssdb-server  /usr/chapter7/ssdb_desc_8883.conf &   
通过 ps -aux | grep ssdb 命令看是否启动了，tail -f nohup.out 查看错误信息。

其他信息 Redis 配置

vim /usr/chapter7/redis_6660.conf

Java 代码

port 6660  
pidfile "/var/run/redis_6660.pid"  
\#设置内存大小，根据实际情况设置，此处测试仅设置20mb  
maxmemory 20mb  
\#内存不足时，所有KEY按照LRU算法删除  
maxmemory-policy allkeys-lru  
\#Redis的过期算法不是精确的而是通过采样来算的，默认采样为3个，此处我们改成10  
maxmemory-samples 10  
\#不进行RDB持久化  
save “”  
\#不进行AOF持久化  
appendonly no  
vim /usr/chapter7/redis_6661.conf

Java 代码

port 6661  
pidfile "/var/run/redis_6661.pid"  
\#设置内存大小，根据实际情况设置，此处测试仅设置20mb  
maxmemory 20mb  
\#内存不足时，所有KEY按照LRU算法进行删除  
maxmemory-policy allkeys-lru  
\#Redis的过期算法不是精确的而是通过采样来算的，默认采样为3个，此处我们改成10  
maxmemory-samples 10  
\#不进行RDB持久化  
save “”  
\#不进行AOF持久化  
appendonly no  
\#主从  
slaveof 127.0.0.1 6660  
vim /usr/chapter7/redis_6662.conf

Java 代码

port 6662  
pidfile "/var/run/redis_6662.pid"  
\#设置内存大小，根据实际情况设置，此处测试仅设置20mb  
maxmemory 20mb  
\#内存不足时，所有KEY按照LRU算法进行删除  
maxmemory-policy allkeys-lru  
\#Redis的过期算法不是精确的而是通过采样来算的，默认采样为3个，此处我们改成10  
maxmemory-samples 10  
\#不进行RDB持久化  
save “”  
\#不进行AOF持久化  
appendonly no  
\#主从  
slaveof 127.0.0.1 6660    
如上配置放到配置文件最末尾即可；此处内存不足时的驱逐算法为所有 KEY 按照 LRU 进行删除（实际是内存基本上不会遇到满的情况）；主从关系：6660(主)-->6661(从)和6660(主)-->6662(从)。

启动

Java 代码

nohup /usr/servers/redis-2.8.19/src/redis-server /usr/chapter7/redis_6660.conf &  
nohup /usr/servers/redis-2.8.19/src/redis-server /usr/chapter7/redis_6661.conf &  
nohup /usr/servers/redis-2.8.19/src/redis-server /usr/chapter7/redis_6662.conf &  
通过 ps -aux | grep redis 命令看是否启动了，tail -f nohup.out 查看错误信息。

测试

测试时在主 SSDB/Redis 中写入数据，然后从从 SSDB/Redis 能读取到数据即表示配置主从成功。

测试商品基本信息 SSDB 集群

Java 代码

root@kaitao:/usr/chapter7# /usr/servers/redis-2.8.19/src/redis-cli  -p 7770  
127.0.0.1:7770> set i 1  
OK  
127.0.0.1:7770>   
root@kaitao:/usr/chapter7# /usr/servers/redis-2.8.19/src/redis-cli  -p 7772  
127.0.0.1:7772> get i  
"1"  
测试商品介绍 SSDB 集群

Java 代码

root@kaitao:/usr/chapter7# /usr/servers/redis-2.8.19/src/redis-cli  -p 8880  
127.0.0.1:8880> set i 1  
OK  
127.0.0.1:8880>   
root@kaitao:/usr/chapter7# /usr/servers/redis-2.8.19/src/redis-cli  -p 8882  
127.0.0.1:8882> get i  
"1"  
测试其他信息集群

Java 代码

root@kaitao:/usr/chapter7# /usr/servers/redis-2.8.19/src/redis-cli  -p 6660  
127.0.0.1:6660> set i 1  
OK  
127.0.0.1:6660> get i  
"1"  
127.0.0.1:6660>   
root@kaitao:/usr/chapter7# /usr/servers/redis-2.8.19/src/redis-cli  -p 6661  
127.0.0.1:6661> get i  
"1"  
Twemproxy 配置

vim /usr/chapter7/nutcracker.yml

Java 代码

basic_master:  
  listen: 127.0.0.1:1111  
  hash: fnv1a_64  
  distribution: ketama  
  redis: true  
  timeout: 1000  
  hash_tag: "::"  
  servers:  
   - 127.0.0.1:7770:1 server1  
   - 127.0.0.1:7771:1 server2  

basic_slave:  
  listen: 127.0.0.1:1112  
  hash: fnv1a_64  
  distribution: ketama  
  redis: true  
  timeout: 1000  
  hash_tag: "::"  
  servers:  
   - 127.0.0.1:7772:1 server1  
   - 127.0.0.1:7773:1 server2  

desc_master:  
  listen: 127.0.0.1:1113  
  hash: fnv1a_64  
  distribution: ketama  
  redis: true  
  timeout: 1000  
  hash_tag: "::"  
  servers:  
   - 127.0.0.1:8880:1 server1  
   - 127.0.0.1:8881:1 server2  

desc_slave:  
  listen: 127.0.0.1:1114  
  hash: fnv1a_64  
  distribution: ketama  
  redis: true  
  timeout: 1000  
  servers:  
   - 127.0.0.1:8882:1 server1  
   - 127.0.0.1:8883:1 server2  

other_master:  
  listen: 127.0.0.1:1115  
  hash: fnv1a_64  
  distribution: random  
  redis: true  
  timeout: 1000  
  hash_tag: "::"  
  servers:  
   - 127.0.0.1:6660:1 server1  

other_slave:  
  listen: 127.0.0.1:1116  
  hash: fnv1a_64  
  distribution: random  
  redis: true  
  timeout: 1000  
  hash_tag: "::"  
  servers:  
   - 127.0.0.1:6661:1 server1  
   - 127.0.0.1:6662:1 server2   
因为我们使用了主从，所以需要给 server 起一个名字如 server1、server2；否则分片算法默认根据 ip:port:weight，这样就会主从数据的分片算法不一致；
其他信息 Redis 因为每个 Redis 是对等的，因此分片算法可以使用 random；
我们使用了 hash_tag，可以保证相同的 tag 在一个分片上（本例配置了但没有用到该特性）。
复制第六章的 nutcracker.init，帮把配置文件改为 usr/chapter7/nutcracker.yml。然后通过 /usr/chapter7/nutcracker.init start 启动 Twemproxy。

测试主从集群是否工作正常：

Java 代码

root@kaitao:/usr/chapter7# /usr/servers/redis-2.8.19/src/redis-cli -p 1111  
127.0.0.1:1111> set i 1  
OK  
127.0.0.1:1111>   
root@kaitao:/usr/chapter7# /usr/servers/redis-2.8.19/src/redis-cli -p 1112  
127.0.0.1:1112> get i  
"1"  
127.0.0.1:1112>   
root@kaitao:/usr/chapter7# /usr/servers/redis-2.8.19/src/redis-cli -p 1113  
127.0.0.1:1113> set i 1  
OK  
127.0.0.1:1113>   
root@kaitao:/usr/chapter7# /usr/servers/redis-2.8.19/src/redis-cli -p 1114  
127.0.0.1:1114> get i  
"1"  
127.0.0.1:1114>   
root@kaitao:/usr/chapter7# /usr/servers/redis-2.8.19/src/redis-cli -p 1115  
127.0.0.1:1115> set i 1  
OK  
127.0.0.1:1115>   
root@kaitao:/usr/chapter7# /usr/servers/redis-2.8.19/src/redis-cli -p 1116  
127.0.0.1:1116> get i  
"1"  
到此数据集群配置成功。

动态服务实现
因为真实数据是从多个子系统获取，很难模拟这么多子系统交互，所以此处我们使用假数据来进行实现。

项目搭建

我们使用 Maven 搭建 Web 项目，Maven 知识请自行学习。

项目依赖

本文将最小化依赖，即仅依赖我们需要的 servlet、jackson、guava、jedis。

Java 代码

<dependencies>  
  <dependency>  
    <groupId>javax.servlet</groupId>  
    <artifactId>javax.servlet-api</artifactId>  
    <version>3.0.1</version>  
    <scope>provided</scope>  
  </dependency>  
  <dependency>  
    <groupId>com.google.guava</groupId>  
    <artifactId>guava</artifactId>  
    <version>17.0</version>  
  </dependency>  
  <dependency>  
    <groupId>redis.clients</groupId>  
    <artifactId>jedis</artifactId>  
    <version>2.5.2</version>  
  </dependency>  
  <dependency>  
    <groupId>com.fasterxml.jackson.core</groupId>  
    <artifactId>jackson-core</artifactId>  
    <version>2.3.3</version>  
  </dependency>  
  <dependency>  
    <groupId>com.fasterxml.jackson.core</groupId>  
    <artifactId>jackson-databind</artifactId>  
    <version>2.3.3</version>  
  </dependency>  
</dependencies>  
guava 是类似于 apache commons 的一个基础类库，用于简化一些重复操作，可以参考http://ifeve.com/google-guava/。

核心代码

com.github.zhangkaitao.chapter7.servlet.ProductServiceServlet

Java 代码

@Override  
protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {  
    String type = req.getParameter("type");  
    String content = null;  
    try {  
        if("basic".equals(type)) {  
            content = getBasicInfo(req.getParameter("skuId"));  
        } else if("desc".equals(type)) {  
            content = getDescInfo(req.getParameter("skuId"));  
        } else if("other".equals(type)) {  
            content = getOtherInfo(req.getParameter("ps3Id"), req.getParameter("brandId"));  
        }  
    } catch (Exception e) {  
        e.printStackTrace();  
        resp.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);  
        return;  
    }  
    if(content != null) {  
        resp.setCharacterEncoding("UTF-8");  
        resp.getWriter().write(content);  
    } else {  
        resp.setStatus(HttpServletResponse.SC_NOT_FOUND);  
    }  
}    
根据请求参数 type 来决定调用哪个服务获取数据。

基本信息服务

Java 代码

private String getBasicInfo(String skuId) throws Exception {  
    Map<String, Object> map = new HashMap<String, Object>();  
    //商品编号  
    map.put("skuId", skuId);  
    //名称  
    map.put("name", "苹果（Apple）iPhone 6 (A1586) 16GB 金色 移动联通电信4G手机");  
    //一级二级三级分类  
    map.put("ps1Id", 9987);  
    map.put("ps2Id", 653);  
    map.put("ps3Id", 655);  
    //品牌ID  
    map.put("brandId", 14026);  
    //图片列表  
    map.put("imgs", getImgs(skuId));  
    //上架时间  
    map.put("date", "2014-10-09 22:29:09");  
    //商品毛重  
    map.put("weight", "400");  
    //颜色尺码  
    map.put("colorSize", getColorSize(skuId));  
    //扩展属性  
    map.put("expands", getExpands(skuId));  
    //规格参数  
    map.put("propCodes", getPropCodes(skuId));  
    map.put("date", System.currentTimeMillis());  
    String content = objectMapper.writeValueAsString(map);  
    //实际应用应该是发送MQ  
    asyncSetToRedis(basicInfoJedisPool, "p:" + skuId + ":", content);  
    return objectMapper.writeValueAsString(map);  
}  

private List<String> getImgs(String skuId) {  
    return Lists.newArrayList(  
            "jfs/t277/193/1005339798/768456/29136988/542d0798N19d42ce3.jpg",  
            "jfs/t352/148/1022071312/209475/53b8cd7f/542d079bN3ea45c98.jpg",  
            "jfs/t274/315/1008507116/108039/f70cb380/542d0799Na03319e6.jpg",  
            "jfs/t337/181/1064215916/27801/b5026705/542d079aNf184ce18.jpg"  
    );  
}  

private List<Map<String, Object>> getColorSize(String skuId) {  
    return Lists.newArrayList(  
        makeColorSize(1217499, "金色", "公开版（16GB ROM）"),  
        makeColorSize(1217500, "深空灰", "公开版（16GB ROM）"),  
        makeColorSize(1217501, "银色", "公开版（16GB ROM）"),  
        makeColorSize(1217508, "金色", "公开版（64GB ROM）"),  
        makeColorSize(1217509, "深空灰", "公开版（64GB ROM）"),  
        makeColorSize(1217509, "银色", "公开版（64GB ROM）"),  
        makeColorSize(1217493, "金色", "移动4G版 （16GB）"),  
        makeColorSize(1217494, "深空灰", "移动4G版 （16GB）"),  
        makeColorSize(1217495, "银色", "移动4G版 （16GB）"),  
        makeColorSize(1217503, "金色", "移动4G版 （64GB）"),  
        makeColorSize(1217503, "金色", "移动4G版 （64GB）"),  
        makeColorSize(1217504, "深空灰", "移动4G版 （64GB）"),  
        makeColorSize(1217505, "银色", "移动4G版 （64GB）")  
    );  
}  
private Map<String, Object> makeColorSize(long skuId, String color, String size) {  
    Map<String, Object> cs1 = Maps.newHashMap();  
    cs1.put("SkuId", skuId);  
    cs1.put("Color", color);  
    cs1.put("Size", size);  
    return cs1;  
}  

private List<List<?>> getExpands(String skuId) {  
    return Lists.newArrayList(  
            (List<?>)Lists.newArrayList("热点", Lists.newArrayList("超薄7mm以下", "支持NFC")),  
            (List<?>)Lists.newArrayList("系统", "苹果（IOS）"),  
            (List<?>)Lists.newArrayList("系统", "苹果（IOS）"),  
            (List<?>)Lists.newArrayList("购买方式", "非合约机")  
    );  
}  

private Map<String, List<List<String>>> getPropCodes(String skuId) {  
    Map<String, List<List<String>>> map = Maps.newHashMap();  
    map.put("主体", Lists.<List<String>>newArrayList(  
            Lists.<String>newArrayList("品牌", "苹果（Apple）"),  
            Lists.<String>newArrayList("型号", "iPhone 6 A1586"),  
            Lists.<String>newArrayList("颜色", "金色"),  
            Lists.<String>newArrayList("上市年份", "2014年")  
    ));  
    map.put("存储", Lists.<List<String>>newArrayList(  
            Lists.<String>newArrayList("机身内存", "16GB ROM"),  
            Lists.<String>newArrayList("储存卡类型", "不支持")  
    ));  
    map.put("显示", Lists.<List<String>>newArrayList(  
            Lists.<String>newArrayList("屏幕尺寸", "4.7英寸"),  
            Lists.<String>newArrayList("触摸屏", "Retina HD"),  
            Lists.<String>newArrayList("分辨率", "1334 x 750")  
    ));  
    return map;  
}   
本例基本信息提供了如商品名称、图片列表、颜色尺码、扩展属性、规格参数等等数据；而为了简化逻辑大多数数据都是 List/Map 数据结构。

商品介绍服务

Java 代码

private String getDescInfo(String skuId) throws Exception {  
    Map<String, Object> map = new HashMap<String, Object>();  
    map.put("content", "<div><img data-lazyload='http://img30.360buyimg.com/jgsq-productsoa/jfs/t448/127/574781110/103911/b3c80634/5472ba22N45400f4e.jpg' alt='' /><img data-lazyload='http://img30.360buyimg.com/jgsq-productsoa/jfs/t802/133/19465528/162152/e463e43/54e2b34aN11bceb70.jpg' alt='' height='386' width='750' /></div>");  
    map.put("date", System.currentTimeMillis());  
    String content = objectMapper.writeValueAsString(map);  
    //实际应用应该是发送MQ  
    asyncSetToRedis(descInfoJedisPool, "d:" + skuId + ":", content);  
    return objectMapper.writeValueAsString(map);  
}  
其他信息服务

Java 代码

private String getOtherInfo(String ps3Id, String brandId) throws Exception {  
    Map<String, Object> map = new HashMap<String, Object>();  
    //面包屑  
    List<List<?>> breadcrumb = Lists.newArrayList();  
    breadcrumb.add(Lists.newArrayList(9987, "手机"));  
    breadcrumb.add(Lists.newArrayList(653, "手机通讯"));  
    breadcrumb.add(Lists.newArrayList(655, "手机"));  
    //品牌  
    Map<String, Object> brand = Maps.newHashMap();  
    brand.put("name", "苹果（Apple）");  
    brand.put("logo", "BrandLogo/g14/M09/09/10/rBEhVlK6vdkIAAAAAAAFLXzp-lIAAHWawP_QjwAAAVF472.png");  
    map.put("breadcrumb", breadcrumb);  
    map.put("brand", brand);  
    //实际应用应该是发送MQ  
    asyncSetToRedis(otherInfoJedisPool, "s:" + ps3Id + ":", objectMapper.writeValueAsString(breadcrumb));  
    asyncSetToRedis(otherInfoJedisPool, "b:" + brandId + ":", objectMapper.writeValueAsString(brand));  
    return objectMapper.writeValueAsString(map);  
}    
本例中其他信息只使用了面包屑和品牌数据。

辅助工具

Java 代码

private ObjectMapper objectMapper = new ObjectMapper();  
private JedisPool basicInfoJedisPool = createJedisPool("127.0.0.1", 1111);  
private JedisPool descInfoJedisPool = createJedisPool("127.0.0.1", 1113);  
private JedisPool otherInfoJedisPool = createJedisPool("127.0.0.1", 1115);  

private JedisPool createJedisPool(String host, int port) {  
    GenericObjectPoolConfig poolConfig = new GenericObjectPoolConfig();  
    poolConfig.setMaxTotal(100);  
    return new JedisPool(poolConfig, host, port);  
}  

private ExecutorService executorService = Executors.newFixedThreadPool(10);  
private void asyncSetToRedis(final JedisPool jedisPool, final String key, final String content) {  
    executorService.submit(new Runnable() {  
        @Override  
        public void run() {  
            Jedis jedis = null;  
            try {  
                jedis = jedisPool.getResource();  
                jedis.set(key, content);  
            } catch (Exception e) {  
                e.printStackTrace();  
                jedisPool.returnBrokenResource(jedis);  
            } finally {  
                jedisPool.returnResource(jedis);  
            }  

        }  
    });  
}  
本例使用 Jackson 进行 JSON 的序列化；Jedis 进行 Redis 的操作；使用线程池做异步更新（实际应用中可以使用 MQ 做实现）。

web.xml 配置

Java 代码

<servlet>  
    <servlet-name>productServiceServlet</servlet-name>  
    <servlet-class>com.github.zhangkaitao.chapter7.servlet.ProductServiceServlet</servlet-class>  
</servlet>  
<servlet-mapping>  
    <servlet-name>productServiceServlet</servlet-name>  
    <url-pattern>/info</url-pattern>  
</servlet-mapping>  
打 WAR 包

Java 代码

cd D:\workspace\chapter7  
mvn clean package   
此处使用 maven 命令打包，比如本例将得到 chapter7.war，然后将其上传到服务器的 /usr/chapter7/webapp，然后通过 unzip chapter6.war 解压。

配置 Tomcat

复制第六章使用的 tomcat 实例：

Java 代码

cd /usr/servers/  
cp -r tomcat-server1 tomcat-chapter7/  
vim /usr/servers/tomcat-chapter7/conf/Catalina/localhost/ROOT.xml   
Java 代码

<!-- 访问路径是根，web应用所属目录为/usr/chapter7/webapp -->  
<Context path="" docBase="/usr/chapter7/webapp"></Context>  
指向第七章的 web 应用路径。

测试

启动 tomcat 实例。

Java 代码

/usr/servers/tomcat-chapter7/bin/startup.sh  
访问如下 URL 进行测试。

Java 代码

http://192.168.1.2:8080/info?type=basic&skuId=1  
http://192.168.1.2:8080/info?type=desc&skuId=1  
http://192.168.1.2:8080/info?type=other&ps3Id=1&brandId=1  
nginx 配置

vim /usr/chapter7/nginx_chapter7.conf

Java 代码

upstream backend {  
    server 127.0.0.1:8080 max_fails=5 fail_timeout=10s weight=1;  
    check interval=3000 rise=1 fall=2 timeout=5000 type=tcp default_down=false;  
    keepalive 100;  
}  

server {  
    listen       80;  
    server_name  item2015.jd.com item.jd.com d.3.cn;  

    location ~ /backend/(.*) {  
        #internal;  
        keepalive_timeout   30s;  
        keepalive_requests  1000;  
        #支持keep-alive  
        proxy_http_version 1.1;  
        proxy_set_header Connection "";  

        rewrite /backend(/.*) $1 break;  
        proxy_pass_request_headers off;  
        #more_clear_input_headers Accept-Encoding;  
        proxy_next_upstream error timeout;  
        proxy_pass http://backend;  
    }  
}    
此处 server_name 我们指定了 item.jd.com (商品详情页)和 d.3.cn (商品介绍)。其他配置可以参考第六章内容。另外实际生产环境要把 #internal 打开，表示只有本 nginx 能访问。

vim /usr/servers/nginx/conf/nginx.conf

Java 代码

include /usr/chapter7/nginx_chapter7.conf;  
\#为了方便测试，注释掉example.conf  
include /usr/chapter6/nginx_chapter6.conf;  
Java 代码

\#lua模块路径，其中”;;”表示默认搜索路径，默认到/usr/servers/nginx下找  
lua_package_path "/usr/chapter7/lualib/?.lua;;";  #lua 模块  
lua_package_cpath "/usr/chapter7/lualib/?.so;;";  #c模块  
lua模块从/usr/chapter7目录加载，因为我们要写自己的模块使用。
重启 nginx

/usr/servers/nginx/sbin/nginx -s reload

绑定 hosts

192.168.1.2 item.jd.com 192.168.1.2 item2015.jd.com 192.168.1.2 d.3.cn

访问如 http://item.jd.com/backend/info?type=basic&skuId=1 即看到结果。

前端展示实现
我们分为三部分实现：基础组件、商品介绍、前端展示部分。

基础组件

首先我们进行基础组件的实现，商品介绍和前端展示部分都需要读取 Redis 和 Http 服务，因此我们可以抽取公共部分出来复用。

vim /usr/chapter7/lualib/item/common.lua

Java 代码

local redis = require("resty.redis")  
local ngx_log = ngx.log  
local ngx_ERR = ngx.ERR  
local function close_redis(red)  
    if not red then  
        return  
    end  
    --释放连接(连接池实现)  
    local pool_max_idle_time = 10000 --毫秒  
    local pool_size = 100 --连接池大小  
    local ok, err = red:set_keepalive(pool_max_idle_time, pool_size)  

    if not ok then  
        ngx_log(ngx_ERR, "set redis keepalive error : ", err)  
    end  
end  

local function read_redis(ip, port, keys)  
    local red = redis:new()  
    red:set_timeout(1000)  
    local ok, err = red:connect(ip, port)  
    if not ok then  
        ngx_log(ngx_ERR, "connect to redis error : ", err)  
        return close_redis(red)  
    end  
    local resp = nil  
    if #keys == 1 then  
        resp, err = red:get(keys[1])  
    else  
        resp, err = red:mget(keys)  
    end  
    if not resp then  
        ngx_log(ngx_ERR, "get redis content error : ", err)  
        return close_redis(red)  
    end  

    --得到的数据为空处理  
    if resp == ngx.null then  
        resp = nil  
    end  
    close_redis(red)  

    return resp  
end  

local function read_http(args)  
    local resp = ngx.location.capture("/backend/info", {  
        method = ngx.HTTP_GET,  
        args = args  
    })  

    if not resp then  
        ngx_log(ngx_ERR, "request error")  
        return  
    end  
    if resp.status ~= 200 then  
        ngx_log(ngx_ERR, "request error, status :", resp.status)  
        return  
    end  
    return resp.body  
end  

local _M = {  
    read_redis = read_redis,  
    read_http = read_http  
}  
return _M   
整个逻辑和第六章类似；只是 read_redis 根据参数 keys 个数支持 get 和 mget。 比如read_redis(ip, port, {"key1"}) 则调用 get 而 read_redis(ip, port, {"key1", "key2"}) 则调用 mget。

商品介绍
核心代码

vim /usr/chapter7/desc.lua

Java 代码

local common = require("item.common")  
local read_redis = common.read_redis  
local read_http = common.read_http  
local ngx_log = ngx.log  
local ngx_ERR = ngx.ERR  
local ngx_exit = ngx.exit  
local ngx_print = ngx.print  
local ngx_re_match = ngx.re.match  
local ngx_var = ngx.var  

local descKey = "d:" .. skuId .. ":"  
local descInfoStr = read_redis("127.0.0.1", 1114, {descKey})  
if not descInfoStr then  
   ngx_log(ngx_ERR, "redis not found desc info, back to http, skuId : ", skuId)  
   descInfoStr = read_http({type="desc", skuId = skuId})  
end  
if not descInfoStr then  
   ngx_log(ngx_ERR, "http not found basic info, skuId : ", skuId)  
   return ngx_exit(404)  
end  
ngx_print("showdesc(")  
ngx_print(descInfoStr)  
ngx_print(")")    
通过复用逻辑后整体代码简化了许多；此处读取商品介绍从集群；另外前端展示使用 JSONP 技术展示商品介绍。

nginx 配置

vim /usr/chapter7/nginx_chapter7.conf

Java 代码

location ~^/desc/(\d+)$ {  
    if ($host != "d.3.cn") {  
       return 403;  
    }  
    default_type application/x-javascript;  
    charset utf-8;  
    lua_code_cache on;  
    set $skuId $1;  
    content_by_lua_file /usr/chapter7/desc.lua;  
}    
因为 item.jd.com 和 d.3.cn 复用了同一个配置文件，此处需要限定只有 d.3.cn 域名能访问，防止恶意访问。

重启 nginx 后，访问如 http://d.3.cn/desc/1 即可得到 JSONP 结果。

前端展示
核心代码

vim /usr/chapter7/item.lua

Java 代码

local common = require("item.common")  
local item = require("item")  
local read_redis = common.read_redis  
local read_http = common.read_http  
local cjson = require("cjson")  
local cjson_decode = cjson.decode  
local ngx_log = ngx.log  
local ngx_ERR = ngx.ERR  
local ngx_exit = ngx.exit  
local ngx_print = ngx.print  
local ngx_var = ngx.var  

local skuId = ngx_var.skuId  

--获取基本信息  
local basicInfoKey = "p:" .. skuId .. ":"  
local basicInfoStr = read_redis("127.0.0.1", 1112, {basicInfoKey})  
if not basicInfoStr then  
   ngx_log(ngx_ERR, "redis not found basic info, back to http, skuId : ", skuId)  
   basicInfoStr = read_http({type="basic", skuId = skuId})  
end  
if not basicInfoStr then  
   ngx_log(ngx_ERR, "http not found basic info, skuId : ", skuId)  
   return ngx_exit(404)  
end  

local basicInfo = cjson_decode(basicInfoStr)  
local ps3Id = basicInfo["ps3Id"]  
local brandId = basicInfo["brandId"]  
--获取其他信息  
local breadcrumbKey = "s:" .. ps3Id .. ":"  
local brandKey = "b:" .. brandId ..":"  
local otherInfo = read_redis("127.0.0.1", 1116, {breadcrumbKey, brandKey}) or {}  
local breadcrumbStr = otherInfo[1]  
local brandStr = otherInfo[2]  
if breadcrumbStr then  
   basicInfo["breadcrumb"] = cjson_decode(breadcrumbStr)  
end  
if brandStr then  
   basicInfo["brand"] = cjson_decode(brandStr)  
end  
if not breadcrumbStr and not brandStr then  
   ngx_log(ngx_ERR, "redis not found other info, back to http, skuId : ", brandId)  
   local otherInfoStr = read_http({type="other", ps3Id = ps3Id, brandId = brandId})  
   if not otherInfoStr then  
       ngx_log(ngx_ERR, "http not found other info, skuId : ", skuId)  
   else  
     local otherInfo = cjson_decode(otherInfoStr)  
     basicInfo["breadcrumb"] = otherInfo["breadcrumb"]  
     basicInfo["brand"] = otherInfo["brand"]  
   end  
end  

local name = basicInfo["name"]  
--name to unicode  
basicInfo["unicodeName"] = item.utf8_to_unicode(name)  
--字符串截取，超长显示...  
basicInfo["moreName"] = item.trunc(name, 10)  
--初始化各分类的url  
item.init_breadcrumb(basicInfo)  
--初始化扩展属性  
item.init_expand(basicInfo)  
--初始化颜色尺码  
item.init_color_size(basicInfo)  
local template = require "resty.template"  
template.caching(true)  
template.render("item.html", basicInfo)  
整个逻辑分为四部分：1、获取基本信息；2、根据基本信息中的关联关系获取其他信息；3、初始化/格式化数据；4、渲染模板。

初始化模块

vim /usr/chapter7/lualib/item.lua

Java 代码

local bit = require("bit")  
local utf8 = require("utf8")  
local cjson = require("cjson")  
local cjson_encode = cjson.encode  
local bit_band = bit.band  
local bit_bor = bit.bor  
local bit_lshift = bit.lshift  
local string_format = string.format  
local string_byte = string.byte  
local table_concat = table.concat  

--utf8转为unicode  
local function utf8_to_unicode(str)  
    if not str or str == "" or str == ngx.null then  
        return nil  
    end  
    local res, seq, val = {}, 0, nil  
    for i = 1, #str do  
        local c = string_byte(str, i)  
        if seq == 0 then  
            if val then  
                res[#res + 1] = string_format("%04x", val)  
            end  

           seq = c < 0x80 and 1 or c < 0xE0 and 2 or c < 0xF0 and 3 or  
                              c < 0xF8 and 4 or --c < 0xFC and 5 or c < 0xFE and 6 or  
                              0  
            if seq == 0 then  
                ngx.log(ngx.ERR, 'invalid UTF-8 character sequence' .. ",,," .. tostring(str))  
                return str  
            end  

            val = bit_band(c, 2 ^ (8 - seq) - 1)  
        else  
            val = bit_bor(bit_lshift(val, 6), bit_band(c, 0x3F))  
        end  
        seq = seq - 1  
    end  
    if val then  
        res[#res + 1] = string_format("%04x", val)  
    end  
    if #res == 0 then  
        return str  
    end  
    return "\\u" .. table_concat(res, "\\u")  
end  

--utf8字符串截取  
local function trunc(str, len)  
   if not str then  
     return nil  
   end  

   if utf8.len(str) > len then  
      return utf8.sub(str, 1, len) .. "..."  
   end  
   return str  
end  

--初始化面包屑  
local function init_breadcrumb(info)  
    local breadcrumb = info["breadcrumb"]  
    if not breadcrumb then  
       return  
    end  

    local ps1Id = breadcrumb[1][1]  
    local ps2Id = breadcrumb[2][1]  
    local ps3Id = breadcrumb[3][1]  

    --此处应该根据一级分类查找url  
    local ps1Url = "http://shouji.jd.com/"  
    local ps2Url = "http://channel.jd.com/shouji.html"  
    local ps3Url = "http://list.jd.com/list.html?cat=" .. ps1Id .. "," .. ps2Id .. "," .. ps3Id  

    breadcrumb[1][3] = ps1Url  
    breadcrumb[2][3] = ps2Url  
    breadcrumb[3][3] = ps3Url  
end  

--初始化扩展属性  
local function init_expand(info)  
   local expands = info["expands"]  
   if not expands then  
     return  
   end  
   for _, e in ipairs(expands) do  
      if type(e[2]) == "table" then  
         e[2] = table_concat(e[2], "，")  
      end  
   end  
end  

--初始化颜色尺码  
local function init_color_size(info)  
   local colorSize = info["colorSize"]  

   --颜色尺码JSON串  
   local colorSizeJson = cjson_encode(colorSize)  
   --颜色列表（不重复）  
   local colorList = {}  
   --尺码列表（不重复）  
   local sizeList = {}  
   info["colorSizeJson"] = colorSizeJson  
   info["colorList"] = colorList  
   info["sizeList"] = sizeList  

   local colorSet = {}  
   local sizeSet = {}  
   for _, cz in ipairs(colorSize) do  
      local color = cz["Color"]  
      local size = cz["Size"]  
      if color and color ~= "" and not colorSet[color] then  
         colorList[#colorList + 1] = {color = color, url = "http://item.jd.com/" ..cz["SkuId"] .. ".html"}  
         colorSet[color] = true  
      end  
      if size and size ~= "" and not sizeSet[size] then  
         sizeList[#sizeList + 1] = {size = size, url = "http://item.jd.com/" ..cz["SkuId"] .. ".html"}  
         sizeSet[size] = ""  
      end  
   end  
end  

local _M = {  
   utf8_to_unicode = utf8_to_unicode,  
   trunc = trunc,  
   init_breadcrumb = init_breadcrumb,  
   init_expand = init_expand,  
   init_color_size = init_color_size  
}  

return _M    
比如utf8_to_unicode 代码之前已经见过了，其他的都是一些逻辑代码。

模板 html 片段

Java 代码

var pageConfig = {  
     compatible: true,  
     product: {  
         skuid: {* skuId *},  
         name: '{* unicodeName *}',  
         skuidkey:'AFC266E971535B664FC926D34E91C879',  
         href: 'http://item.jd.com/{* skuId *}.html',  
         src: '{* imgs[1] *}',  
         cat: [{* ps1Id *},{* ps2Id *},{* ps3Id *}],  
         brand: {* brandId *},  
         tips: false,  
         pType: 1,  
         venderId:0,  
         shopId:'0',  
         specialAttrs:["HYKHSP-0","isDistribution","isHaveYB","isSelfService-0","isWeChatStock-0","packType","IsNewGoods","isCanUseDQ","isSupportCard","isCanUseJQ","isOverseaPurchase-0","is7ToReturn-1","isCanVAT"],  
         videoPath:'',  
         desc: 'http://d.3.cn/desc/{* skuId *}'  
     }  
 };  
 var warestatus = 1;                  
 {% if colorSizeJson then %} var ColorSize = {* colorSizeJson *};{% end %}  
         {-raw-}  
         try{(function(flag){ if(!flag){return;} if(window.location.hash == '#m'){var exp = new Date();exp.setTime(exp.getTime() + 30 * 24 * 60 * 60 * 1000);document.cookie = "pcm=1;expires=" + exp.toGMTString() + ";path=/;domain=jd.com";return;}else{var cook=document.cookie.match(new RegExp("(^| )pcm=([^;]*)(;|$)"));if(cook&&cook.length>2&&unescape(cook[2])=="2"){flag=false;}} var userAgent = navigator.userAgent; if(userAgent){ userAgent = userAgent.toUpperCase();if(userAgent.indexOf("PAD")>-1){return;} var mobilePhoneList = ["IOS","IPHONE","ANDROID","WINDOWS PHONE"];for(var i=0,len=mobilePhoneList.length;i<len;i++){ if(userAgent.indexOf(mobilePhoneList[i])>-1){var url="http://m.jd.com/product/"+pageConfig.product.skuid+".html";if(flag){window.showtouchurl=true;}else{window.location.href = url;}break;}}}})((function(){var json={"6881":3,"1195":3,"10011":3,"6980":3,"12360":3};if(json[pageConfig.product.cat[0]+""]==1||json[pageConfig.product.cat[1]+""]==2||json[pageConfig.product.cat[2]+""]==3){return false;}else{return true;}})());}catch(e){}  
         {-raw-}  
{* var *}输出变量，{% code %} 写代码片段，{-raw-} 不进行任何处理直接输出。
面包屑

Java 代码

<div class="breadcrumb">  
    <strong><a href='{* breadcrumb[1][3] *}'>{* breadcrumb[1][2] *}</a></strong>  
    <span>  
        &nbsp;&gt;&nbsp;  
        <a href='{* breadcrumb[2][3] *}'>{* breadcrumb[2][2] *}</a>  
        &nbsp;&gt;&nbsp;  
        <a href='{* breadcrumb[3][3] *}'>{* breadcrumb[3][2] *}</a>  
        &nbsp;&gt;&nbsp;  
    </span>  
    <span>  
        {% if brand then %}  
        <a href='http://www.jd.com/pinpai/{* ps3Id *}-{* brandId *}.html'>{* brand['name'] *}</a>  
        &nbsp;&gt;&nbsp;  
       {% end %}  
       <a href='http://item.jd.com/{* skuId *}.html'>{* moreName *}</a>  
    </span>  
</div>  
图片列表

Java 代码

<div id="spec-n1" class="jqzoom" onclick="window.open('http://www.jd.com/bigimage.aspx?id={* skuId *}')" clstag="shangpin|keycount|product|spec-n1">  
    <img data-img="1" width="350" height="350" src="http://img14.360buyimg.com/n1/{* imgs[1] *}" alt="{* name *}"/>  
</div>  
<div id="spec-list" clstag="shangpin|keycount|product|spec-n5">  
    <a href="javascript:;" class="spec-control" id="spec-forward"></a>  
    <a href="javascript:;" class="spec-control" id="spec-backward"></a>  
    <div class="spec-items">  
        <ul class="lh">  
            {% for _, img in ipairs(imgs) do %}  
            <li><img class='img-hover' alt='{* name *}' src='http://img14.360buyimg.com/n5/{* img *}' data-url='{* img *}' data-img='1' width='50' height='50'></li>  
            {% end %}  
        </ul>  
    </div>  
</div>  
颜色尺码选择

Java 代码

<div class="dt">选择颜色：</div>  
    <div class="dd">  
        {% for _, color in ipairs(colorList) do %}  
            <div class="item"><b></b><a href="{* color['url'] *}" title="{* color['color'] *}"><i>{* color['color'] *}</i></a></div>  
        {% end %}  
    </div>  
</div>  
<div id="choose-version" class="li">  
    <div class="dt">选择版本：</div>  
    <div class="dd">  
        {% for _, size in ipairs(sizeList) do %}  
            <div class="item"><b></b><a href="{* size['url'] *}" title="{* size['size'] *}">{* size['size'] *}</a></div>  
        {% end %}  
    </div>  
</div>  
扩展属性

Java 代码

<ul id="parameter2" class="p-parameter-list">  
    <li title='{* name *}'>商品名称：{* name *}</li>  
    <li title='{* skuId *}'>商品编号：{* skuId *}</li>  
    {% if brand then %}  
    <li title='{* brand["name"] *}'>品牌： <a href='http://www.jd.com/pinpai/{* ps3Id *}-{* brandId *}.html' target='_blank'>{* brand["name"] *}</a></li>  
    {% end %}  
    {% if date then %}  
    <li title='{* date *}'>上架时间：{* date *}</li>  
    {% end %}  
    {% if weight then %}  
    <li title='{* weight *}'>商品毛重：{* weight *}</li>  
    {% end %}  
    {% for _, e in pairs(expands) do %}  
    <li title='{* e[2] *}'>{* e[1] *}：{* e[2] *}</li>  
    {% end %}  
</ul>  
规格参数

Java 代码

<table cellpadding="0" cellspacing="1" width="100%" border="0" class="Ptable">  
    {% for group, pc in pairs(propCodes) do  %}  
    <tr><th class="tdTitle" colspan="2">{* group *}</th><tr>  
    {% for _, v in pairs(pc) do %}  
    <tr><td class="tdTitle">{* v[1] *}</td><td>{* v[2] *}</td></tr>  
    {% end %}  
    {% end %}  
</table>  
nginx 配置

vim /usr/chapter7/nginx_chapter7.conf

Java 代码

\#模板加载位置  
set $template_root "/usr/chapter7";  

location ~ ^/(\d+).html$ {  
    if ($host !~ "^(item|item2015)\.jd\.com$") {  
       return 403;  
    }  
    default_type 'text/html';  
    charset utf-8;  
    lua_code_cache on;  
    set $skuId $1;  
    content_by_lua_file /usr/chapter7/item.lua;  
}  
测试

重启 nginx，访问 http://item.jd.com/1217499.html 可得到响应内容，本例和京东的商品详情页的数据是有些出入的，输出的页面可能是缺少一些数据的。

优化

local cache

对于其他信息，对数据一致性要求不敏感，而且数据量很少，完全可以在本地缓存全量；而且可以设置如5-10分钟的过期时间是完全可以接受的；因此可以 lua_shared_dict 全局内存进行缓存。具体逻辑可以参考

Java 代码

local nginx_shared = ngx.shared  
--item.jd.com配置的缓存  
local local_cache = nginx_shared.item_local_cache  
local function cache_get(key)  
    if not local_cache then  
        return nil  
    end  
    return local_cache:get(key)  
end  

local function cache_set(key, value)  
    if not local_cache then  
        return nil  
    end  
    return local_cache:set(key, value, 10 * 60) --10分钟  
end  

local function get(ip, port, keys)  
    local tables = {}  
    local fetchKeys = {}  
    local resp = nil  
    local status = STATUS_OK  
    --如果tables是个map #tables拿不到长度  
    local has_value = false  
    --先读取本地缓存  
    for i, key in ipairs(keys) do  
        local value = cache_get(key)  
        if value then  
            if value == "" then  
                value = nil  
            end  
            tables[key] = value  
            has_value = true  
        else  
            fetchKeys[#fetchKeys + 1] = key  
        end  
    end  

    --如果还有数据没获取 从redis获取  
    if #fetchKeys > 0 then  
        if #fetchKeys == 1 then  
            status, resp = redis_get(ip, port, fetchKeys[1])  
        else  
            status, resp = redis_mget(ip, port, fetchKeys)  
        end  
        if status == STATUS_OK then  
            for i = 1, #fetchKeys do  
                 local key = fetchKeys[i]  
                 local value = nil  
                 if #fetchKeys == 1 then  
                    value = resp  
                 else  
                    value = get_data(resp, i)  
                 end  
                 tables[key] = value  
                  has_value = true  
                  cache_set(key, value or "", ttl)  
            end  
        end  
    end  
    --如果从缓存查到 就认为ok  
    if has_value and status == STATUS_NOT_FOUND then  
        status = STATUS_OK  
    end  
    return status, tables  
end  
nginx proxy cache

为了防止恶意刷页面/热点页面访问频繁，我们可以使用 nginx proxy_cache 做页面缓存，当然更好的选择是使用 CDN 技术，如通过 Apache Traffic Server、Squid、Varnish。

nginx.conf 配置

Java 代码

proxy_buffering on;  
proxy_buffer_size 8k;  
proxy_buffers 256 8k;  
proxy_busy_buffers_size 64k;  
proxy_temp_file_write_size 64k;  
proxy_temp_path /usr/servers/nginx/proxy_temp;  
\#设置Web缓存区名称为cache_one，内存缓存空间大小为200MB，1分钟没有被访问的内容自动清除，硬盘缓存空间大小为30GB。  
proxy_cache_path  /usr/servers/nginx/proxy_cache levels=1:2 keys_zone=cache_item:200m inactive=1m max_size=30g;    
增加 proxy_cache 的配置，可以通过挂载一块内存作为缓存的存储空间。更多配置规则请参考http://nginx.org/cn/docs/http/ngx_http_proxy_module.html。

nginx_chapter7.conf 配置

与 server 指令配置同级

Java 代码

\############ 测试时使用的动态请求  
map $host $item_dynamic {  
    default                    "0";  
    item2015.jd.com            "1";  
}    
即如果域名为 item2015.jd.com则item_dynamic=1。

Java 代码

location ~ ^/(\d+).html$ {  
    set $skuId $1;  
    if ($host !~ "^(item|item2015)\.jd\.com$") {  
       return 403;  
    }  

    expires 3m;  
    proxy_cache cache_item;  
    proxy_cache_key $uri;  
    proxy_cache_bypass $item_dynamic;  
    proxy_no_cache $item_dynamic;  
    proxy_cache_valid 200 301 3m;  
    proxy_cache_use_stale updating error timeout invalid_header http_500 http_502 http_503 http_504;  
    proxy_pass_request_headers off;  
    proxy_set_header Host $host;  
    #支持keep-alive  
    proxy_http_version 1.1;  
    proxy_set_header Connection "";  
    proxy_pass http://127.0.0.1/proxy/$skuId.html;  
    add_header X-Cache '$upstream_cache_status';  
}  

location ~ ^/proxy/(\d+).html$ {  
    allow 127.0.0.1;  
    deny all;  
    keepalive_timeout   30s;  
    keepalive_requests  1000;  
    default_type 'text/html';  
    charset utf-8;  
    lua_code_cache on;  
    set $skuId $1;  
    content_by_lua_file /usr/chapter7/item.lua;  
}  
expires：设置响应缓存头信息，此处是3分钟；将会得到 Cache-Control:max-age=180 和类似 Expires:Sat, 28 Feb 2015 10:01:10 GMT 的响应头；
proxy_cache：使用之前在 nginx.conf 中配置的 cache_item 缓存；
proxy_cache_key：缓存 key 为 uri，不包括 host 和参数，这样不管用户怎么通过在 url 上加随机数都是走缓存的；
proxy_cache_bypass：nginx 不从缓存取响应的条件，可以写多个；如果存在一个字符串条件且不是 “0”，那么 nginx 就不会从缓存中取响应内容；此处如果我们使用的 host 为item2015.jd.com 时就不会从缓存取响应内容；
proxy_no_cache：nginx 不将响应内容写入缓存的条件，可以写多个；如果存在一个字符串条件且不是 “0”，那么 nginx 就不会从将响应内容写入缓存；此处如果我们使用的 host 为item2015.jd.com 时就不会将响应内容写入缓存；
proxy_cache_valid：为不同的响应状态码设置不同的缓存时间，此处我们对 200、301 缓存3分钟；
proxy_cache_use_stale：什么情况下使用不新鲜（过期）的缓存内容；配置和proxy_next_upstream 内容类似；此处配置了如果连接出错、超时、404、500 等都会使用不新鲜的缓存内容；此外我们配置了 updating 配置，通过配置它可以在 nginx 正在更新缓存（其中一个 Worker 进程）时（其他的 Worker 进程）使用不新鲜的缓存进行响应，这样可以减少回源的数量；
proxy_pass_request_headers：我们不需要请求头，所以不传递； proxy_http_version 1.1 和 proxy_set_header Connection ""：支持keepalive； add_header X-Cache '$upstream_cache_status'：添加是否缓存命中的响应头；比如命中 HIT、不命中 MISS、不走缓存 BYPASS；比如命中会看到 X-Cache：HIT响应头；
allow/deny：允许和拒绝访问的 ip 列表，此处我们只允许本机访问； keepalive_timeout 30s 和 keepalive_requests 1000：支持 keepalive；
nginx_chapter7.conf 清理缓存配置

Java 代码

location /purge {  
    allow     127.0.0.1;  
    allow     192.168.0.0/16;  
    deny      all;  
    proxy_cache_purge  cache_item $arg_url;  
}   
只允许内网访问。访问如 http://item.jd.com/purge?url=/11.html；如果看到Successful purge 说明缓存存在并清理了。

修改 item.lua 代码

Java 代码

--添加Last-Modified，用于响应304缓存  
ngx.header["Last-Modified"] = ngx.http_time(ngx.now())  

local template = require "resty.template"  
template.caching(true)  
template.render("item.html", basicInfo)  
~     
在渲染模板前设置 Last-Modified，用于判断内容是否变更的条件，默认 Nginx 通过等于去比较，也可以通过配置 if_modified_since 指令来支持小于等于比较；如果请求头发送的 If-Modified-Since 和 Last-Modified 匹配则返回 304 响应，即内容没有变更，使用本地缓存。此处可能看到了我们的 Last-Modified 是当前时间，不是商品信息变更的时间；商品信息变更时间由：商品信息变更时间、面包屑变更时间和品牌变更时间三者决定的，因此实际应用时应该取三者最大的；还一个问题就是模板内容可能变了，但是商品信息没有变，此时使用 Last-Modified 得到的内容可能是错误的，所以可以通过使用 ETag 技术来解决这个问题，ETag 可以认为是内容的一个摘要，内容变更后摘要就变了。

GZIP 压缩

修改 nginx.conf 配置文件

Java 代码

gzip on;  
gzip_min_length  4k;  
gzip_buffers     4 16k;  
gzip_http_version 1.0;  
gzip_proxied        any;  #前端是squid的情况下要加此参数，否则squid上不缓存gzip文件  
gzip_comp_level 2;  
gzip_types       text/plain application/x-javascript text/css application/xml;  
gzip_vary on;   
此处我们指定至少 4k 时才压缩，如果数据太小压缩没有意义。

到此整个商品详情页逻辑就介绍完了，一些细节和运维内容需要在实际开发中实际处理，无法做到面面俱到。




流量复制 /AB 测试/协程

流量复制
在实际开发中经常涉及到项目的升级，而该升级不能简单的上线就完事了，需要验证该升级是否兼容老的上线，因此可能需要并行运行两个项目一段时间进行数据比对和校验，待没问题后再进行上线。这其实就需要进行流量复制，把流量复制到其他服务器上，一种方式是使用如 tcpcopy 引流；另外我们还可以使用 nginx 的 HttpLuaModule 模块中的 ngx.location.capture_multi 进行并发执行来模拟复制。

构造两个服务

Java 代码

location /test1 {  
    keepalive_timeout 60s;   
    keepalive_requests 1000;  
    content_by_lua '  
        ngx.print("test1 : ", ngx.req.get_uri_args()["a"])  
        ngx.log(ngx.ERR, "request test1")  
    ';  
}  
location /test2 {  
    keepalive_timeout 60s;   
    keepalive_requests 1000;  
    content_by_lua '  
        ngx.print("test2 : ", ngx.req.get_uri_args()["a"])  
        ngx.log(ngx.ERR, "request test2")  
    ';  
}  
通过 ngx.location.capture_multi 调用

Java 代码

location /test {  
     lua_socket_connect_timeout 3s;  
     lua_socket_send_timeout 3s;  
     lua_socket_read_timeout 3s;  
     lua_socket_pool_size 100;  
     lua_socket_keepalive_timeout 60s;  
     lua_socket_buffer_size 8k;  

     content_by_lua '  
         local res1, res2 = ngx.location.capture_multi{  
               { "/test1", { args = ngx.req.get_uri_args() } },  
               { "/test2", { args = ngx.req.get_uri_args()} },  
         }  
         if res1.status == ngx.HTTP_OK then  
             ngx.print(res1.body)  
         end  
         if res2.status ~= ngx.HTTP_OK then  
            --记录错误  
         end  
     ';  
}      
此处可以根据需求设置相应的超时时间和长连接连接池等；ngx.location.capture 底层通过cosocket 实现，而其支持 Lua 中的协程，通过它可以以同步的方式写非阻塞的代码实现。

此处要考虑记录失败的情况，对失败的数据进行重放还是放弃根据自己业务做处理。

AB 测试
AB 测试即多版本测试，有时候我们开发了新版本需要灰度测试，即让一部分人看到新版，一部分人看到老版，然后通过访问数据决定是否切换到新版。比如可以通过根据区域、用户等信息进行切版本。

比如京东商城有一个 cookie 叫做__jda，该 cookie 是在用户访问网站时种下的，因此我们可以拿到这个 cookie，根据这个 cookie 进行版本选择。

比如两次清空 cookie 访问发现第二个数字串是变化的，即我们可以根据第二个数字串进行判断。
__jda=122270672.1059377902.1425691107.1425691107.1425699059.1 __jda=122270672.556927616.1425699216.1425699216.1425699216.1。

判断规则可以比较多的选择，比如通过尾号；要切 30% 的流量到新版，可以通过选择尾号为 1，3,5 的切到新版，其余的还停留在老版。

使用 map 选择版本

Java 代码

map $cookie___jda $ab_key {  
    default                                       "0";  
    ~^\d+\.\d+(?P<k>(1|3|5))\.                    "1";  
}  
使用 map 映射规则，即如果是到新版则等于 "1"，到老版等于 “0”； 然后我们就可以通过 ngx.var.ab_key 获取到该数据。

Java 代码

location /abtest1 {  
    if ($ab_key = "1") {  
        echo_location /test1 ngx.var.args;  
    }  
    if ($ab_key = "0") {  
        echo_location /test2 ngx.var.args;  
    }  
}    
此处也可以使用 proxy_pass 到不同版本的服务器上

Java 代码

location /abtest2 {  
    if ($ab_key = "1") {  
        rewrite ^ /test1 break;  
        proxy_pass http://backend1;  
    }  
    rewrite ^ /test2 break;  
    proxy_pass http://backend2;  
}  
直接在 Lua 中使用 lua-resty-cookie 获取该 Cookie 进行解析

首先下载 lua-resty-cookie

Java 代码

cd /usr/example/lualib/resty/  
wget https://raw.githubusercontent.com/cloudflare/lua-resty-cookie/master/lib/resty/cookie.lua  
Java 代码

location /abtest3 {  
    content_by_lua '  

         local ck = require("resty.cookie")  
         local cookie = ck:new()  
         local ab_key = "0"  
         local jda = cookie:get("__jda")  
         if jda then  
             local v = ngx.re.match(jda, [[^\d+\.\d+(1|3|5)\.]])  
             if v then  
                ab_key = "1"  
             end  
         end  

         if ab_key == "1" then  
             ngx.exec("/test1", ngx.var.args)  
         else  
             ngx.print(ngx.location.capture("/test2", {args = ngx.req.get_uri_args()}).body)  
         end  
    ';  

}    
首先使用 lua-resty-cookie 获取 cookie，然后使用 ngx.re.match 进行规则的匹配，最后使用 ngx.exec 或者 ngx.location.capture 进行处理。此处同时使用 ngx.exec 和ngx.location.capture 目的是为了演示，此外没有对 ngx.location.capture 进行异常处理。

协程
Lua 中没有线程和异步编程编程的概念，对于并发执行提供了协程的概念，个人认为协程是在A运行中发现自己忙则把 CPU 使用权让出来给B使用，最后 A 能从中断位置继续执行，本地还是单线程，CPU 独占的；因此如果写网络程序需要配合非阻塞 I/O 来实现。

ngx_lua 模块对协程做了封装，我们可以直接调用 ngx.thread API 使用，虽然称其为“轻量级线程”，但其本质还是 Lua 协程。 该 API 必须配合该 ngx_lua 模块提供的非阻塞 I/O API 一起使用，比如我们之前使用的 ngx.location.capture_multi 和 lua-resty-redis、lua-resty-mysql 等基于 cosocket 实现的都是支持的。

通过 Lua 协程我们可以并发的调用多个接口，然后谁先执行成功谁先返回，类似于 BigPipe 模型。

依赖的 API

Java 代码

location /api1 {  
    echo_sleep 3;  
    echo api1 : $arg_a;  
}  
location /api2 {  
    echo_sleep 3;  
    echo api2 : $arg_a;  
}   
我们使用 echo_sleep 等待 3 秒。

串行实现

Java 代码

location /serial {  
    content_by_lua '  
        local t1 = ngx.now()  
        local res1 = ngx.location.capture("/api1", {args = ngx.req.get_uri_args()})  
        local res2 = ngx.location.capture("/api2", {args = ngx.req.get_uri_args()})  
        local t2 = ngx.now()  
        ngx.print(res1.body, "<br/>", res2.body, "<br/>", tostring(t2-t1))  
    ';  
}  
即一个个的调用，总的执行时间在6秒以上，比如访问 http://192.168.1.2/serial?a=22

Java 代码

api1 : 22   
api2 : 22   
6.0040001869202  
ngx.location.capture_multi 实现

Java 代码

location /concurrency1 {  
    content_by_lua '  
        local t1 = ngx.now()  
        local res1,res2 = ngx.location.capture_multi({  
              {"/api1", {args = ngx.req.get_uri_args()}},  
              {"/api2", {args = ngx.req.get_uri_args()}}  

        })  
        local t2 = ngx.now()  
        ngx.print(res1.body, "<br/>", res2.body, "<br/>", tostring(t2-t1))  
    ';  
}    
直接使用 ngx.location.capture_multi 来实现，比如访问 http://192.168.1.2/concurrency1?a=22

Java 代码

api1 : 22   
api2 : 22   
3.0020000934601  
协程 API 实现

Java 代码

location /concurrency2 {  
    content_by_lua '  
        local t1 = ngx.now()  
        local function capture(uri, args)  
           return ngx.location.capture(uri, args)  
        end  
        local thread1 = ngx.thread.spawn(capture, "/api1", {args = ngx.req.get_uri_args()})  
        local thread2 = ngx.thread.spawn(capture, "/api2", {args = ngx.req.get_uri_args()})  
        local ok1, res1 = ngx.thread.wait(thread1)  
        local ok2, res2 = ngx.thread.wait(thread2)  
        local t2 = ngx.now()  
        ngx.print(res1.body, "<br/>", res2.body, "<br/>", tostring(t2-t1))  
    ';  
}    
使用 ngx.thread.spawn 创建一个轻量级线程，然后使用 ngx.thread.wait 等待该线程的执行成功。比如访问 http://192.168.1.2/concurrency2?a=22

Java 代码

api1 : 22   
api2 : 22   
3.0030000209808  
其有点类似于 Java 中的线程池执行模型，但不同于线程池，其每次只执行一个函数，遇到 IO 等待则让出 CPU 让下一个执行。我们可以通过下面的方式实现任意一个成功即返回，之前的是等待所有执行成功才返回。

Java 代码

local  ok, res = ngx.thread.wait(thread1, thread2)  






Nginx 新手起步

为什选择 Nginx

为什么选择 Nginx？因为它具有以下特点：

1、处理响应请求很快

在正常的情况下，单次请求会得到更快的响应。在高峰期，Nginx 可以比其它的 Web 服务器更快的响应请求。

2、高并发连接

在互联网快速发展，互联网用户数量不断增加的今天，一些大公司、网站都需要面对高并发请求，如果有一个能够在峰值顶住 10 万以上并发请求的 Server，肯定会得到大家的青睐。理论上，Nginx 支持的并发连接上限取决于你的内存，10 万远未封顶。

3、低的内存消耗

在一般的情况下，10000 个非活跃的 HTTP Keep-Alive 连接在 Nginx 中仅消耗 2.5MB 的内存，这也是 Nginx 支持高并发连接的基础。

4、具有很高的可靠性：

Nginx 是一个高可靠性的 Web 服务器，这也是我们为什么选择 Nginx 的基本条件，现在很多的网站都在使用 Nginx，足以说明 Nginx 的可靠性。高可靠性来自其核心框架代码的优秀设计、模块设计的简单性，并且这些模块都非常的稳定。

5、高扩展性

Nginx 的设计极具扩展性，它完全是由多个不同功能、不同层次、不同类型且耦合度极低的模块组成。这种设计造就了 Nginx 庞大的第三方模块。

6、热部署

master 管理进程与 worker 工作进程的分离设计，使得 Nginx 具有热部署的功能，可以在 7×24 小时不间断服务的前提下，升级 Nginx 的可执行文件。也可以在不停止服务的情况下修改配置文件，更换日志文件等功能。

7、自由的 BSD 许可协议

BSD 许可协议不只是允许用户免费使用 Nginx，也允许用户修改 Nginx 源码，还允许用户用于商业用途。

如何使用 Nginx

Nginx 安装：

不同系统依赖包可能不同，例如 pcre，zlib，openssl 等。

获取 Nginx，在 http://nginx.org/en/download.html 上可以获取当前最新的版本。
解压缩 nginx-xx.tar.gz 包。
进入解压缩目录，执行 ./configure
make & make install
若安装时找不到上述依赖模块，使用 --with-openssl=<openssl_dir>、--with-pcre=<pcre_dir>、--with-zlib=<zlib_dir> 指定依赖的模块目录。如已安装过，此处的路径为安装目录；若未安装，则此路径为编译安装包路径，nginx 将执行模块的默认编译安装。

启动 nginx 之后，浏览器中输入 http://localhost 可以验证是否安装启动成功。



Nginx 配置示例:

安装完成之后，配置目录 conf 下有以下配置文件，过滤掉了 xx.default 配置：

ubuntu: /opt/nginx-1.7.7/conf$ tree |grep -v default
.
├── fastcgi.conf
├── fastcgi_params
├── koi-utf
├── koi-win
├── mime.types
├── nginx.conf
├── scgi_params
├── uwsgi_params
└── win-utf
除了 nginx.conf，其余配置文件，一般只需要使用默认提供即可。

nginx.conf是主配置文件，默认配置去掉注释之后的内容如下图所示：

worker_process      # 表示工作进程的数量，一般设置为cpu的核数

worker_connections  # 表示每个工作进程的最大连接数

server{}            # 块定义了虚拟主机

    listen          # 监听端口

    server_name     # 监听域名

    location {}     # 是用来为匹配的 URI 进行配置，URI 即语法中的“/uri/”

    location /{}    # 匹配任何查询，因为所有请求都以 / 开头

        root        # 指定对应uri的资源查找路径，这里html为相对路径，完整路径为
                    # /opt/nginx-1.7.7/html/

        index       # 指定首页index文件的名称，可以配置多个，以空格分开。如有多
                    # 个，按配置顺序查找。
真实用例


从配置可以看出，nginx 监听了 80 端口、域名为 localhost、根路径为 html 文件夹（我的安装路径为 /opt/nginx-1.7.7，所以 /opt/nginx-1.7.7/html）、默认 index 文件为 index.html，index.htm 服务器错误重定向到 50x.html 页面。

可以看到 /opt/nginx-1.7.7/html/ 有以下文件：

ubuntu:/opt/nginx-1.7.7/html$ ls
50x.html  index.html
这也是上面在浏览器中输入 http://localhost，能够显示欢迎页面的原因。实际上访问的是 /opt/nginx-1.7.7/html/index.html 文件。





location 匹配规则

语法规则

location [=|~|~*|^~] /uri/ { … }
符号  含义
=   开头表示精确匹配
^~  开头表示 uri 以某个常规字符串开头，理解为匹配 url 路径即可。nginx 不对 url 做编码，因此请求为/static/20%/aa，可以被规则^~ /static/ /aa匹配到（注意是空格）
~   开头表示区分大小写的正则匹配
~*  开头表示不区分大小写的正则匹配
/   通用匹配，任何请求都会匹配到
多个 location 配置的情况下匹配顺序为（参考资料而来，还未实际验证，试试就知道了，不必拘泥，仅供参考）:

首先匹配 =
其次匹配 ^~
其次是按文件中顺序的正则匹配
最后是交给 / 通用匹配
当有匹配成功时候，停止匹配，按当前匹配规则处理请求
例子，有如下匹配规则：

location = / {
   #规则A
}
location = /login {
   #规则B
}
location ^~ /static/ {
   #规则C
}
location ~ \.(gif|jpg|png|js|css)$ {
   #规则D
}
location ~* \.png$ {
   #规则E
}
location / {
   #规则F
}
那么产生的效果如下：

访问根目录 /， 比如 http://localhost/ 将匹配规则 A
访问 http://localhost/login 将匹配规则 B，http://localhost/register 则匹配规则 F
访问 http://localhost/static/a.html 将匹配规则 C
访问 http://localhost/a.gif, http://localhost/b.jpg 将匹配规则 D 和规则 E，但是规则 D 顺序优先，规则 E 不起作用，而 http://localhost/static/c.png 则优先匹配到规则 C
访问 http://localhost/a.PNG 则匹配规则 E，而不会匹配规则 D，因为规则 E 不区分大小写。
访问 http://localhost/category/id/1111 则最终匹配到规则 F，因为以上规则都不匹配，这个时候应该是 nginx 转发请求给后端应用服务器，比如 FastCGI（php），tomcat（jsp），nginx 作为反向代理服务器存在。

所以实际使用中，笔者觉得至少有三个匹配规则定义，如下：

# 直接匹配网站根，通过域名访问网站首页比较频繁，使用这个会加速处理，官网如是说。
# 这里是直接转发给后端应用服务器了，也可以是一个静态首页
# 第一个必选规则
location = / {
    proxy_pass http://tomcat:8080/index
}

# 第二个必选规则是处理静态文件请求，这是 nginx 作为 http 服务器的强项
# 有两种配置模式，目录匹配或后缀匹配，任选其一或搭配使用
location ^~ /static/ {
    root /webroot/static/;
}
location ~* \.(gif|jpg|jpeg|png|css|js|ico)$ {
    root /webroot/res/;
}

# 第三个规则就是通用规则，用来转发动态请求到后端应用服务器
# 非静态文件请求就默认是动态请求，自己根据实际把握
# 毕竟目前的一些框架的流行，带.php、.jsp后缀的情况很少了
location / {
    proxy_pass http://tomcat:8080/
}
ReWrite语法

last – 基本上都用这个 Flag
break – 中止 Rewirte，不在继续匹配
redirect – 返回临时重定向的 HTTP 状态 302
permanent – 返回永久重定向的 HTTP 状态 301
1、下面是可以用来判断的表达式：

-f 和 !-f 用来判断是否存在文件
-d 和 !-d 用来判断是否存在目录
-e 和 !-e 用来判断是否存在文件或目录
-x 和 !-x 用来判断文件是否可执行
2、下面是可以用作判断的全局变量

例：http://localhost:88/test1/test2/test.php?k=v
$host：localhost
$server_port：88
$request_uri：/test1/test2/test.php?k=v
$document_uri：/test1/test2/test.php
$document_root：D:\nginx/html
$request_filename：D:\nginx/html/test1/test2/test.php
Redirect 语法

server {
    listen 80;
    server_name start.igrow.cn;
    index index.html index.php;
    root html;
    if ($http_host !~ “^star\.igrow\.cn$") {
        rewrite ^(.*) http://star.igrow.cn$1 redirect;
    }
}
防盗链

location ~* \.(gif|jpg|swf)$ {
    valid_referers none blocked start.igrow.cn sta.igrow.cn;
    if ($invalid_referer) {
       rewrite ^/ http://$host/logo.png;
    }
}
根据文件类型设置过期时间

location ~* \.(js|css|jpg|jpeg|gif|png|swf)$ {
    if (-f $request_filename) {
        expires 1h;
        break;
    }
}
禁止访问某个目录

location ~* \.(txt|doc)${
    root /data/www/wwwroot/linuxtone/test;
    deny all;
}
一些可用的全局变量，可以参考获取 Nginx 内置绑定变量章节。






if 是邪恶的

当在 location 区块中使用 if 指令的时候会有一些问题, 在某些情况下它并不按照你的预期运行而是做一些完全不同的事情。 而在另一些情况下他甚至会出现段错误。 一般来说避免使用 if 指令是个好主意。

在 location 区块里 if 指令下唯一 100% 安全的指令应该只有:

return …; rewrite … last;
除此以外的指令都可能导致不可预期的行为, 包括诡异的发出段错误信号 (SIGSEGV)。

要着重注意的是 if 的行为不是反复无常的, 给出两个条件完全一致的请求, Nginx 并不会出现一个正常工作而一个请求失败的随机情况, 在明晰的测试和理解下 if 是完全可用的。尽管如此, 在这里还是建议使用其他指令。

总有一些情况你无法避免去使用 if 指令, 比如你需要测试一个变量, 而它没有相应的配置指令。

if ($request_method = POST) {
    return 405;
}
if ($args ~ post=140){
    rewrite ^ http://example.com/ permanent;
}
如何替换掉 if

使用 try_files 如果他适合你的需求。 在其他的情况下使用 return … 或者 rewrite … last。 还有一些情况可能要把 if 移动到 server 区块下(只有当其他的 rewrite 模块指令也允许放在的地方才是安全的)。

如下可以安全地改变用于处理请求的 location。

location / {
    error_page 418 = @other;
    recursive_error_pages on;

    if ($something) {
        return 418;
    }

    # some configuration
    # ...
}

location @other {
    # some other configuration
    # ...
}
在某些情况下使用嵌入脚本模块(嵌入 perl 或者其他一些第三方模块)处理这些脚本更佳。

以下是一些例子用来解释为什么 if 是邪恶的。 非专业人士, 请勿模仿!

# 这里收集了一些出人意料的坑爹配置来展示 location 中的 if 指令是万恶的

# 只有第二个 header 才会在响应中展示
# 这不是 Bug, 只是他的处理流程如此

location /only-one-if {
    set $true 1;

    if ($true) {
        add_header X-First 1;
    }

    if ($true) {
        add_header X-Second 2;
    }

    return 204;
}

# 因为 if, 请求会在未改变 uri 的情况下下发送到后台的 '/'

location /proxy-pass-uri {
    proxy_pass http://127.0.0.1:8080/;

    set $true 1;

    if ($true) {
        # nothing
    }
}

# 因为if, try_files 失效

location /if-try-files {
    try_files  /file  @fallback;

    set $true 1;

    if ($true) {
        # nothing
    }
}

# nginx 将会发出段错误信号(SIGSEGV)

location /crash {

    set $true 1;

    if ($true) {
        # fastcgi_pass here
        fastcgi_pass  127.0.0.1:9000;
    }

    if ($true) {
        # no handler here
    }
}

# alias with captures isn't correcly inherited into implicit nested location created by if
# alias with captures 不能正确的继承到由if创建的隐式嵌入的location

location ~* ^/if-and-alias/(?<file>.*) {
    alias /tmp/$file;

    set $true 1;

    if ($true) {
        # nothing
    }
}
为什么会这样且到现在都没修复这些问题?

if 指令是 rewrite 模块中的一部分, 是实时生效的指令。另一方面来说, nginx 配置大体上是陈述式的。在某些时候用户出于特殊是需求的尝试, 会在if里写入一些非rewrite指令, 这直接导致了我们现处的情况。 大多数情况下他可以工作, 但是…看看上面。 看起来唯一正确的修复方式是完全禁用if中的非rewrite指令。 但是这将破坏很多现有可用的配置, 所以还没有修复。

如果你还是想知道该如何使用 if

如果你看完了上面所有内容还是想使用 if，请确认你确实理解了该怎么用它。一些比较基本的用法可以在这里找到。

做适当的测试

我已经警告过你了!

文章选自：http://xwsoul.com/posts/761 TODO:这个文章后面需要自己翻译，可能有版权问题：https://www.nginx.com/resources/wiki/start/topics/depth/ifisevil/




Nginx 静态文件服务

我们先来看看最简单的本地静态文件服务配置示例：

server {
        listen       80;
        server_name www.test.com;
        charset utf-8;
        root   /data/www.test.com;
        index  index.html index.htm;
       }
就这些？恩，就这些！如果只是提供简单的对外静态文件，它真的就可以用了。可是他不完美，远远没有发挥 Nginx 的半成功力，为什么这么说呢，看看下面的配置吧，为了大家看着方便，我们把每一项的作用都做了注释。

http {
    # 这个将为打开文件指定缓存，默认是没有启用的，max 指定缓存数量，
    # 建议和打开文件数一致，inactive 是指经过多长时间文件没被请求后删除缓存。
    open_file_cache max=204800 inactive=20s;

    # open_file_cache 指令中的inactive 参数时间内文件的最少使用次数，
    # 如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个
    # 文件在inactive 时间内一次没被使用，它将被移除。
    open_file_cache_min_uses 1;

    # 这个是指多长时间检查一次缓存的有效信息
    open_file_cache_valid 30s;

    # 默认情况下，Nginx的gzip压缩是关闭的， gzip压缩功能就是可以让你节省不
    # 少带宽，但是会增加服务器CPU的开销哦，Nginx默认只对text/html进行压缩 ，
    # 如果要对html之外的内容进行压缩传输，我们需要手动来设置。
    gzip on;
    gzip_min_length 1k;
    gzip_buffers 4 16k;
    gzip_http_version 1.0;
    gzip_comp_level 2;
    gzip_types text/plain application/x-javascript text/css application/xml;

    server {
            listen       80;
            server_name www.test.com;
            charset utf-8;
            root   /data/www.test.com;
            index  index.html index.htm;
           }
}
我们都知道，应用程序和网站一样，其性能关乎生存。但如何使你的应用程序或者网站性能更好，并没有一个明确的答案。代码质量和架构是其中的一个原因，但是在很多例子中我们看到，你可以通过关注一些十分基础的应用内容分发技术（basic application delivery techniques），来提高终端用户的体验。其中一个例子就是实现和调整应用栈（application stack）的缓存。

文件缓存漫谈

一个 web 缓存坐落于客户端和原始服务器（origin server）中间，它保留了所有可见内容的拷贝。如果一个客户端请求的内容在缓存中存储，则可以直接在缓存中获得该内容而不需要与服务器通信。这样一来，由于web缓存距离客户端“更近”，就可以提高响应性能，并更有效率的使用应用服务器，因为服务器不用每次请求都进行页面生成工作。

在浏览器和应用服务器之间，存在多种潜在缓存，如：客户端浏览器缓存、中间缓存、内容分发网络（CDN）和服务器上的负载平衡和反向代理。缓存，仅在反向代理和负载均衡的层面，就对性能提高有很大的帮助。

举个例子说明，去年，我接手了一项任务，这项任务的内容是对一个加载缓慢的网站进行性能优化。首先引起我注意的事情是，这个网站差不多花费了超过1秒钟才生成了主页。经过一系列调试，我发现加载缓慢的原因在于页面被标记为不可缓存，即为了响应每一个请求，页面都是动态生成的。由于页面本身并不需要经常性的变更，并且不涉及个性化，那么这样做其实并没有必要。为了验证一下我的结论，我将页面标记为每5秒缓存一次，仅仅做了这一个调整，就能明显的感受到性能的提升。第一个字节到达的时间降低到几毫秒，同时页面的加载明显要更快。

并不是只有大规模的内容分发网络（CDN）可以在使用缓存中受益——缓存还可以提高负载平衡器、反向代理和应用服务器前端web服务的性能。通过上面的例子，我们看到，缓存内容结果，可以更高效的使用应用服务器，因为不需要每次都去做重复的页面生成工作。此外，Web缓存还可以用来提高网站可靠性。当服务器宕机或者繁忙时，比起返回错误信息给用户，不如通过配置Nginx将已经缓存下来的内容发送给用户。这意味着，网站在应用服务器或者数据库故障的情况下，可以保持部分甚至全部的功能运转。

下面讨论如何安装和配置 Nginx 的基础缓存（Basic Caching）。

如何安装和配置基础缓存

我们只需要两个命令就可以启用基础缓存： proxy_cache_path 和 proxy_cache 。proxy_cache_path 用来设置缓存的路径和配置，proxy_cache 用来启用缓存。

proxy_cache_path/path/to/cache levels=1:2 keys_zone=my_cache:10m max_size=10g inactive=60m
use_temp_path=off;

server {
    ...
    location / {
        proxy_cache my_cache;
        proxy_pass http://my_upstream;
    }

}
proxy_cache_path 命令中的参数及对应配置说明如下：

用于缓存的本地磁盘目录是 /path/to/cache/
levels 在 /path/to/cache/ 设置了一个两级层次结构的目录。将大量的文件放置在单个目录中会导致文件访问缓慢，所以针对大多数部署，我们推荐使用两级目录层次结构。如果 levels 参数没有配置，则 Nginx 会将所有的文件放到同一个目录中。
keys_zone 设置一个共享内存区，该内存区用于存储缓存键和元数据，有些类似计时器的用途。将键的拷贝放入内存可以使 Nginx 在不检索磁盘的情况下快速决定一个请求是HIT还是MISS，这样大大提高了检索速度。一个 1MB 的内存空间可以存储大约 8000个key，那么上面配置的 10MB 内存空间可以存储差不多 80000 个 key。
max_size 设置了缓存的上限（在上面的例子中是 10G）。这是一个可选项；如果不指定具体值，那就是允许缓存不断增长，占用所有可用的磁盘空间。当缓存达到这个上线，处理器便调用 cache manager 来移除最近最少被使用的文件，这样把缓存的空间降低至这个限制之下。
inactive 指定了项目在不被访问的情况下能够在内存中保持的时间。在上面的例子中，如果一个文件在 60 分钟之内没有被请求，则缓存管理将会自动将其在内存中删除，不管该文件是否过期。该参数默认值为 10 分钟（10m）。注意，非活动内容有别于过期内容。 Nginx 不会自动删除由缓存控制头部指定的过期内容（本例中 Cache-Control:max-age=120）。过期内容只有在 inactive 指定时间内没有被访问的情况下才会被删除。如果过期内容被访问了，那么 Nginx 就会将其从原服务器上刷新，并更新对应的inactive计时器。
Nginx 最初会将注定写入缓存的文件先放入一个临时存储区域，use_temp_path=off命令指示 Nginx 将在缓存这些文件时将它们写入同一个目录下。我们强烈建议你将参数设置为off来避免在文件系统中不必要的数据拷贝。use_temp_path在 Nginx 1.7版本和 Nginx Plus R6中有所介绍。
最终，proxy_cache 命令启动缓存那些URL与location部分匹配的内容（本例中，为/）。你同样可以将proxy_cache命令添加到server部分，这将会将缓存应用到所有的那些location中未指定自己的proxy_cache命令的服务中。

陈旧总比没有强

Nginx 内容缓存的一个非常强大的特性是：当无法从原始服务器获取最新的内容时， Nginx 可以分发缓存中的陈旧（stale，编者注：即过期内容）内容。这种情况一般发生在关联缓存内容的原始服务器宕机或者繁忙时。比起对客户端传达错误信息， Nginx 可发送在其内存中的陈旧的文件。 Nginx 的这种代理方式，为服务器提供额外级别的容错能力，并确保了在服务器故障或流量峰值的情况下的正常运行。为了开启该功能，只需要添加 proxy_cache_use_stale 命令即可：

location / {
    ...
    proxy_cache_use_stale error timeout http_500 http_502 http_503 http_504;
}
按照上面例子中的配置，当 Nginx 收到服务器返回的error，timeout或者其他指定的5xx错误，并且在其缓存中有请求文件的陈旧版本，则会将这些陈旧版本的文件而不是错误信息发送给客户端。

缓存微调

Nginx 提供了丰富的可选项配置用于缓存性能的微调。下面是使用了几个配置的例子：

proxy_cache_path /path/to/cache levels=1:2 keys_zone=my_cache:10m max_size=10g inactive=60m
use_temp_path=off;
server {
    ...
    location / {
        proxy_cache my_cache;
        proxy_cache_revalidate on;
        proxy_cache_min_uses 3;
        proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
        proxy_cache_lock on;
        proxy_pass http://my_upstream;
    }
}
这些命令配置了下列的行为：

proxy_cache_revalidate 指示 Nginx 在刷新来自服务器的内容时使用 GET 请求。如果客户端的请求项已经被缓存过了，但是在缓存控制头部中定义为过期，那么 Nginx 就会在 GET 请求中包含 If-Modified-Since 字段，发送至服务器端。这项配置可以节约带宽，因为对于 Nginx 已经缓存过的文件，服务器只会在该文件请求头中 Last-Modified 记录的时间内被修改时才将全部文件一起发送。
proxy_cache_min_uses 该指令设置同一链接请求达到几次即被缓存，默认值为 1 。当缓存不断被填满时，这项设置便十分有用，因为这确保了只有那些被经常访问的内容会被缓存。
proxy_cache_use_stale 中的 updating 参数告知 Nginx 在客户端请求的项目的更新正在原服务器中下载时发送旧内容，而不是向服务器转发重复的请求。第一个请求陈旧文件的用户不得不等待文件在原服务器中更新完毕。陈旧的文件会返回给随后的请求直到更新后的文件被全部下载。 4.当 proxy_cache_lock 被启用时，当多个客户端请求一个缓存中不存在的文件（或称之为一个 MISS），只有这些请求中的第一个被允许发送至服务器。其他请求在第一个请求得到满意结果之后在缓存中得到文件。如果不启用 proxy_cache_lock，则所有在缓存中找不到文件的请求都会直接与服务器通信。
跨多硬盘分割缓存

使用 Nginx 不需要建立一个RAID（磁盘阵列）。如果有多个硬盘， Nginx 可以用来在多个硬盘之间分割缓存。下面是一个基于请求 URI 跨越两个硬盘之间均分缓存的例子：

proxy_cache_path /path/to/hdd1 levels=1:2 keys_zone=my_cache_hdd1:10m max_size=10g

inactive=60m use_temp_path=off;
proxy_cache_path /path/to/hdd2 levels=1:2 keys_zone=my_cache_hdd2:10m max_size=10g inactive=60m use_temp_path=off;
split_clients $request_uri $my_cache {
    50% "my_cache_hdd1";
    50% "my_cache_hdd2";
}

server {
    ...
    location / {
        proxy_cache $my_cache;
        proxy_pass http://my_upstream;
    }
}





日志

Nginx 日志主要有两种：access_log(访问日志) 和 error_log(错误日志)。

access_log 访问日志

access_log 主要记录客户端访问 Nginx 的每一个请求，格式可以自定义。通过 access_log 你可以得到用户地域来源、跳转来源、使用终端、某个URL访问量等相关信息。

log_format 指令用于定义日志的格式，语法: log_format name string; 其中 name 表示格式名称，string 表示定义的格式字符串。log_format 有一个默认的无需设置的组合日志格式。

默认的无需设置的组合日志格式
log_format combined '$remote_addr - $remote_user  [$time_local]  '
                    ' "$request"  $status  $body_bytes_sent  '
                    ' "$http_referer"  "$http_user_agent" ';
access_log 指令用来指定访问日志文件的存放路径（包含日志文件名）、格式和缓存大小，语法：access_log path [format_name [buffer=size | off]]; 其中 path 表示访问日志存放路径，format_name 表示访问日志格式名称，buffer 表示缓存大小，off 表示关闭访问日志。

log_format 使用事例：在 access.log 中记录客户端 IP 地址、请求状态和请求时间
log_format myformat '$remote_addr  $status  $time_local';
access_log logs/access.log  myformat;
需要注意的是：log_format 配置必须放在 http 内，否则会出现警告。Nginx 进程设置的用户和组必须对日志路径有创建文件的权限，否则，会报错。

定义日志使用的字段及其作用：

字段  作用
$remote_addr与$http_x_forwarded_for  记录客户端IP地址
$remote_user    记录客户端用户名称
$request    记录请求的URI和HTTP协议
$status 记录请求状态
$body_bytes_sent    发送给客户端的字节数，不包括响应头的大小
$bytes_sent 发送给客户端的总字节数
$connection 连接的序列号
$connection_requests    当前通过一个连接获得的请求数量
$msec   日志写入时间。单位为秒，精度是毫秒
$pipe   如果请求是通过HTTP流水线(pipelined)发送，pipe值为“p”，否则为“.”
$http_referer   记录从哪个页面链接访问过来的
$http_user_agent    记录客户端浏览器相关信息
$request_length 请求的长度（包括请求行，请求头和请求正文）
$request_time   请求处理时间，单位为秒，精度毫秒
$time_iso8601   ISO8601标准格式下的本地时间
$time_local 记录访问时间与时区
error_log 错误日志
error_log 主要记录客户端访问 Nginx 出错时的日志，格式不支持自定义。通过查看错误日志，你可以得到系统某个服务或 server 的性能瓶颈等。因此，将日志利用好，你可以得到很多有价值的信息。

error_log 指令用来指定错误日志，语法: error_log path level; 其中 path 表示错误日志存放路径，level 表示错误日志等级，日志等级包括 debug、info、notice、warn、error、crit，从左至右，日志详细程度逐级递减，即 debug 最详细，crit 最少，默认为 crit。

注意：error_log off 并不能关闭错误日志记录，此时日志信息会被写入到文件名为 off 的文件当中。如果要关闭错误日志记录，可以使用如下配置：

Linux 系统把存储位置设置为空设备

error_log /dev/null;

http {
    # ...
}
Windows 系统把存储位置设置为空设备

error_log nul;

http {
    # ...
}
另外 Linux 系统可以使用 tail 命令方便的查阅正在改变的文件,tail -f filename会把 filename 里最尾部的内容显示在屏幕上,并且不断刷新,使你看到最新的文件内容。Windows 系统没有这个命令，你可以在网上找到动态查看文件的工具。





反向代理

什么是反向代理

反向代理（Reverse Proxy）方式是指用代理服务器来接受 internet 上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给 internet 上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。

举个例子，一个用户访问 http://www.example.com/readme，但是 www.example.com 上并不存在 readme 页面，它是偷偷从另外一台服务器上取回来，然后作为自己的内容返回给用户。但是用户并不知情这个过程。对用户来说，就像是直接从 www.example.com 获取 readme 页面一样。这里所提到的 www.example.com 这个域名对应的服务器就设置了反向代理功能。

反向代理服务器，对于客户端而言它就像是原始服务器，并且客户端不需要进行任何特别的设置。客户端向反向代理的命名空间(name-space)中的内容发送普通请求，接着反向代理将判断向何处(原始服务器)转交请求，并将获得的内容返回给客户端，就像这些内容原本就是它自己的一样。如下图所示：

proxy

反向代理典型应用场景

反向代理的典型用途是将防火墙后面的服务器提供给 Internet 用户访问，加强安全防护。反向代理还可以为后端的多台服务器提供负载均衡，或为后端较慢的服务器提供 缓冲 服务。另外，反向代理还可以启用高级 URL 策略和管理技术，从而使处于不同 web 服务器系统的 web 页面同时存在于同一个 URL 空间下。

Nginx 的其中一个用途是做 HTTP 反向代理，下面简单介绍 Nginx 作为反向代理服务器的方法。

场景描述：访问本地服务器上的 README.md 文件 http://localhost/README.md，本地服务器进行反向代理，从 https://github.com/moonbingbing/openresty-best-practices/blob/master/README.md 获取页面内容。
nginx.conf 配置示例：

worker_processes 1;

pid logs/nginx.pid;
error_log logs/error.log warn;

events {
    worker_connections 3000;
}

http {
    include mime.types;
    server_tokens off;

    ## 下面配置反向代理的参数
    server {
        listen    80;

        ## 1. 用户访问 http://ip:port，则反向代理到 https://github.com
        location / {
            proxy_pass  https://github.com;
            proxy_redirect     off;
            proxy_set_header   Host             $host;
            proxy_set_header   X-Real-IP        $remote_addr;
            proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;
        }

        ## 2.用户访问 http://ip:port/README.md，则反向代理到
        ##   https://github.com/.../README.md
        location /README.md {
            proxy_set_header  X-Real-IP  $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_pass https://github.com/moonbingbing/openresty-best-practices/blob/master/README.md;
        }
    }
}
成功启动 nginx 后，我们打开浏览器，验证下反向代理的效果。在浏览器地址栏中输入 localhost/README.md，返回的结果是我们 github 源代码的 README 页面。如下图：

proxy_example

我们只需要配置一下 nginx.conf 文件，不用写任何 web 页面，就可以偷偷地从别的服务器上读取一个页面返回给用户。

下面我们来看一下 nginx.conf 里用到的配置项：

（1）location

location 项对请求 URI 进行匹配，location 后面配置了匹配规则。例如上面的例子中，如果请求的 URI 是 localhost/，则会匹配 location / 这一项；如果请求的 URI 是 localhost/README.md，则会比配 location /README.md 这项。

上面这个例子只是针对一个确定的 URI 做了反向代理，有的读者会有疑惑：如果对每个页面都进行这样的配置，那将会大量重复，能否做 批量 配置呢？此时需要配合使用 location 的正则匹配功能。具体实现方法可参考本书的 URL 匹配章节。

（2）proxy_pass

proxy_pass 后面跟着一个 URL，用来将请求反向代理到 URL 参数指定的服务器上。例如我们上面例子中的 proxy_pass https://github.com，则将匹配的请求反向代理到 https://github.com。

（3）proxy_set_header

默认情况下，反向代理不会转发原始请求中的 Host 头部，如果需要转发，就需要加上这句：proxy_set_header Host $host;

除了上面提到的常用配置项，还有 proxy_redirect、proxy_set_body、proxy_limit_rate 等参数，具体用法可以到Nginx 官网查看。

正向代理

既然有反向代理，自然也有正向代理。简单来说，正向代理就像一个跳板，例如一个用户访问不了某网站（例如 www.google.com），但是他能访问一个代理服务器，这个代理服务器能访问 www.google.com，于是用户可以先连上代理服务器，告诉它需要访问的内容，代理服务器去取回来返回给用户。例如一些常见的翻墙工具、游戏代理就是利用正向代理的原理工作的，我们需要在这些正向代理工具上配置服务器的 IP 地址等信息。






负载均衡

负载均衡（Load balancing）是一种计算机网络技术，用来在多个计算机（计算机集群）、网络连接、CPU、磁盘驱动器或其他资源中分配负载，以达到最佳化资源使用、最大化吞吐率、最小化响应时间、同时避免过载的目的。

使用带有负载均衡的多个服务器组件，取代单一的组件，可以通过冗余提高可靠性。负载均衡服务通常是由专用软体和硬件来完成。

负载均衡最重要的一个应用是利用多台服务器提供单一服务，这种方案有时也称之为服务器农场。通常，负载均衡主要应用于Web网站，大型的 Internet Relay Chat 网络，高流量的文件下载网站，NNTP（Network News Transfer Protocol）服务和 DNS 服务。现在负载均衡器也开始支持数据库服务，称之为数据库负载均衡器。

对于互联网服务，负载均衡器通常是一个软体程序，这个程序侦听一个外部端口，互联网用户可以通过这个端口来访问服务，而作为负载均衡器的软体会将用户的请求转发给后台内网服务器，内网服务器将请求的响应返回给负载均衡器，负载均衡器再将响应发送到用户，这样就向互联网用户隐藏了内网结构，阻止了用户直接访问后台（内网）服务器，使得服务器更加安全，可以阻止对核心网络栈和运行在其它端口服务的攻击。

当所有后台服务器出现故障时，有些负载均衡器会提供一些特殊的功能来处理这种情况。例如转发请求到一个备用的负载均衡器、显示一条关于服务中断的消息等。负载均衡器使得IT团队可以显著提高容错能力。它可以自动提供大量的容量以处理任何应用程序流量的增加或减少。

负载均衡在互联网世界中的作用如此重要，本章我们一起了解一下 Nginx 是如何帮我们完成 HTTP 协议负载均衡的。

upstream 负载均衡概要

配置示例，如下：

upstream test.net{
    ip_hash;
    server 192.168.10.13:80;
    server 192.168.10.14:80  down;
    server 192.168.10.15:8009  max_fails=3  fail_timeout=20s;
    server 192.168.10.16:8080;
}
server {
    location / {
        proxy_pass  http://test.net;
    }
}
upstream 是 Nginx 的 HTTP Upstream 模块，这个模块通过一个简单的调度算法来实现客户端 IP 到后端服务器的负载均衡。在上面的设定中，通过 upstream 指令指定了一个负载均衡器的名称 test.net。这个名称可以任意指定，在后面需要用到的地方直接调用即可。

upstream 支持的负载均衡算法

Nginx 的负载均衡模块目前支持 6 种调度算法，下面进行分别介绍，其中后两项属于第三方调度算法。

轮询（默认）：每个请求按时间顺序逐一分配到不同的后端服务器，如果后端某台服务器宕机，故障系统被自动剔除，使用户访问不受影响。Weight 指定轮询权值，Weight 值越大，分配到的访问机率越高，主要用于后端每个服务器性能不均的情况下。
ip_hash：每个请求按访问IP的hash结果分配，这样来自同一个IP的访客固定访问一个后端服务器，有效解决了动态网页存在的session共享问题。
fair：这是比上面两个更加智能的负载均衡算法。此种算法可以依据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间来分配请求，响应时间短的优先分配。Nginx本身是不支持fair的，如果需要使用这种调度算法，必须下载Nginx的upstream_fair模块。
url_hash：此方法按访问 url 的 hash 结果来分配请求，使每个 url 定向到同一个后端服务器，可以进一步提高后端缓存服务器的效率。Nginx 本身是不支持 url_hash 的，如果需要使用这种调度算法，必须安装 Nginx 的 hash 软件包。
least_conn：最少连接负载均衡算法，简单来说就是每次选择的后端都是当前最少连接的一个 server(这个最少连接不是共享的，是每个 worker 都有自己的一个数组进行记录后端 server 的连接数)。
hash：这个 hash 模块又支持两种模式 hash, 一种是普通的 hash, 另一种是一致性 hash(consistent)。
upstream 支持的状态参数

在 HTTP Upstream 模块中，可以通过 server 指令指定后端服务器的 IP 地址和端口，同时还可以设定每个后端服务器在负载均衡调度中的状态。常用的状态有：

down：表示当前的server暂时不参与负载均衡。
backup：预留的备份机器。当其他所有的非 backup 机器出现故障或者忙的时候，才会请求 backup 机器，因此这台机器的压力最轻。
max_fails：允许请求失败的次数，默认为 1。当超过最大次数时，返回 proxy_next_upstream 模块定义的错误。
fail_timeout：在经历了 max_fails 次失败后，暂停服务的时间。max_fails 可以和 fail_timeout 一起使用。
当负载调度算法为 ip_hash 时，后端服务器在负载均衡调度中的状态不能是 backup。

配置 nginx 负载均衡

实验拓扑

Nginx 配置负载均衡
upstream webservers {
    server 192.168.18.201 weight=1;
    server 192.168.18.202 weight=1;
}
server {
    listen       80;
    server_name  localhost;
    #charset koi8-r;
    #access_log  logs/host.access.log  main;
    location / {
        proxy_pass      http://webservers;
        proxy_set_header  X-Real-IP  $remote_addr;
    }
}
注，upstream 是定义在 server{ } 之外的，不能定义在 server{ } 内部。定义好 upstream 之后，用 proxy_pass 引用一下即可。

重新加载一下配置文件
# service nginx reload
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
# curl http://192.168.18.208
web1.test.com
# curl http://192.168.18.208
web2.test.com
# curl http://192.168.18.208
web1.test.com
# curl http://192.168.18.208
web2.test.com
注，大家可以不断的刷新浏览的内容，可以发现 web1 与 web2 是交替出现的，达到了负载均衡的效果。

查看一下Web访问服务器日志
Web1:

# tail /var/log/nginx/access_log
192.168.18.138 - - [04/Sep/2013:09:41:58 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:41:58 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:41:59 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:41:59 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:42:00 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:42:00 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:42:00 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:44:21 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:44:22 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:44:22 +0800] "GET / HTTP/1.0" 200 23 "-"
Web2:

先修改一下，Web服务器记录日志的格式。

# LogFormat "%{X-Real-IP}i %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\"" combined
# tail /var/log/nginx/access_log
192.168.18.138 - - [04/Sep/2013:09:50:28 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:50:28 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:50:28 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:50:28 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:50:28 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:50:28 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:50:28 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:50:28 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:50:29 +0800] "GET / HTTP/1.0" 200 23 "-"
192.168.18.138 - - [04/Sep/2013:09:50:29 +0800] "GET / HTTP/1.0" 200 23 "-"
注，大家可以看到，两台服务器日志都记录是192.168.18.138访问的日志，也说明了负载均衡配置成功。

配置 nginx 进行健康状态检查

利用 max_fails、fail_timeout 参数，控制异常情况，示例配置如下：

upstream webservers {
    server 192.168.18.201 weight=1 max_fails=2 fail_timeout=2;
    server 192.168.18.202 weight=1 max_fails=2 fail_timeout=2;
}
重新加载一下配置文件:

# service nginx reload
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
重新载入 nginx：                                           [确定]
先停止Web1，进行测试：

# service nginx stop
停止 nginx：                                               [确定]
# curl http://192.168.18.208
web2.test.com
# curl http://192.168.18.208
web2.test.com
# curl http://192.168.18.208
web2.test.com
注，大家可以看到，现在只能访问Web2，再重新启动Web1，再次访问一下。

# service nginx start
正在启动 nginx：                                           [确定]
# curl http://192.168.18.208
web1.test.com
# curl http://192.168.18.208
web2.test.com
# curl http://192.168.18.208
web1.test.com
# curl http://192.168.18.208
web2.test.com
PS：大家可以看到，现在又可以重新访问，说明 nginx 的健康状态查检配置成功。但大家想一下，如果不幸的是所有服务器都不能提供服务了怎么办，用户打开页面就会出现出错页面，那么会带来用户体验的降低，所以我们能不能像配置 LVS 是配置 sorry_server 呢，答案是可以的，但这里不是配置 sorry_server 而是配置 backup。

配置 backup 服务器

备份服务器配置：
server {
    listen 8080;
    server_name localhost;
    root /data/www/errorpage;
    index index.html;
}
index.html 文件内容：
# cat index.html
<h1>Sorry......</h1>
负载均衡配置：
upstream webservers {
    server 192.168.18.201 weight=1 max_fails=2 fail_timeout=2;
    server 192.168.18.202 weight=1 max_fails=2 fail_timeout=2;
    server 127.0.0.1:8080 backup;
}
重新加载配置文件：

# service nginx reload
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
重新载入 nginx：                                           [确定]
关闭Web服务器并进行测试：

# service nginx stop
停止 nginx：                                               [确定]
进行测试：

# curl http://192.168.18.208
<h1>Sorry......</h1>
# curl http://192.168.18.208
<h1>Sorry......</h1>
# curl http://192.168.18.208
<h1>Sorry......</h1>
注，大家可以看到，当所有服务器都不能工作时，就会启动备份服务器。好了，backup服务器就配置到这里，下面我们来配置 ip_hash 负载均衡。

配置 ip_hash 负载均衡

ip_hash：每个请求按访问IP的hash结果分配，这样来自同一个IP的访客固定访问一个后端服务器，有效解决了动态网页存在的session共享问题，电子商务网站用的比较多。

# vim /etc/nginx/nginx.conf
upstream webservers {
    ip_hash;
    server 192.168.18.201 weight=1 max_fails=2 fail_timeout=2;
    server 192.168.18.202 weight=1 max_fails=2 fail_timeout=2;
    #server 127.0.0.1:8080 backup;
}
注，当负载调度算法为ip_hash时，后端服务器在负载均衡调度中的状态不能有 backup。有人可能会问，为什么呢？大家想啊，如果负载均衡把你分配到 backup 服务器上，你能访问到页面吗？不能，所以了不能配置 backup 服务器。

重新加载一下服务器：

# service nginx reload
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
重新载入 nginx：                                           [确定]
测试一下：

# curl http://192.168.18.208
web2.test.com
# curl http://192.168.18.208
web2.test.com
# curl http://192.168.18.208
web2.test.com
注，大家可以看到，你不断的刷新页面一直会显示 Web2，说明 ip_hash 负载均衡配置成功。






Nginx 陷阱和常见错误

翻译自：https://www.Nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/

警告：

请阅读下面所有的内容！是所有的！
不管是新手还是老用户，都可能会掉到一个陷阱中去。下面我们会列出一些我们经常看到，和 经常需要解释如何解决的问题。在 Freenode 的 Nginx IRC 频道中，我们频繁的看到这些问题出现。

本指南说

最经常看到的是，有人从一些其他的指南中，尝试拷贝、粘贴一个配置片段。并不是说其他所有的指南都是错的，但是里面错误的比例很可怕。即使是在 Linode 库中也有质量较差的信息，一些 Nginx 社区成员曾经徒劳的试图去纠正。

本指南的文档，是社区成员所创建和审查，他们直接和所有类型的 Nginx 用户在一起工作。 这个特定的文档之所以存在，是因为社区成员看到有大量普遍和重复出现的问题。

我的问题没有被列出来

在这里你没有看到和你具体问题相关的东西。也许我们并没有解决你经历的具体问题。 不要只是大概浏览下这个网页，也不要假设你是无意才找到这里的。你找到这里，是因为这里列出了你做错的一些东西。

在许多问题上，当涉及到支持很多用户，社区成员不希望去支持破碎的配置。所以在提问求助前，先修复你的配置。 通读这个文档来修复你的配置，不要只是走马观花。

chmod 777

永远不要 使用 777，这可能是一个漂亮的数字，有时候可以懒惰的解决权限问题， 但是它同样也表示你没有线索去解决权限问题，你只是在碰运气。 你应该检查整个路径的权限，并思考发生了什么事情。

要轻松的显示一个路径的所有权限，你可以使用：

 namei -om /path/to/check
把 root 放在 location 区块内

糟糕的配置：

server {
    server_name www.example.com;
    location / {
        root /var/www/Nginx -default/;
        # [...]
      }
    location /foo {
        root /var/www/Nginx -default/;
        # [...]
    }
    location /bar {
        root /var/www/Nginx -default/;
        # [...]
    }
}
这个是能工作的。把 root 放在 location 区块里面会工作，但并不是完全有效的。 错就错在只要你开始增加其他的 location 区块，就需要给每一个 location 区块增加一个 root 。 如果没有添加，就会没有 root 。让我们看下正确的配置。

推荐的配置：

server {
    server_name www.example.com;
    root /var/www/Nginx -default/;
    location / {
        # [...]
    }
    location /foo {
        # [...]
    }
    location /bar {
        # [...]
    }
}
重复的 index 指令

糟糕的配置：

http {
    index index.php index.htm index.html;
    server {
        server_name www.example.com;
        location / {
            index index.php index.htm index.html;
            # [...]
        }
    }
    server {
        server_name example.com;
        location / {
            index index.php index.htm index.html;
            # [...]
        }
        location /foo {
            index index.php;
            # [...]
        }
    }
}
为什么重复了这么多行不需要的配置呢？简单的使用“index”指令一次就够了。只需要把它放到 http {} 区块里面，下面的就会继承这个配置。

推荐的配置：

http {
    index index.php index.htm index.html;
    server {
        server_name www.example.com;
        location / {
            # [...]
        }
    }
    server {
        server_name example.com;
        location / {
            # [...]
        }
        location /foo {
            # [...]
        }
    }
}
使用 if

这里篇幅有限，只介绍一部分使用 if 指令的陷阱。更多陷阱你应该点击看看邪恶的 if 指令。 我们看下 if 指令的几个邪恶的用法。

注意看这里：

邪恶的 if 指令
用 if 判断 Server Name

糟糕的配置：

server {
    server_name example.com *.example.com;
        if ($host ~* ^www\.(.+)) {
            set $raw_domain $1;
            rewrite ^/(.*)$ $raw_domain/$1 permanent;
        }
        # [...]
    }
}
这个配置有三个问题。首先是 if 的使用, 为啥它这么糟糕呢? 你有阅读邪恶的 if 指令吗? 当 Nginx 收到无论来自哪个子域名的何种请求, 不管域名是 www.example.com 还是 example.com，这个 if 指令 总是 会被执行。 因此 Nginx 会检查 每个请求 的 Host header，这是十分低效的。 你应该避免这种情况，而是使用下面配置里面的两个 server 指令。

推荐的配置：

 server {
    server_name www.example.com;
    return 301 $scheme://example.com$request_uri;
}
server {
    server_name example.com;
    # [...]
}
除了增强了配置的可读性，这种方法还降低了 Nginx 的处理要求；我们摆脱了不必要的 if 指令； 我们用了 $scheme 来表示 URI 中是 http 还是 https 协议，避免了硬编码。

用 if 检查文件是否存在

使用 if 指令来判断文件是否存在是很可怕的，如果你在使用新版本的 Nginx ， 你应该看看 try_files，这会让你的生活变得更轻松。

糟糕的配置：

server {
    root /var/www/example.com;
    location / {
        if (!-f $request_filename) {
            break;
        }
    }
}
推荐的配置：

server {
    root /var/www/example.com;
    location / {
        try_files $uri $uri/ /index.html;
    }
}
我们不再尝试使用 if 来判断$uri是否存在，用 try_files 意味着你可以测试一个序列。 如果 $uri 不存在，就会尝试 $uri/ ，还不存在的话，在尝试一个回调 location 。

在上面配置的例子里面，如果 $uri 这个文件存在，就正常服务； 如果不存在就检测 $uri/ 这个目录是否存在；如果不存在就按照 index.html 来处理，你需要保证 index.html 是存在的。 try_files的加载是如此简单。这是另外一个你可以完全的消除 if 指令的实例。

前端控制器模式的web应用

“前端控制器模式”是流行的设计，被用在很多非常流行的 PHP 软件包里面。 里面的很多示例配置都过于复杂。想要 Drupal, Joomla 等运行起来，只用这样做就可以了：

try_files $uri $uri/ /index.php?q=$uri&$args;
注意：你实际使用的软件包，在参数名字上会有差异。比如：

"q"参数用在Drupal, Joomla, WordPress
"page"用在CMS Made Simple
一些软件甚至不需要查询字符串，它们可以从 REQUEST_URI 中读取。 比如 WordPress 就支持这样的配置：

try_files $uri $uri/ /index.php;
当然在你的开发中可能会有变化，你可能需要基于你的需要设置更复杂的配置。 但是对于一个基础的网站来说，这个配置可以工作得很完美。 你应该永远从简单开始来搭建你的系统。

如果你不关心目录是否存在这个检测的话，你也可以决定忽略这个目录的检测，去掉 “$uri/” 这个配置。

把不可控制的请求发给PHP

很多网络上面推荐的和 PHP 相关的 Nginx 配置，都是把每一个 .php 结尾的 URI 传递给 PHP 解释器。 请注意，大部分这样的 PHP 设置都有严重的安全问题，因为它可能允许执行任意第三方代码。

有问题的配置通常如下：

location ~* \.php$ {
    fastcgi_pass backend;
    # [...]
}
在这里，每一个.php结尾的请求，都会传递给 FastCGI 的后台处理程序。 这样做的问题是，当完整的路径未能指向文件系统里面一个确切的文件时， 默认的 PHP 配置试图是猜测你想执行的是哪个文件。

举个例子，如果一个请求中的 /forum/avatar/1232.jpg/file.php 文件不存在， 但是/forum/avatar/1232.jpg存在，那么PHP解释器就会取而代之， 使用/forum/avatar/1232.jpg来解释。如果这里面嵌入了 PHP 代码， 这段代码就会被执行起来。

有几个避免这种情况的选择：

在php.ini中设置cgi.fix_pathinfo=0。 这会让 PHP 解释器只尝试给定的文件路径，如果没有找到这个文件就停止处理。

确保 Nginx 只传递指定的 PHP 文件去执行
location ~* (file_a|file_b|file_c)\.php$ {
    fastcgi_pass backend;
    # [...]
}
对于任何用户可以上传的目录，特别的关闭 PHP 文件的执行权限
location /uploaddir {
    location ~ \.php$ {return 403;}
    # [...]
}
使用 try_files 指令过滤出文件不存在的情况
location ~* \.php$ {
    try_files $uri =404;
    fastcgi_pass backend;
    # [...]
}
使用嵌套的 location 过滤出文件不存在的情况
location ~* \.php$ {
    location ~ \..*/.*\.php$ {return 404;}
    fastcgi_pass backend;
    # [...]
}
脚本文件名里面的 FastCGI 路径

很多外部指南喜欢依赖绝对路径来获取你的信息。这在 PHP 的配置块里面很常见。 当你从仓库安装 Nginx ，通常都是以在配置里面折腾好“include fastcgi_params;”来收尾。 这个配置文件位于你的 Nginx 根目录下，通常在 /etc/Nginx/ 里面。

推荐的配置：

fastcgi_param  SCRIPT_FILENAME    $document_root$fastcgi_script_name;
糟糕的配置：

fastcgi_param  SCRIPT_FILENAME    /var/www/yoursite.com/$fastcgi_script_name;
$document_root$ 在哪里设置呢？它是 server 块里面的 root 指令来设置的。 你的 root 指令不在 server 块内？请看前面关于 root 指令的陷阱。

费力的 rewrites

不要知难而退， rewrite 很容易和正则表达式混为一谈。 实际上， rewrite 是很容易的，我们应该努力去保持它们的整洁。 很简单，不添加冗余代码就行了。

糟糕的配置：

rewrite ^/(.*)$ http://example.com/$1 permanent;
好点儿的配置：

rewrite ^ http://example.com$request_uri? permanent;
更好的配置：

return 301 http://example.com$request_uri;
反复对比下这几个配置。 第一个 rewrite 捕获不包含第一个斜杠的完整 URI 。 使用内置的变量 $request_uri ，我们可以有效的完全避免任何捕获和匹配。

忽略 http:// 的rewrite

这个非常简单， rewrites 是用相对路径的，除非你告诉 Nginx 不是相对路径。 生成绝对路径的 rewrite 也很简单，加上 scheme 就行了。

糟糕的配置：

rewrite ^ example.com permanent;
推荐的配置：

rewrite ^ http://example.com permanent;
你可以看到我们做的只是在 rewrite 里面增加了 http://。这个很简单而且有效。

代理所有东西

糟糕的配置：

server {
    server_name _;
    root /var/www/site;
    location / {
        include fastcgi_params;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        fastcgi_pass unix:/tmp/phpcgi.socket;
    }
}
这个是令人讨厌的配置，你把 所有东西 都丢给了 PHP 。 为什么呢？ Apache 可能要这样做，但在 Nginx 里你不必这样。 换个思路，try_files 有一个神奇之处，它是按照特定顺序去尝试文件的。 这意味着 Nginx 可以先尝试下静态文件，如果没有才继续往后走。 这样PHP就不用参与到这个处理中，会快很多。 特别是如果你提供一个1MB图片数千次请求的服务，通过PHP处理还是直接返回静态文件呢？ 让我们看下怎么做到吧。

推荐的配置：

server {
    server_name _;
    root /var/www/site;
    location / {
        try_files $uri $uri/ @proxy;
    }
    location @proxy {
        include fastcgi_params;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        fastcgi_pass unix:/tmp/phpcgi.socket;
    }
}
另外一个推荐的配置：

server {
    server_name _;
    root /var/www/site;
    location / {
        try_files $uri $uri/ /index.php;
    }
    location ~ \.php$ {
        include fastcgi_params;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        fastcgi_pass unix:/tmp/phpcgi.socket;
    }
}
这个很容易，不是吗？你看，如果请求的 URI 存在， Nginx 会处理掉； 如果不存在，检查下目录是不是存在，是的话也可以被 Nginx 处理； 只有在 Nginx 不能直接处理请求的URI的时候，才会进入 proxy 这个 location 来处理。

现在，考虑下你的请求中有多少静态内容，比如图片、css、javascript等。这可能会帮你节省很多开销。

配置的修改没有起效

浏览器缓存。你的配置可能是对的，但怎么尝试结果总是不对，百思不得其解。 罪魁祸首是你的浏览器缓存。当你下载东西的时候，浏览器做了缓存。

怎么修复：

在 Firefox 里面 Ctrl+Shift+Delete ，检查缓存，点击立即清理。可以用你喜欢的搜索引擎找到其他浏览器清理缓存的方法。 每次更改配置后，都需要清理下缓存（除非你知道这个不必要），这会省很多事儿。

使用 curl 。
VirtualBox

如果你在 VirtualBox 的虚拟机中运行 Nginx ，而它不工作，可能是因为 sendfile() 引起的麻烦。 只用简单的注释掉 sendfile 指令，或者设置为 off。 该指令大都会写在 Nginx .conf 文件中：

 sendfile off;
丢失（消失）的 HTTP 头

如果你没有明确的设置 underscores_in_headers on; , Nginx 将会自动丢弃带有下划线的 HTTP 头(根据 HTTP 标准，这样做是完全正当的). 这样做是为了防止头信息映射到 CGI 变量时产生歧义，因为破折号和下划线都会被映射为下划线。

没有使用标准的 Document Root Location

在所有的文件系统中，一些目录永远也不应该被用做数据的托管。这些目录包括 / 和 /root 。 你永远不应该使用这些目录作为你的 document root。

使用这些目录的话，等于打开了潘多拉魔盒，请求会超出你的预期获取到隐私的数据。

永远也不要这样做！！！ ( 对，我们还是要看下飞蛾扑火的配置长什么样子)

server {
    root /;

    location / {
        try_files /web/$uri $uri @php;
    }

    location @php {
        [...]
    }
}
当一个对 /foo 的请求，会传递给 PHP 处理，因为文件没有找到。 这可能没有问题，直到遇到 /etc/passwd 这个请求。没错，你刚才给了我们这台服务器的所有用户列表。 在某些情况下， Nginx 的 workers 甚至是 root 用户运行的。那么，我们现在有你的用户列表， 以及密码哈希值，我们也知道哈希的方法。这台服务器已经变成我们的肉鸡了。

Filesystem Hierarchy Standard (FHS) 定义了数据应该如何存在。你一定要去阅读下。 简单点儿说，你应该把 web 的内容 放在 /var/www/ , /srv 或者 /usr/share/www 里面。

使用默认的 Document Root

在 Ubuntu、 Debian 等操作系统中， Nginx 会被封装成一个易于安装的包， 里面通常会提供一个“默认”的配置文件作为范例，也通常包含一个 document root 来保存基础的 HTML 文件。

大部分这些打包系统，并没有检查默认的 document root 里面的文件是否修改或者存在。 在包升级的时候，可能会导致代码失效。有经验的系统管理员都知道，不要假设默认的 document root 里面的数据在升级的时候会原封不动。

你不应该使用默认的 document root 做网站的任何关键文件的目录。 并没有默认的 document root 目录会保持不变这样的约定，你网站的关键数据， 很可能在更新和升级系统提供的 Nginx 包时丢失。

使用主机名来解析地址

糟糕的配置：

upstream {
    server http://someserver;
}

server {
    listen myhostname:80;
    # [...]
}
你不应该在 listen 指令里面使用主机名。 虽然这样可能是有效的，但它会带来层出不穷的问题。 其中一个问题是，这个主机名在启动时或者服务重启中不能解析。 这会导致 Nginx 不能绑定所需的 TCP socket 而启动失败。

一个更安全的做法是使用主机名对应 IP 地址，而不是主机名。 这可以防止 Nginx 去查找 IP 地址，也去掉了去内部、外部解析程序的依赖。

例子中的 upstream location 也有同样的问题，虽然有时候在 upstream 里面不可避免要使用到主机名， 但这是一个不好的实践，需要仔细考虑以防出现问题。

推荐的配置：

upstream {
    server http://10.48.41.12;
}

server {
    listen 127.0.0.16:80;
    # [...]
}
在 HTTPS 中使用 SSLv3

由于 SSLv3 的 POODLE 漏洞， 建议不要在开启 SSL 的网站使用 SSLv3。 你可以简单粗暴的直接禁止 SSLv3， 用 TLS 来替代：

ssl_protocols TLSv1 TLSv1.1 TLSv1.2;

  