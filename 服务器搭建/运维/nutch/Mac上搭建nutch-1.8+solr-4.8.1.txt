 
cd apache-nutch-1.8
bin/nutch
如果有warning 看看是不是没有设置JAVA_HOME，如果是Permission denied，用chmod改权限
现在添加种子URL，《逆转未来 X-Men》

mkdir ~/urls
vim ～/urls/seed.txt
http://movie.douban.com/subject/10485647/
接下去是设置URL过滤准则
 
cd conf
vim ./regex-urlfilter.txt
然后set nu!找到第32行，改为如下形式
 
# skip URLs containing certain characters as probable queries, etc.
#-[?*!@=]
# accept anything else
#+.
+^http://movie.douban.com/subject/[0-9]+/(?.+)?$
然后vim nutch-site.xml，增加agent name的配置
 
<property>
  <name>http.agent.name</name>
  <value>My Nutch Spider</value>
</property>
下载解压solr
 
cd ~
wget http://mirrors.cnnic.cn/apache/lucene/solr/4.8.1/solr-4.8.1.tgz
tar -zxvf solr-4.8.1.tgz
检查安装
 
cd example
java -jar start.jar
打开本地URL:localhost:8983/solr/admin，有显示说明安装成功了
现在结合nutch和solr
将~/apache-nutch-1.8/conf/schema-solr4.xml拷贝到~/solr-4.8.1/solr/collection1/conf/，重命名为schema.xml
 
cp ./schema-solr4.xml /Users/hongliyu/solr-4.8.1/example/solr/collection1/conf
mv ./schema-solr4.xml ./schema.xml
并在<fields>…</fields>最后添加一行添加
 
<field name="_version_" type="long" indexed="true" stored="true" multiValued="false"/>
现在修改lockType
 
cd solr-4.8.1/example/solr/collection1/conf
vim solrconfig.xml
做如下修改：
 
<unlockOnStartup>true</unlockOnStartup>
<lockType>simple</lockType>
<maxFieldLength>10000</maxFieldLength>
<writeLockTimeout>60000</writeLockTimeout>
<commitLockTimeout>60000</commitLockTimeout>
然后在solr-4.8.1/example/solr/collection1/data/index目录下找到write.lock然后删除这个文件，再运行solr就不会有lockType错误
现在回到nutch目录下，bin/crawl，会看到

crawl <seedDir> <crawlDir> <solrURL> <numberOfRounds>

其中seedDir是url描述文件放置的目录，crawlDir是存放数据的根目录，solrURL是solr服务地址，最后一个参数是迭代的次数，运行这条命令
 
./bin/crawl ~/urls/ ./hlyuCrawl http://localhost:8983/solr/ 2
一般会等几分钟，屏幕上会有很多fetching url，

最后键入
 
bin/nutch readdb hlyuCrawl/crawldb/ -stats
可以看到爬虫结果了

这个是自动抓取，现在让我们来手动操作一遍，首先把刚才抓取的数据都删除
 
rm -rf ./hlyuCrawl/
nutch抓取的数据是由这些组成的：
crawldb,这里的信息包括，每一个nutch可以识别的url，包括url是否被抓取，以及抓取的时间
linkdb,它的信息包括，已知的每个URL的链接列表,包括源URL和锚文本的链接
一组segments，每个segments是urls抓取作为一个单元的组，segments是字典形式，也就是key-value，包括：

crawl_generate 表示将要被抓取的一组urls
crawl_fetch 表示抓取的url的状态
content 包含从每条url获取的原始的内容
parse_text 包含每条url的解析文本
parse_data 包含每条url的外链和元数据解析
parse_parse 包括外链urls用来更新crawldb
inject:使用种子URL列表，生成crawldb

 
 bin/nutch inject hlyuCrawl/crawldb ~/urls
将主目录urls下的种子URL生成一个URL数据库，放在crawdb目录下

 
bin/nutch generate hlyuCrawl/crawldb hlyuCrawl/segments
生成fetch list，存放在一个segments/日期目录下，然后把这个目录的名字保存在shell变量ss里

 
ss='hlyuCrawl/segments/2* | tail -1'
echo $ss
开始抓取

 
bin/nutch fetch $ss
这样会在$ss目录下，生成两个子目录，crawl_fetch和content

开始解析
 
bin/nutch parse $ss
更新crawldb

 
bin/nutch updatedb hlyuCrawl/crawldb $ss
这会把crawldb/current重命名为crawldb/old，并生成新的crawldb/current

在建立索引之前，我们首先要反转所有的链接，这样我们就可以获得一个页面所有的锚文本，并给这些锚文本建立索引


bin/nutch invertlinks hlyuCrawl/linkdb -dir hlyuCrawl/segments
提交数据给solr，建立索引

bin/nutch solrindex http://localhost:8983/solr hlyuCrawl/crawldb/ -linkdb hlyuCrawl/linkdb/ hlyuCrawl/segments/20140530125611/ -filter -normalize
如果索引里有重复数据，我们需要去重

bin/nutch dedup http://localhost:8983/solr
如果数据过时了，在索引里删除吧

 bin/nutch solrclean hlyuCrawl/crawldb/ http://localhost:8983/solr
参考http://cn.soulmachine.me/blog/20140121/，并纠正了版本不一致的错误